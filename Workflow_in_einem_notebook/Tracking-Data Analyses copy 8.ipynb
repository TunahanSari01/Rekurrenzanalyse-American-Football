{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3ff947",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurzer Tracking-Scan für StatsBomb-Dateien\n",
    "# Keine Zeitlogik, kein Merge, nur grobe Kompatibilitätsprüfung.\n",
    "# Pfad ggf. anpassen:\n",
    "INPUT_GLOB = \"/Users/tunahansari/football_ra/data/tracking/SB_tracking_*.json.gz\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Spielfeldmaße und Schwellenwerte\n",
    "FIELD_LEN = 120.0\n",
    "FIELD_WID = 53.33\n",
    "THRESH_FAIL_XY_NUMERIC = 0.80\n",
    "THRESH_WARN_XY_NUMERIC = 0.99\n",
    "THRESH_WARN_OOB = 0.005\n",
    "THRESH_WARN_PLAYER_ID = 0.95\n",
    "\n",
    "def _to_float(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _first(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _pct(n, d):\n",
    "    return (n / d) if d else 0.0\n",
    "\n",
    "def scan_file(file_path):\n",
    "    # Prüft eine Tracking-Datei auf die wichtigsten Felder\n",
    "    name = os.path.basename(file_path)\n",
    "    try:\n",
    "        with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        return {\"file\": name, \"status\": \"FAIL\", \"reason\": f\"json_error:{e}\"}\n",
    "\n",
    "    plays = data.get(\"plays\", [])\n",
    "    if not isinstance(plays, list) or len(plays) == 0:\n",
    "        return {\"file\": name, \"status\": \"FAIL\", \"reason\": \"no_plays\"}\n",
    "\n",
    "    n_plays = len(plays)\n",
    "    n_tracks = 0\n",
    "    steps_total = 0\n",
    "    xy_numeric = 0\n",
    "    oob = 0\n",
    "    tracks_total = 0\n",
    "    tracks_with_player_id = 0\n",
    "    plays_with_ltr = 0\n",
    "    plays_with_yard = 0\n",
    "\n",
    "    # Beispiel-Keys für Übersicht\n",
    "    example_play_keys = set()\n",
    "    example_track_keys = set()\n",
    "    example_step_keys = set()\n",
    "    for p in plays[:10]:\n",
    "        example_play_keys |= set(p.keys())\n",
    "\n",
    "    for play in plays:\n",
    "        if play.get(\"offense_left_to_right\") is not None:\n",
    "            plays_with_ltr += 1\n",
    "        if _to_float(play.get(\"play_yardline\")) is not None:\n",
    "            plays_with_yard += 1\n",
    "\n",
    "        tracks = play.get(\"tracks\", [])\n",
    "        if isinstance(tracks, list):\n",
    "            n_tracks += len(tracks)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for tr in tracks:\n",
    "            tracks_total += 1\n",
    "            example_track_keys |= set(tr.keys())\n",
    "\n",
    "            player = tr.get(\"player\") or tr.get(\"track_player\") or {}\n",
    "            if isinstance(player, dict) and player.get(\"player_id\") is not None:\n",
    "                tracks_with_player_id += 1\n",
    "\n",
    "            steps = tr.get(\"steps\") or tr.get(\"track_steps\") or []\n",
    "            for s in steps[:10]:\n",
    "                if isinstance(s, dict):\n",
    "                    example_step_keys |= set(s.keys())\n",
    "\n",
    "            for s in steps:\n",
    "                steps_total += 1\n",
    "                x = _to_float(_first(s.get(\"x\"), s.get(\"ngs_x\")))\n",
    "                y = _to_float(_first(s.get(\"y\"), s.get(\"ngs_y\")))\n",
    "                if x is not None and y is not None:\n",
    "                    xy_numeric += 1\n",
    "                    if not (0.0 <= x <= FIELD_LEN) or not (0.0 <= y <= FIELD_WID):\n",
    "                        oob += 1\n",
    "\n",
    "    r_xy_numeric = _pct(xy_numeric, steps_total)\n",
    "    r_oob = _pct(oob, max(xy_numeric, 1))\n",
    "    r_player_id = _pct(tracks_with_player_id, max(tracks_total, 1))\n",
    "    r_play_ltr = _pct(plays_with_ltr, max(n_plays, 1))\n",
    "    r_yardline = _pct(plays_with_yard, max(n_plays, 1))\n",
    "\n",
    "    # Status-Logik\n",
    "    status = \"PASS\"\n",
    "    reasons = []\n",
    "    if steps_total == 0:\n",
    "        status = \"FAIL\"\n",
    "        reasons.append(\"no_steps\")\n",
    "    if r_xy_numeric < THRESH_FAIL_XY_NUMERIC:\n",
    "        status = \"FAIL\"\n",
    "        reasons.append(f\"xy_numeric<{int(THRESH_FAIL_XY_NUMERIC*100)}%\")\n",
    "    elif r_xy_numeric < THRESH_WARN_XY_NUMERIC:\n",
    "        if status != \"FAIL\":\n",
    "            status = \"WARN\"\n",
    "            reasons.append(f\"xy_numeric<{int(THRESH_WARN_XY_NUMERIC*100)}%\")\n",
    "    if r_oob > THRESH_WARN_OOB:\n",
    "        if status != \"FAIL\":\n",
    "            status = \"WARN\"\n",
    "            reasons.append(f\"oob>{THRESH_WARN_OOB*100:.1f}%\")\n",
    "    if r_player_id < THRESH_WARN_PLAYER_ID:\n",
    "        if status != \"FAIL\":\n",
    "            status = \"WARN\"\n",
    "            reasons.append(f\"player_id<{int(THRESH_WARN_PLAYER_ID*100)}%\")\n",
    "\n",
    "    return {\n",
    "        \"file\": name,\n",
    "        \"status\": status,\n",
    "        \"reason\": \";\".join(reasons),\n",
    "        \"n_plays\": n_plays,\n",
    "        \"n_tracks\": n_tracks,\n",
    "        \"steps_total\": steps_total,\n",
    "        \"r_xy_numeric\": r_xy_numeric,\n",
    "        \"r_oob\": r_oob,\n",
    "        \"r_player_id\": r_player_id,\n",
    "        \"r_play_ltr\": r_play_ltr,\n",
    "        \"r_yardline\": r_yardline,\n",
    "        \"example_play_keys\": \", \".join(sorted(list(example_play_keys))[:30]),\n",
    "        \"example_track_keys\": \", \".join(sorted(list(example_track_keys))[:30]),\n",
    "        \"example_step_keys\": \", \".join(sorted(list(example_step_keys))[:30]),\n",
    "    }\n",
    "\n",
    "# Scan ausführen\n",
    "files = sorted(glob.glob(INPUT_GLOB))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien für Muster: {INPUT_GLOB}\")\n",
    "\n",
    "print(f\"Tracking-Scan: {len(files)} Dateien gefunden\")\n",
    "results = []\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    print(f\"[{i}/{len(files)}] {name} ... \", end=\"\")\n",
    "    res = scan_file(fp)\n",
    "    print(f\"{res['status']} {('('+res['reason']+')') if res['reason'] else ''}\")\n",
    "    results.append(res)\n",
    "\n",
    "df_scan = pd.DataFrame(results)\n",
    "print(\"\\nStatus-Übersicht:\")\n",
    "print(df_scan[\"status\"].value_counts())\n",
    "\n",
    "# DataFrame anzeigen\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_scan.sort_values([\"status\", \"file\"]))\n",
    "except:\n",
    "    print(df_scan.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73876163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Häufigkeit der Gründe für WARN/FAIL\n",
    "from collections import Counter\n",
    "\n",
    "reason_counts = Counter()\n",
    "for r in df_scan['reason'].fillna(''):\n",
    "    for part in [p for p in r.split(';') if p]:\n",
    "        reason_counts[part] += 1\n",
    "print(\"WARN/FAIL-Gründe (Häufigkeit):\")\n",
    "for k,v in reason_counts.most_common():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Top-10 mit höchstem OOB-Anteil\n",
    "cols = [\"file\",\"status\",\"r_oob\",\"r_xy_numeric\",\"r_player_id\"]\n",
    "print(\"\\nTop-10 OOB:\")\n",
    "display(df_scan.sort_values(\"r_oob\", ascending=False)[cols].head(10))\n",
    "\n",
    "# Dateien mit xy_numeric<99% \n",
    "print(\"\\nxy_numeric<99%:\")\n",
    "display(df_scan[df_scan[\"r_xy_numeric\"] < 0.99][cols].sort_values(\"r_xy_numeric\").head(20))\n",
    "\n",
    "# Präsenzraten der Play-Felder\n",
    "print(\"\\nDurchschnittliche Präsenz (über Dateien):\")\n",
    "print(\"offense_left_to_right  (mean r_play_ltr):\", df_scan[\"r_play_ltr\"].mean().round(3))\n",
    "print(\"play_yardline          (mean r_yardline):\", df_scan[\"r_yardline\"].mean().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8559b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse der Tracking-Daten\n",
    "\n",
    "INPUT_GLOB = \"/Users/tunahansari/football_ra/data/tracking/SB_tracking_*.json.gz\"\n",
    "\n",
    "import os, glob, gzip, json, math\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FIELD_LEN = 120.0\n",
    "FIELD_WID  = 53.33\n",
    "\n",
    "def _to_float(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _first(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "#\n",
    "def _oob_overshoot_mag(x, y):\n",
    "    \"\"\"Außerhalb des Rechtecks.\n",
    "       0 wenn in bounds, sonst Distanz zum nächstliegenden Rand.\"\"\"\n",
    "    ox = 0.0\n",
    "    oy = 0.0\n",
    "    if x is not None and y is not None:\n",
    "        if x < 0: ox = 0 - x\n",
    "        elif x > FIELD_LEN: ox = x - FIELD_LEN\n",
    "        if y < 0: oy = 0 - y\n",
    "        elif y > FIELD_WID: oy = y - FIELD_WID\n",
    "    return math.hypot(ox, oy)\n",
    "\n",
    "def _bin_overshoot(m):\n",
    "    \"\"\"Bins für OOB-Schweregrad in yards.\"\"\"\n",
    "    if m <= 0:          return \"in_bounds\"\n",
    "    elif m <= 0.5:      return \"<=0.5y\"\n",
    "    elif m <= 1.0:      return \"0.5–1y\"\n",
    "    elif m <= 2.0:      return \"1–2y\"\n",
    "    else:               return \">2y\"\n",
    "\n",
    "# Hauptanalyse\n",
    "files = sorted(glob.glob(INPUT_GLOB))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien für Muster: {INPUT_GLOB}\")\n",
    "\n",
    "print(f\"🔎 Zusatz-QA: {len(files)} Dateien\")\n",
    "\n",
    "# Aggregatoren pro Datei\n",
    "per_file = []\n",
    "\n",
    "# Play-Qualität (xy-Anteil) pro Play\n",
    "play_quality = {}  # key=(file, play_uuid) -> dict: steps_total, xy_numeric\n",
    "play_lengths = []  # Liste aller Play-Dauern\n",
    "\n",
    "# Positions-Stats\n",
    "position_counts_global = Counter()\n",
    "tracks_with_pos = 0\n",
    "tracks_total     = 0\n",
    "\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    print(f\"[{i:03d}/{len(files)}] Analysiere {name} ...\", flush=True)\n",
    "\n",
    "    # Zähler für Datei\n",
    "    steps_total = 0\n",
    "    steps_xy_ok = 0\n",
    "    oob_bins = Counter()   # in_bounds, (<=0.5y, 0.5–1y, 1–2y, >2y)\n",
    "    plays_in_file = 0\n",
    "    plays_with_tss = 0\n",
    "    positions_in_file = Counter()\n",
    "    tracks_with_pos_file = 0\n",
    "    tracks_total_file = 0\n",
    "\n",
    "    #  # Für Play-Längen, pro Play max(tss >= 0)\n",
    "    play_max_tss = {}  \n",
    "\n",
    "    # Für xy-Qualität pro Play\n",
    "    play_xy_steps = defaultdict(lambda: {\"steps_total\":0, \"xy_numeric\":0})\n",
    "\n",
    "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    plays = data.get(\"plays\", [])\n",
    "    plays_in_file = len(plays)\n",
    "\n",
    "    for play in plays:\n",
    "        play_uuid = play.get(\"play_uuid\")\n",
    "\n",
    "        tracks = play.get(\"tracks\", []) or []\n",
    "        for tr in tracks:\n",
    "            tracks_total += 1\n",
    "            tracks_total_file += 1\n",
    "\n",
    "            # Positionsfeld (nur Statistik)\n",
    "            player = tr.get(\"player\", tr.get(\"track_player\", {})) or {}\n",
    "            pos = player.get(\"position_code\")\n",
    "            if pos:\n",
    "                positions_in_file[pos] += 1\n",
    "                position_counts_global[pos] += 1\n",
    "                tracks_with_pos      += 1\n",
    "                tracks_with_pos_file += 1\n",
    "\n",
    "            steps = tr.get(\"steps\", tr.get(\"track_steps\", [])) or []\n",
    "            for s in steps:\n",
    "                steps_total += 1\n",
    "                play_xy_steps[(name, play_uuid)][\"steps_total\"] += 1\n",
    "\n",
    "                x = _to_float(_first(s.get(\"x\"), s.get(\"ngs_x\")))\n",
    "                y = _to_float(_first(s.get(\"y\"), s.get(\"ngs_y\")))\n",
    "                if x is not None and y is not None:\n",
    "                    steps_xy_ok += 1\n",
    "                    play_xy_steps[(name, play_uuid)][\"xy_numeric\"] += 1\n",
    "                    # OOB-Schweregrad\n",
    "                    m = _oob_overshoot_mag(x, y)\n",
    "                    oob_bins[_bin_overshoot(m)] += 1\n",
    "                else:\n",
    "                    # kein xy -> zählt nur zu steps_total/play_xy_steps.steps_total\n",
    "                    pass\n",
    "\n",
    "                # Play-Länge (nur 'Analyse', keine Normierung)\n",
    "                tss = _to_float(s.get(\"time_since_snap\"))\n",
    "                if tss is not None and tss >= 0:\n",
    "                    prev = play_max_tss.get(play_uuid)\n",
    "                    play_max_tss[play_uuid] = tss if (prev is None or tss > prev) else prev\n",
    "\n",
    "    # Datei-Ergebnis\n",
    "    valid_xy = steps_xy_ok\n",
    "    in_bounds = oob_bins.get(\"in_bounds\", 0)\n",
    "    oob_count = (valid_xy - in_bounds)\n",
    "\n",
    "    # Anteile bezogen auf gültige xy\n",
    "    def _share(key):\n",
    "        denom = max(valid_xy, 1)\n",
    "        return oob_bins.get(key, 0) / denom\n",
    "\n",
    "    per_file.append({\n",
    "        \"file\": name,\n",
    "        \"steps_total\": steps_total,\n",
    "        \"xy_valid\": valid_xy,\n",
    "        \"xy_valid_share\": (valid_xy / max(steps_total,1)),\n",
    "        \"oob_share_total\": (oob_count / max(valid_xy,1)),\n",
    "        \"oob_<=0.5y\": _share(\"<=0.5y\"),\n",
    "        \"oob_0.5–1y\": _share(\"0.5–1y\"),\n",
    "        \"oob_1–2y\": _share(\"1–2y\"),\n",
    "        \"oob_>2y\": _share(\">2y\"),\n",
    "        \"tracks_with_pos_share\": tracks_with_pos_file / max(tracks_total_file,1),\n",
    "        \"plays_in_file\": plays_in_file,\n",
    "    })\n",
    "\n",
    "    # Play-Längen sammeln\n",
    "    for puid, tmax in play_max_tss.items():\n",
    "        play_lengths.append(tmax)\n",
    "\n",
    "    # Play-Qualität (xy-Anteil) sammeln\n",
    "    for key, d in play_xy_steps.items():\n",
    "        play_quality[key] = {\n",
    "            \"file\": key[0],\n",
    "            \"play_uuid\": key[1],\n",
    "            \"steps_total\": d[\"steps_total\"],\n",
    "            \"xy_valid\": d[\"xy_numeric\"],\n",
    "            \"xy_valid_share\": d[\"xy_numeric\"] / max(d[\"steps_total\"],1)\n",
    "        }\n",
    "\n",
    "# Ergebnisse in DataFrames\n",
    "df_files = pd.DataFrame(per_file).sort_values(\"oob_share_total\", ascending=False)\n",
    "df_plays = pd.DataFrame(play_quality.values()).sort_values(\"xy_valid_share\")\n",
    "\n",
    "print(\"\\n Zusatz-QA fertig.\\n\")\n",
    "\n",
    "# 1) OOB-Schweregrad\n",
    "print(\"1) OOB-Schweregrad (global, Anteil an gültigen Punkten) – gemittelt über Dateien:\")\n",
    "cols_oob = [\"oob_share_total\",\"oob_<=0.5y\",\"oob_0.5–1y\",\"oob_1–2y\",\"oob_>2y\"]\n",
    "print(df_files[cols_oob].mean().round(4).to_string())\n",
    "\n",
    "print(\"\\nTop-10 Dateien nach OOB-Gesamtanteil:\")\n",
    "display(df_files[[\"file\",\"oob_share_total\",\"oob_<=0.5y\",\"oob_0.5–1y\",\"oob_1–2y\",\"oob_>2y\",\"xy_valid_share\"]].head(10))\n",
    "\n",
    "# 2) xy-Lücken – schlechteste Plays\n",
    "print(\"\\n2) Schlechteste 15 Plays nach xy_valid_share:\")\n",
    "display(df_plays[[\"file\",\"play_uuid\",\"steps_total\",\"xy_valid\",\"xy_valid_share\"]].head(15))\n",
    "\n",
    "# 3) Positionsabdeckung\n",
    "print(\"\\n3) Positionsabdeckung:\")\n",
    "pos_total = sum(position_counts_global.values())\n",
    "pos_df = (pd.Series(position_counts_global, name=\"count\")\n",
    "            .sort_values(ascending=False)\n",
    "            .to_frame())\n",
    "pos_df[\"share\"] = pos_df[\"count\"] / max(pos_total,1)\n",
    "display(pos_df)\n",
    "\n",
    "print(\"\\nAnteil Tracks mit position_code – pro Datei (Top 10 niedrigste):\")\n",
    "display(df_files[[\"file\",\"tracks_with_pos_share\"]].sort_values(\"tracks_with_pos_share\").head(10))\n",
    "\n",
    "# 4) Play-Längen (Sekunden ab Snap)\n",
    "if play_lengths:\n",
    "    arr = np.array(play_lengths)\n",
    "    summary = {\n",
    "        \"count\": int(arr.size),\n",
    "        \"min\": round(float(arr.min()), 3),\n",
    "        \"p25\": round(float(np.percentile(arr, 25)), 3),\n",
    "        \"median\": round(float(np.percentile(arr, 50)), 3),\n",
    "        \"p75\": round(float(np.percentile(arr, 75)), 3),\n",
    "        \"p90\": round(float(np.percentile(arr, 90)), 3),\n",
    "        \"p95\": round(float(np.percentile(arr, 95)), 3),\n",
    "        \"max\": round(float(arr.max()), 3),\n",
    "        \">=4s\": int((arr >= 4.0).sum()),\n",
    "        \">=5s\": int((arr >= 5.0).sum()),\n",
    "        \">=6s\": int((arr >= 6.0).sum()),\n",
    "    }\n",
    "    print(\"\\n4) Play-Längen (Sekunden, nur wenn time_since_snap vorhanden):\")\n",
    "    for k,v in summary.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"\\n4) Play-Längen: Keine time_since_snap gefunden – Länge nicht auswertbar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting der OOB-Anteile pro Play\n",
    "\n",
    "INPUT_GLOB = \"/Users/tunahansari/football_ra/data/tracking/SB_tracking_*.json.gz\"\n",
    "TOP_FILES = 3              # wie viele Dateien mit höchstem OOB-Anteil anschauen\n",
    "TOP_PLAYS_PER_FILE = 2     # wie viele Plays je Datei (höchster OOB-Anteil) plotten\n",
    "MAX_STEPS_PLOT = None     \n",
    "\n",
    "import os, glob, gzip, json, math\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FIELD_LEN, FIELD_WID = 120.0, 53.33\n",
    "\n",
    "def _to_float(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _first(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def file_oob_ratio(file_path):\n",
    "    \"\"\"Grober OOB-Anteil pro Datei (nur gültige xy zählen als Basis).\"\"\"\n",
    "    steps_valid = 0\n",
    "    oob = 0\n",
    "    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for play in data.get(\"plays\", []):\n",
    "        for tr in play.get(\"tracks\", []) or []:\n",
    "            for s in tr.get(\"steps\", tr.get(\"track_steps\", [])) or []:\n",
    "                x = _to_float(_first(s.get(\"x\"), s.get(\"ngs_x\")))\n",
    "                y = _to_float(_first(s.get(\"y\"), s.get(\"ngs_y\")))\n",
    "                if x is None or y is None:\n",
    "                    continue\n",
    "                steps_valid += 1\n",
    "                if not (0.0 <= x <= FIELD_LEN) or not (0.0 <= y <= FIELD_WID):\n",
    "                    oob += 1\n",
    "    return (oob / steps_valid) if steps_valid else 0.0\n",
    "\n",
    "def plays_oob_stats(file_path):\n",
    "    \"\"\"Per-Play OOB-Anteil + Rohpunkte (lazy) für spätere Auswahl/Plot.\"\"\"\n",
    "    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    by_play = defaultdict(lambda: {\"valid\":0, \"oob\":0})\n",
    "    # Für Plot speichern wir pro Play nur die Punkte (x,y) als zwei Listen (in/out)\n",
    "    raw_points = defaultdict(lambda: {\"in\": [], \"out\": []})\n",
    "\n",
    "    for play in data.get(\"plays\", []):\n",
    "        puid = play.get(\"play_uuid\")\n",
    "        for tr in play.get(\"tracks\", []) or []:\n",
    "            steps = tr.get(\"steps\", tr.get(\"track_steps\", [])) or []\n",
    "            for idx, s in enumerate(steps):\n",
    "                x = _to_float(_first(s.get(\"x\"), s.get(\"ngs_x\")))\n",
    "                y = _to_float(_first(s.get(\"y\"), s.get(\"ngs_y\")))\n",
    "                if x is None or y is None:\n",
    "                    continue\n",
    "                by_play[puid][\"valid\"] += 1\n",
    "                in_bounds = (0.0 <= x <= FIELD_LEN) and (0.0 <= y <= FIELD_WID)\n",
    "                if in_bounds:\n",
    "                    raw_points[puid][\"in\"].append((x,y))\n",
    "                else:\n",
    "                    by_play[puid][\"oob\"] += 1\n",
    "                    raw_points[puid][\"out\"].append((x,y))\n",
    "\n",
    "    rows = []\n",
    "    for puid, d in by_play.items():\n",
    "        valid = d[\"valid\"]\n",
    "        oob = d[\"oob\"]\n",
    "        share = (oob / valid) if valid else 0.0\n",
    "        rows.append((puid, valid, oob, share))\n",
    "    rows.sort(key=lambda t: t[3], reverse=True)  # nach OOB-Anteil\n",
    "    return rows, raw_points\n",
    "\n",
    "def plot_play_scatter(file_name, play_uuid, raw_points):\n",
    "    \"\"\"Ein Plot pro Play: In-bounds Punkte '.' und OOB Punkte 'x'. Keine Farben gesetzt.\"\"\"\n",
    "    pts_in  = raw_points[play_uuid][\"in\"]\n",
    "    pts_out = raw_points[play_uuid][\"out\"]\n",
    "\n",
    "    # Steps begrenzen (nur für sehr große Plays)\n",
    "    if MAX_STEPS_PLOT is not None:\n",
    "        pts_in  = pts_in[:MAX_STEPS_PLOT]\n",
    "        pts_out = pts_out[:MAX_STEPS_PLOT]\n",
    "\n",
    "    plt.figure(figsize=(7.0, 3.6))\n",
    "\n",
    "    # In-bounds als Punkte\n",
    "    if pts_in:\n",
    "        xi, yi = zip(*pts_in)\n",
    "        plt.plot(xi, yi, '.', markersize=2, label=\"in-bounds\")\n",
    "\n",
    "    # OOB als X-Marker\n",
    "    if pts_out:\n",
    "        xo, yo = zip(*pts_out)\n",
    "        plt.plot(xo, yo, 'x', markersize=3, label=\"OOB\")\n",
    "\n",
    "    # Feldrahmen\n",
    "    plt.axvline(0); plt.axvline(FIELD_LEN)\n",
    "    plt.axhline(0); plt.axhline(FIELD_WID)\n",
    "    plt.xlim(-2, FIELD_LEN+2)\n",
    "    plt.ylim(-2, FIELD_WID+2)\n",
    "    plt.xlabel(\"x (yards)\")\n",
    "    plt.ylabel(\"y (yards)\")\n",
    "    plt.title(f\"{file_name} | play={play_uuid}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Dateien nach OOB-Anteil sortieren und Top auswählen\n",
    "files = sorted(glob.glob(INPUT_GLOB))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien für Muster: {INPUT_GLOB}\")\n",
    "print(f\" Wähle Top-{TOP_FILES} Dateien mit höchstem OOB-Anteil …\")\n",
    "file_scores = []\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    print(f\"  [{i:03d}/{len(files)}] Scanne {name} …\", end=\"\", flush=True)\n",
    "    score = file_oob_ratio(fp)\n",
    "    file_scores.append((name, fp, score))\n",
    "    print(f\" OOB={score:.4%}\")\n",
    "file_scores.sort(key=lambda t: t[2], reverse=True)\n",
    "top_files = file_scores[:TOP_FILES]\n",
    "print(\"\\n Top-Dateien:\")\n",
    "for name, _, sc in top_files:\n",
    "    print(f\"   {name}: OOB≈{sc:.2%}\")\n",
    "\n",
    "# Aus jeder Top-Datei die schlimmsten Plays wählen & plotten\n",
    "for name, fp, sc in top_files:\n",
    "    print(f\"\\n Datei: {name} (OOB≈{sc:.2%}) → ermittle Top-{TOP_PLAYS_PER_FILE} Plays …\")\n",
    "    rows, raw_points = plays_oob_stats(fp)\n",
    "    picks = rows[:TOP_PLAYS_PER_FILE]\n",
    "    for (puid, valid, oob, share) in picks:\n",
    "        print(f\"  • Play {puid}: valid={valid}, oob={oob}, OOB-Anteil={share:.2%} → plot\")\n",
    "        plot_play_scatter(name, puid, raw_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusatz-Analyse: OOB-Anteile pro Play\n",
    "\n",
    "INPUT_GLOB = \"/Users/tunahansari/football_ra/data/tracking/SB_tracking_*.json.gz\"\n",
    "TOP_FILES = 3            # wie viele Dateien mit höchstem OOB-Anteil prüfen\n",
    "TOP_PLAYS_PER_FILE = 2   # pro Datei wie viele Plays (mit höchstem OOB-Anteil)\n",
    "\n",
    "import os, glob, gzip, json, math\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FIELD_LEN, FIELD_WID = 120.0, 53.33\n",
    "\n",
    "def _to_float(v):\n",
    "    try: return float(v)\n",
    "    except Exception: return None\n",
    "\n",
    "def _first(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def _oob(x, y):\n",
    "    return not (0.0 <= x <= FIELD_LEN) or not (0.0 <= y <= FIELD_WID)\n",
    "\n",
    "def _overshoot_mag(x, y):\n",
    "    ox = (0 - x) if x < 0 else (x - FIELD_LEN) if x > FIELD_LEN else 0.0\n",
    "    oy = (0 - y) if y < 0 else (y - FIELD_WID) if y > FIELD_WID else 0.0\n",
    "    return math.hypot(ox, oy)\n",
    "\n",
    "def _bin_overshoot(m):\n",
    "    if m <= 0:   return \"in_bounds\"\n",
    "    if m <= 0.5: return \"<=0.5y\"\n",
    "    if m <= 1.0: return \"0.5–1y\"\n",
    "    if m <= 2.0: return \"1–2y\"\n",
    "    return \">2y\"\n",
    "\n",
    "def file_oob_ratio(fp):\n",
    "    steps_valid = 0; oob = 0\n",
    "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for play in data.get(\"plays\", []):\n",
    "        for tr in play.get(\"tracks\", []) or []:\n",
    "            for s in tr.get(\"steps\", tr.get(\"track_steps\", [])) or []:\n",
    "                x = _to_float(_first(s.get(\"x\"), s.get(\"ngs_x\"))); y = _to_float(_first(s.get(\"y\"), s.get(\"ngs_y\")))\n",
    "                if x is None or y is None: continue\n",
    "                steps_valid += 1\n",
    "                if _oob(x,y): oob += 1\n",
    "    return (oob/steps_valid) if steps_valid else 0.0\n",
    "\n",
    "def plays_oob_with_cal(fp):\n",
    "    \"\"\"Gibt pro Play: valid, oob, oob_cal_true, oob_cal_false, OOB-Bins zurück (Liste von Dicts, sortiert).\"\"\"\n",
    "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    per = {}\n",
    "    bins = {}\n",
    "    for play in data.get(\"plays\", []):\n",
    "        puid = play.get(\"play_uuid\")\n",
    "        if puid not in per:\n",
    "            per[puid] = {\"play_uuid\": puid, \"valid\":0, \"oob\":0, \"oob_cal_true\":0, \"oob_cal_false\":0}\n",
    "            bins[puid] = Counter()\n",
    "        for tr in play.get(\"tracks\", []) or []:\n",
    "            for s in tr.get(\"steps\", tr.get(\"track_steps\", [])) or []:\n",
    "                x = _to_float(_first(s.get(\"x\"), s.get(\"ngs_x\"))); y = _to_float(_first(s.get(\"y\"), s.get(\"ngs_y\")))\n",
    "                if x is None or y is None: continue\n",
    "                per[puid][\"valid\"] += 1\n",
    "                if _oob(x,y):\n",
    "                    per[puid][\"oob\"] += 1\n",
    "                    cal = s.get(\"calibration_fault\")\n",
    "                    if cal is None: cal = s.get(\"step_calibration_fault\")\n",
    "                    if bool(cal): per[puid][\"oob_cal_true\"] += 1\n",
    "                    else:         per[puid][\"oob_cal_false\"] += 1\n",
    "                    bins[puid][_bin_overshoot(_overshoot_mag(x,y))] += 1\n",
    "    rows = []\n",
    "    for puid, d in per.items():\n",
    "        valid = d[\"valid\"]; oob = d[\"oob\"]\n",
    "        share = (oob/valid) if valid else 0.0\n",
    "        row = {\n",
    "            \"play_uuid\": puid,\n",
    "            \"valid\": valid,\n",
    "            \"oob\": oob,\n",
    "            \"oob_share\": share,\n",
    "            \"oob_cal_true\": d[\"oob_cal_true\"],\n",
    "            \"oob_cal_true_share\": (d[\"oob_cal_true\"]/oob) if oob else 0.0,\n",
    "            \"oob_cal_false\": d[\"oob_cal_false\"],\n",
    "            \"oob_cal_false_share\": (d[\"oob_cal_false\"]/oob) if oob else 0.0,\n",
    "            \"oob_<=0.5y\": bins[puid][\"<=0.5y\"] / max(oob,1),\n",
    "            \"oob_0.5–1y\": bins[puid][\"0.5–1y\"] / max(oob,1),\n",
    "            \"oob_1–2y\":   bins[puid][\"1–2y\"]   / max(oob,1),\n",
    "            \"oob_>2y\":    bins[puid][\">2y\"]    / max(oob,1),\n",
    "        }\n",
    "        rows.append(row)\n",
    "    rows.sort(key=lambda r: r[\"oob_share\"], reverse=True)\n",
    "    return rows\n",
    "\n",
    "# Auswahl Top-Dateien nach OOB- Anteil\n",
    "files = sorted(glob.glob(INPUT_GLOB))\n",
    "if not files: raise FileNotFoundError(\"Keine Dateien gefunden.\")\n",
    "scores = [(os.path.basename(fp), fp, file_oob_ratio(fp)) for fp in files]\n",
    "scores.sort(key=lambda t: t[2], reverse=True)\n",
    "top = scores[:TOP_FILES]\n",
    "print(\"Top-Dateien (höchster OOB-Anteil):\")\n",
    "for name, _, sc in top:\n",
    "    print(f\"  {name}: OOB≈{sc:.2%}\")\n",
    "\n",
    "# Prüfe Plays in den Top-Dateien\n",
    "all_rows = []\n",
    "for name, fp, sc in top:\n",
    "    print(f\"\\n {name} — prüfe Plays …\")\n",
    "    rows = plays_oob_with_cal(fp)[:TOP_PLAYS_PER_FILE]\n",
    "    for r in rows:\n",
    "        r[\"file\"] = name\n",
    "        all_rows.append(r)\n",
    "        print(f\"  • play={r['play_uuid']} | OOB={r['oob']}/{r['valid']} ({r['oob_share']:.2%}), \"\n",
    "              f\"cal_true={r['oob_cal_true']}/{r['oob']} ({r['oob_cal_true_share']:.2%})\")\n",
    "\n",
    "df_check = pd.DataFrame(all_rows, columns=[\n",
    "    \"file\",\"play_uuid\",\"valid\",\"oob\",\"oob_share\",\n",
    "    \"oob_cal_true\",\"oob_cal_true_share\",\"oob_cal_false\",\"oob_cal_false_share\",\n",
    "    \"oob_<=0.5y\",\"oob_0.5–1y\",\"oob_1–2y\",\"oob_>2y\"\n",
    "]).sort_values([\"file\",\"oob_share\"], ascending=[True, False])\n",
    "\n",
    "display(df_check)\n",
    "\n",
    "# Aggregierte Aussage über die Stichprobe:\n",
    "if not df_check.empty:\n",
    "    agg = {\n",
    "        \"plays_geprueft\": len(df_check),\n",
    "        \"median_oob_share\": df_check[\"oob_share\"].median(),\n",
    "        \"median_cal_true_share\": df_check[\"oob_cal_true_share\"].median(),\n",
    "        \"mean_cal_true_share\": df_check[\"oob_cal_true_share\"].mean(),\n",
    "        \"mean_oob_gt2y_share\": df_check[\"oob_>2y\"].mean(),\n",
    "    }\n",
    "    print(\"\\nZusammenfassung (Stichprobe):\")\n",
    "    for k,v in agg.items():\n",
    "        print(f\"  {k}: {v:.3f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6aebad",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff669cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking-Daten scannen\n",
    "\n",
    "import os, glob, gzip, json, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "INPUT_GLOB = \"/Users/tunahansari/football_ra/data/tracking/SB_tracking_*.json.gz\"\n",
    "\n",
    "# Output pro Datei speichern\n",
    "OUTPUT_DIR = Path(\"/Users/tunahansari/football_ra/out_simple\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Master-Output (Zusammenführung)\n",
    "MASTER_OUT_DIR = Path(\"/Users/tunahansari/football_ra/out_1hz_clean\")\n",
    "MASTER_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MASTER_BASENAME = \"master_1hz_4s_ready\"  \n",
    "\n",
    "# Bestehende Outputs überschreiben?\n",
    "FORCE_OVERWRITE = False\n",
    "\n",
    "# Master bauen?\n",
    "REBUILD_MASTER = False  # Auf True setzen, um Master zu erstellen\n",
    "\n",
    "# Einheit: \"yards\", \"feet\" oder None (automatisch ermitteln)\n",
    "FORCE_UNITS = None\n",
    "\n",
    "WINDOW_SECONDS = 4             # Zeitfenster in Sekunden\n",
    "PLAY_MIN_VALID_SHARE = 0.90    # Mindestanteil gültiger Schritte pro Play\n",
    "OOB_DROP_YARDS = 2.0           # OOB-Schwellenwert in Yards\n",
    "FIELD_LEN, FIELD_WID = 120.0, 53.33\n",
    "ENDZONE = 10.0\n",
    "\n",
    "print(f\"Config: files='{INPUT_GLOB}', out='{OUTPUT_DIR}', master='{MASTER_OUT_DIR}', overwrite={FORCE_OVERWRITE}, rebuild_master={REBUILD_MASTER}\")\n",
    "\n",
    "# ----- Hilfsfunktionen -----\n",
    "def _to_float(v):\n",
    "    try:\n",
    "        x = float(v)\n",
    "        if math.isnan(x):\n",
    "            return None\n",
    "        return x\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _overshoot_mag(x, y):\n",
    "    # Abstand außerhalb des Spielfelds berechnen\n",
    "    ox = (0 - x) if x < 0 else (x - FIELD_LEN) if x > FIELD_LEN else 0.0\n",
    "    oy = (0 - y) if y < 0 else (y - FIELD_WID) if y > FIELD_WID else 0.0\n",
    "    return math.hypot(ox, oy)\n",
    "\n",
    "def _clip_xy(x, y):\n",
    "    # (x,y) an Spielfeldgrenzen anpassen\n",
    "    return (min(max(x, 0.0), FIELD_LEN), min(max(y, 0.0), FIELD_WID))\n",
    "\n",
    "def _safe_parquet_path(base_dir: Path, stem: str, ts: bool = True) -> Path:\n",
    "    \"\"\"Erzeuge sicheren Speicherpfad mit Zeitstempel.\"\"\"\n",
    "    if ts:\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        return base_dir / f\"{stem}_{tag}.parquet\"\n",
    "    return base_dir / f\"{stem}.parquet\"\n",
    "\n",
    "def _save_parquet(df: pd.DataFrame, out_path: Path):\n",
    "    try:\n",
    "        df.to_parquet(out_path, index=False, engine=\"pyarrow\")\n",
    "        print(f\" gespeichert: {out_path} (Zeilen: {len(df):,})\")\n",
    "    except Exception as e:\n",
    "        fb = out_path.with_suffix(\".pkl\")\n",
    "        df.to_pickle(fb)\n",
    "        print(f\" Parquet fehlgeschlagen ({e}); Fallback: {fb}\")\n",
    "\n",
    "def _pick_xy_keys(first_play):\n",
    "    # Ermitteln plausible (x,y)-Schlüssel aus ersten Daten\n",
    "    XY_KEYS = [(\"x\",\"y\"), (\"track_x\",\"track_y\"), (\"ngs_x\",\"ngs_y\"), (\"px\",\"py\"), (\"X\",\"Y\")]\n",
    "    tracks = (first_play or {}).get(\"tracks\") or []\n",
    "    for tr in tracks:\n",
    "        steps = tr.get(\"steps\") or tr.get(\"track_steps\") or []\n",
    "        if not steps:\n",
    "            continue\n",
    "        s0 = steps[0]\n",
    "        for kx, ky in XY_KEYS:\n",
    "            if kx in s0 and ky in s0:\n",
    "                return kx, ky\n",
    "    return \"x\", \"y\"  # Standard\n",
    "\n",
    "def _gather_sample_xy(plays, kx, ky, max_n=5000):\n",
    "    # (x,y)-Werte zur Einheitenerkennung\n",
    "    out = []\n",
    "    for play in plays:\n",
    "        for tr in (play.get(\"tracks\") or []):\n",
    "            for s in (tr.get(\"steps\") or []):\n",
    "                if len(out) >= max_n:\n",
    "                    return out\n",
    "                x = _to_float(s.get(kx)); y = _to_float(s.get(ky))\n",
    "                if x is None or y is None:\n",
    "                    continue\n",
    "                out.append((x,y))\n",
    "    return out\n",
    "\n",
    "def _auto_units(sample_xy):\n",
    "    if FORCE_UNITS in (\"yards\",\"feet\"):\n",
    "        return FORCE_UNITS\n",
    "    if not sample_xy:\n",
    "        return \"yards\"\n",
    "    def score_xy(pairs):\n",
    "        n = min(len(pairs), 2000)\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        inb = 0\n",
    "        for i in range(n):\n",
    "            x,y = pairs[i]\n",
    "            if 0 <= x <= FIELD_LEN and 0 <= y <= FIELD_WID:\n",
    "                inb += 1\n",
    "        return inb / n\n",
    "    yards_pairs = sample_xy\n",
    "    feet_pairs  = [(x/3.0, y/3.0) for (x,y) in sample_xy]\n",
    "    sy, sf = score_xy(yards_pairs), score_xy(feet_pairs)\n",
    "    return \"yards\" if sy >= sf else \"feet\"\n",
    "\n",
    "files = sorted(glob.glob(INPUT_GLOB))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien gefunden für das Muster: {INPUT_GLOB}\")\n",
    "\n",
    "print(f\"Starte Preprocessing: {len(files)} Dateien\")\n",
    "qc_rows = []\n",
    "per_file_outputs = []\n",
    "\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    out_parquet = OUTPUT_DIR / (name.replace(\".json.gz\", \".parquet\"))\n",
    "    print(f\"\\n[{i:03d}/{len(files)}] {name}\")\n",
    "\n",
    "    if out_parquet.exists() and not FORCE_OVERWRITE:\n",
    "        print(f\"  ↪ Datei existiert bereits, überspringe (FORCE_OVERWRITE={FORCE_OVERWRITE})\")\n",
    "        per_file_outputs.append(out_parquet)\n",
    "        continue\n",
    "\n",
    "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    plays = data.get(\"plays\") or []\n",
    "    if not plays:\n",
    "        print(\"  Keine Plays gefunden, überspringe\")\n",
    "        continue\n",
    "\n",
    "    # (x,y)-Schlüssel und Einheiten\n",
    "    kx, ky = _pick_xy_keys(plays[0])\n",
    "    sample_xy_raw = _gather_sample_xy(plays, kx, ky, max_n=4000)\n",
    "    units = _auto_units(sample_xy_raw)\n",
    "    print(f\" Keys=({kx},{ky})  Einheiten={units}\")\n",
    "\n",
    "    # Zähler initialisieren\n",
    "    steps_total = steps_numeric_win = steps_kept_win = 0\n",
    "    drop_cal = drop_oob_gt2 = clip_oob_le2 = 0\n",
    "    rows_acc = defaultdict(lambda: {\n",
    "        \"sx\":0.0,\"sy\":0.0,\"c\":0,\n",
    "        \"pname\":None,\"pos\":None,\"tid\":None,\n",
    "        \"gid\":data.get(\"game_id\"),\n",
    "        \"home\":data.get(\"home_abbr\") or (data.get(\"home_team\",{}) or {}).get(\"nfl_team_id\"),\n",
    "        \"away\":data.get(\"away_abbr\") or (data.get(\"away_team\",{}) or {}).get(\"nfl_team_id\"),\n",
    "        \"off\":None,\"def\":None,\n",
    "        \"q\":None,\"down\":None,\"ytg\":None,\"ptype\":None\n",
    "    })\n",
    "\n",
    "    for play in plays:\n",
    "        puid = play.get(\"play_uuid\")\n",
    "        if not puid:\n",
    "            continue\n",
    "\n",
    "        ltr = bool(play.get(\"offense_left_to_right\", True))\n",
    "        yln = _to_float(play.get(\"play_yardline\"))\n",
    "        if yln is None or not (0.0 <= yln <= 100.0):\n",
    "            # Ungültige Yardline überspringen\n",
    "            continue\n",
    "\n",
    "        off_id = play.get(\"play_offense_team_id\") or play.get(\"offense_team_id\")\n",
    "        def_id = play.get(\"play_defense_team_id\") or play.get(\"defense_team_id\")\n",
    "\n",
    "        meta_by_play[puid] = dict(\n",
    "            ltr=ltr, yln=yln, off=off_id, de=def_id,\n",
    "            q=play.get(\"play_quarter\"),\n",
    "            down=play.get(\"play_down\"),\n",
    "            ytg=play.get(\"play_yards_to_go\"),\n",
    "            ptype=play.get(\"play_type\")\n",
    "        )\n",
    "\n",
    "        for tr in (play.get(\"tracks\") or []):\n",
    "            player = tr.get(\"player\") or tr.get(\"track_player\") or {}\n",
    "            pid = player.get(\"player_id\")\n",
    "            if pid is None:\n",
    "                continue\n",
    "            pname = player.get(\"name\")\n",
    "            ppos  = player.get(\"position_code\")\n",
    "            tid   = tr.get(\"team_id\") or tr.get(\"track_team_id\") or tr.get(\"nfl_team_id\")\n",
    "            steps = tr.get(\"steps\") or tr.get(\"track_steps\") or []\n",
    "            for s in steps:\n",
    "                tss = _to_float(s.get(\"time_since_snap\"))\n",
    "                if tss is None or tss < 0:\n",
    "                    continue\n",
    "                t_sec = int(math.floor(tss))\n",
    "                if t_sec < 0 or t_sec >= WINDOW_SECONDS:\n",
    "                    continue\n",
    "\n",
    "                steps_total += 1\n",
    "\n",
    "                xr = _to_float(s.get(kx)); yr = _to_float(s.get(ky))\n",
    "                if xr is None or yr is None:\n",
    "                    continue\n",
    "\n",
    "                # Einheitstransformation\n",
    "                if units == \"feet\":\n",
    "                    x_raw, y_raw = xr/3.0, yr/3.0\n",
    "                else:\n",
    "                    x_raw, y_raw = xr, yr\n",
    "\n",
    "                steps_numeric_win += 1\n",
    "\n",
    "                # calibration_fault prüfen\n",
    "                cal = s.get(\"calibration_fault\")\n",
    "                if cal is None:\n",
    "                    cal = s.get(\"step_calibration_fault\")\n",
    "                if bool(cal):\n",
    "                    drop_cal += 1\n",
    "                    continue\n",
    "\n",
    "                # OOB-Check vor Orientierung\n",
    "                m = _overshoot_mag(x_raw, y_raw)\n",
    "                if m > OOB_DROP_YARDS:\n",
    "                    drop_oob_gt2 += 1\n",
    "                    continue\n",
    "                if m > 0:\n",
    "                    x_raw, y_raw = _clip_xy(x_raw, y_raw)\n",
    "                    clip_oob_le2 += 1\n",
    "\n",
    "                # Orientierung: X \n",
    "                x = x_raw if ltr else (FIELD_LEN - x_raw)\n",
    "                y = y_raw\n",
    "\n",
    "                play_seen[puid] += 1\n",
    "\n",
    "                # Aggregation pro (play, player, Sekunde)\n",
    "                key = (puid, pid, t_sec)\n",
    "                acc = rows_acc[key]\n",
    "                acc[\"sx\"] += x\n",
    "                acc[\"sy\"] += y\n",
    "                acc[\"c\"]  += 1\n",
    "                if acc[\"pname\"] is None: acc[\"pname\"] = pname\n",
    "                if acc[\"pos\"]   is None: acc[\"pos\"]  = ppos\n",
    "                if acc[\"tid\"]   is None: acc[\"tid\"]  = tid\n",
    "                if acc[\"off\"]   is None: acc[\"off\"]  = off_id\n",
    "                if acc[\"def\"]   is None: acc[\"def\"]  = def_id\n",
    "                if acc[\"q\"]     is None: acc[\"q\"]    = play.get(\"play_quarter\")\n",
    "                if acc[\"down\"]  is None: acc[\"down\"] = play.get(\"play_down\")\n",
    "                if acc[\"ytg\"]   is None: acc[\"ytg\"]  = play.get(\"play_yards_to_go\")\n",
    "                if acc[\"ptype\"] is None: acc[\"ptype\"]= play.get(\"play_type\")\n",
    "\n",
    "                steps_kept_win += 1\n",
    "                play_kept[puid] += \n",
    "\n",
    "    # Spiele mit zu wenigen gültigen Schritten verwerfen\n",
    "    drop_plays = set()\n",
    "    for puid, seen in play_seen.items():\n",
    "        kept = play_kept.get(puid, 0)\n",
    "        share = kept / max(seen, 1)\n",
    "        if share < PLAY_MIN_VALID_SHARE:\n",
    "            drop_plays.add(puid)\n",
    "\n",
    "    print(f\"Steps: total={steps_total:,} | numeric={steps_numeric_win:,} | kept={steps_kept_win:,}\")\n",
    "    print(f\"    - calibration_fault: {drop_cal:,}\")\n",
    "    print(f\"    - OOB >{OOB_DROP_YARDS}yd gedroppt: {drop_oob_gt2:,}\")\n",
    "    print(f\"    - OOB ≤{OOB_DROP_YARDS}yd geclippt: {clip_oob_le2:,}\")\n",
    "    print(f\"Plays: total={len(plays)} | gedroppt (<{int(PLAY_MIN_VALID_SHARE*100)}% gültig): {len(drop_plays)}\")\n",
    "\n",
    "    # Ausgabe-Daten erstellen\n",
    "    rows = []\n",
    "    for (puid, pid, t_sec), a in rows_acc.items():\n",
    "        if a[\"c\"] == 0 or puid in drop_plays:\n",
    "            continue\n",
    "        meta = meta_by_play.get(puid, {})\n",
    "        ltr = meta.get(\"ltr\", True)\n",
    "        yln = meta.get(\"yln\", 0.0)\n",
    "\n",
    "        # LOS relativ zur Orientierung\n",
    "        L   = (ENDZONE + yln) if ltr else (110.0 - yln)\n",
    "        rows.append({\n",
    "            \"play_uuid\": puid,\n",
    "            \"player_id\": pid,\n",
    "            \"t_sec\": t_sec,\n",
    "            \"x_norm\": (a[\"sx\"]/a[\"c\"]) - L,   \n",
    "            \"y\": a[\"sy\"]/a[\"c\"],              \n",
    "            \"player_name\": a[\"pname\"],\n",
    "            \"position_code\": a[\"pos\"],\n",
    "            \"team_id\": a[\"tid\"],\n",
    "            \"game_id\": a[\"gid\"],\n",
    "            \"home_abbr\": a[\"home\"],\n",
    "            \"away_abbr\": a[\"away\"],\n",
    "            \"offense_team_id\": a[\"off\"],\n",
    "            \"defense_team_id\": a[\"def\"],\n",
    "            \"play_quarter\": a[\"q\"],\n",
    "            \"play_down\": a[\"down\"],\n",
    "            \"play_yards_to_go\": a[\"ytg\"],\n",
    "            \"play_type\": a[\"ptype\"],\n",
    "            \"play_yardline\": yln,\n",
    "            \"ori\": \"KEEP\" if ltr else \"MIRROR\",\n",
    "            \"units\": units,\n",
    "            \"x_key\": kx, \"y_key\": ky,\n",
    "        })\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values([\"play_uuid\",\"player_id\",\"t_sec\"])\n",
    "        # Einzeldatei speichern\n",
    "        if out_parquet.exists() and not FORCE_OVERWRITE:\n",
    "            print(f\"  Ziel existiert bereits und FORCE_OVERWRITE=False → Skip Save: {out_parquet}\")\n",
    "        else:\n",
    "            _save_parquet(df, out_parquet)\n",
    "            per_file_outputs.append(out_parquet)\n",
    "    else:\n",
    "        print(\"  Nichts zu speichern (alle Daten verworfen)\")\n",
    "\n",
    "    qc_rows.append({\n",
    "        \"file\": name,\n",
    "        \"plays_total\": len(plays),\n",
    "        \"plays_dropped\": len(drop_plays),\n",
    "        \"steps_total_4s\": steps_total,\n",
    "        \"steps_numeric_4s\": steps_numeric_win,\n",
    "        \"steps_kept_4s\": steps_kept_win,\n",
    "        \"drop_calibration\": drop_cal,\n",
    "        \"drop_oob_gt2\": drop_oob_gt2,\n",
    "        \"clip_oob_le2\": clip_oob_le2,\n",
    "    })\n",
    "\n",
    "# Gesamt-QC anzeigen\n",
    "df_qc = pd.DataFrame(qc_rows)\n",
    "print(\"\\n Fertig (pro Datei).\")\n",
    "if not df_qc.empty:\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_qc.head(10))\n",
    "        display(df_qc[[\"steps_total_4s\",\"steps_numeric_4s\",\"steps_kept_4s\",\"drop_calibration\",\"drop_oob_gt2\",\"clip_oob_le2\"]].sum())\n",
    "    except Exception:\n",
    "        print(df_qc.head(10).to_string(index=False))\n",
    "        sums = df_qc[[\"steps_total_4s\",\"steps_numeric_4s\",\"steps_kept_4s\",\"drop_calibration\",\"drop_oob_gt2\",\"clip_oob_le2\"]].sum()\n",
    "        for k,v in sums.items():\n",
    "            print(f\"  {k}: {int(v):,}\")\n",
    "\n",
    "# Master-Output erstellen (Concat aller Parquets)\n",
    "if REBUILD_MASTER:\n",
    "    print(\"\\n Baue Master…\")\n",
    "    # Alle Parquets im OUTPUT_DIR verwenden\n",
    "    parts = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
    "    if not parts:\n",
    "        print(\"  Keine Teile gefunden – Master entfällt.\")\n",
    "    else:\n",
    "        dfs = []\n",
    "        for p in parts:\n",
    "            try:\n",
    "                d = pd.read_parquet(p)\n",
    "                # Prüfung der Kernspalten\n",
    "                need = {\"play_uuid\",\"player_id\",\"t_sec\",\"x_norm\",\"y\"}\n",
    "                if not need.issubset(d.columns):\n",
    "                    print(f\"  {p.name}: fehlende Spalten {need - set(d.columns)} – Teil überspringen\")\n",
    "                    continue\n",
    "                dfs.append(d)\n",
    "            except Exception as e:\n",
    "                print(f\"  {p.name}: Read-Error {e} – Teil überspringen\")\n",
    "\n",
    "        if not dfs:\n",
    "            print(\"  Keine verwertbaren Teile – Master entfällt.\")\n",
    "        else:\n",
    "            master = pd.concat(dfs, ignore_index=True)\n",
    "            master.sort_values([\"game_id\",\"play_uuid\",\"player_id\",\"t_sec\"], inplace=True)\n",
    "\n",
    "            # Berechne dx, dy, speed pro 1 Hz\n",
    "            grp = [\"game_id\",\"play_uuid\",\"player_id\"]\n",
    "            master[\"dx\"] = master.groupby(grp, observed=True)[\"x_norm\"].diff().fillna(0.0)\n",
    "            master[\"dy\"] = master.groupby(grp, observed=True)[\"y\"].diff().fillna(0.0)\n",
    "            master[\"speed\"] = np.sqrt(master[\"dx\"]**2 + master[\"dy\"]**2)\n",
    "\n",
    "            # Schreibpfad mit Zeitstempel\n",
    "            out_master = _safe_parquet_path(MASTER_OUT_DIR, MASTER_BASENAME, ts=True)\n",
    "            _save_parquet(master, out_master)\n",
    "            print(f\"Master geschrieben → {out_master}\")\n",
    "else:\n",
    "    print(\"\\nREBUILD_MASTER=False – kein Master erstellt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9be5e3",
   "metadata": {},
   "source": [
    "merge & mini check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caea992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x): print(x)\n",
    "\n",
    "BASE_DIR = Path(\"/Users/tunahansari/football_ra/out_1hz_clean\")\n",
    "if not BASE_DIR.exists():\n",
    "    BASE_DIR = Path.cwd() / \"out_1hz_clean\"\n",
    "\n",
    "PARQUET_GLOB = str(BASE_DIR / \"*.parquet\")\n",
    "MASTER_OUT = str(BASE_DIR / \"master_1hz_4s.parquet\")\n",
    "\n",
    "FIELD_WID = 53.33\n",
    "T_MIN, T_MAX = 0, 3\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"play_uuid\", \"player_id\", \"t_sec\", \"x_norm\",\n",
    "    \"position_code\", \"track_team_id\", \"offense_team_id\", \"defense_team_id\",\n",
    "    \"play_yardline\", \"play_type\", \"home_abbr\", \"away_abbr\", \"game_id\", \"gsis_play_id\"\n",
    "]\n",
    "\n",
    "ALIASES = {\n",
    "    \"player_id\": [\"player_id\", \"nfl_id\", \"nflId\"],\n",
    "    \"gsis_play_id\": [\"gsis_play_id\", \"play_id\", \"gsisPlayId\"],\n",
    "    \"position_code\": [\"position_code\", \"position\"],\n",
    "    \"track_team_id\": [\"track_team_id\", \"team_id\", \"teamId\", \"team\"],\n",
    "    \"offense_team_id\": [\"offense_team_id\", \"offenseTeamId\", \"offense_team\"],\n",
    "    \"defense_team_id\": [\"defense_team_id\", \"defenseTeamId\", \"defense_team\"],\n",
    "    \"play_yardline\": [\"play_yardline\", \"yardline\", \"yardLine\"],\n",
    "    \"play_type\": [\"play_type\", \"playType\"],\n",
    "    \"home_abbr\": [\"home_abbr\", \"homeTeamAbbr\", \"home_team\"],\n",
    "    \"away_abbr\": [\"away_abbr\", \"awayTeamAbbr\", \"away_team\"],\n",
    "    \"game_id\": [\"game_id\", \"gameId\"],\n",
    "}\n",
    "\n",
    "def ensure_alias_cols(df, required_cols, aliases):\n",
    "    missing = []\n",
    "    for col in required_cols:\n",
    "        if col in df.columns:\n",
    "            continue\n",
    "        if col in (\"x_norm\", \"t_sec\", \"play_uuid\"):\n",
    "            if col not in df.columns:\n",
    "                missing.append(col)\n",
    "            continue\n",
    "        for a in aliases.get(col, []):\n",
    "            if a in df.columns:\n",
    "                df[col] = df[a]\n",
    "                break\n",
    "        else:\n",
    "            missing.append(col)\n",
    "    return df, missing\n",
    "\n",
    "print(\"Suche Parquet-Dateien ...\")\n",
    "files = sorted(glob.glob(PARQUET_GLOB))\n",
    "files = [f for f in files if not os.path.basename(f).startswith(\"master_\")]\n",
    "print(f\"Gefunden: {len(files)} Dateien in {BASE_DIR}\")\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien gefunden. Bitte prüfen: {PARQUET_GLOB}\")\n",
    "\n",
    "print(\"\\nBestimme Spaltennamen für y (step_y vs. y) aus der ersten Datei ...\")\n",
    "probe = pd.read_parquet(files[0])\n",
    "if \"y\" in probe.columns:\n",
    "    Y_COL = \"y\"\n",
    "elif \"step_y\" in probe.columns:\n",
    "    Y_COL = \"step_y\"\n",
    "else:\n",
    "    raise KeyError(\"Weder 'y' noch 'step_y' in den Parquet-Dateien gefunden.\")\n",
    "print(f\"y-Spalte: {Y_COL}\")\n",
    "\n",
    "print(\"\\nLade & merge alle Dateien (das dauert je nach Platte kurz) ...\")\n",
    "dfs = []\n",
    "running_rows = 0\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    df = pd.read_parquet(fp)\n",
    "    df[\"file\"] = name\n",
    "    dfs.append(df)\n",
    "    running_rows += len(df)\n",
    "    if i % 10 == 0 or i == len(files):\n",
    "        print(f\"[{i:03d}/{len(files)}] geladen: {name}  (aktuelle Gesamtzeilen ~ {running_rows:,})\")\n",
    "\n",
    "master = pd.concat(dfs, ignore_index=True)\n",
    "del dfs, probe\n",
    "\n",
    "print(\"\\nMerge fertig.\")\n",
    "print(f\"master.shape = {master.shape[0]:,} Zeilen × {master.shape[1]} Spalten\")\n",
    "mem_mb = master.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"geschätzter Speicherbedarf: {mem_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nMINI-QC startet ...\")\n",
    "\n",
    "print(\"\\nPflichtspalten prüfen ...\")\n",
    "master, missing_after_alias = ensure_alias_cols(master, REQUIRED_COLS, ALIASES)\n",
    "\n",
    "HARD_REQ = {\"play_uuid\", \"player_id\", \"t_sec\", \"x_norm\"}\n",
    "hard_missing = [c for c in HARD_REQ if c not in master.columns]\n",
    "soft_missing = [c for c in missing_after_alias if c not in HARD_REQ]\n",
    "\n",
    "if hard_missing:\n",
    "    print(f\"Harte Pflichtspalten fehlen: {hard_missing}\")\n",
    "    raise KeyError(f\"Pflichtspalten fehlen: {hard_missing}\")\n",
    "if soft_missing:\n",
    "    for c in soft_missing:\n",
    "        print(f\"Hinweis: optionale/Meta-Spalte fehlt: {c}\")\n",
    "print(\"Pflichtspalten ok.\")\n",
    "\n",
    "print(\"\\nt_sec-Check ... (erwartet 0..3)\")\n",
    "t_min, t_max = master[\"t_sec\"].min(), master[\"t_sec\"].max()\n",
    "vals = np.sort(master[\"t_sec\"].unique())\n",
    "share_out_range = ((master[\"t_sec\"] < T_MIN) | (master[\"t_sec\"] > T_MAX)).mean()\n",
    "print(f\"t_sec Werte: min={t_min}, max={t_max}, Unique={vals[:10]}{' ...' if len(vals) > 10 else ''}\")\n",
    "print(f\"Anteil außerhalb [{T_MIN},{T_MAX}]: {share_out_range:.4%}\")\n",
    "if share_out_range > 0:\n",
    "    counts_out = master.loc[(master[\"t_sec\"] < T_MIN) | (master[\"t_sec\"] > T_MAX), \"t_sec\"].value_counts().sort_index()\n",
    "    print(\"Werte außerhalb Range (Counts):\")\n",
    "    print(counts_out.to_string())\n",
    "\n",
    "print(\"\\ny-Grenzen (0 .. 53.33 yd) ...\")\n",
    "y = pd.to_numeric(master[Y_COL], errors=\"coerce\")\n",
    "oob_low = (y < 0).sum()\n",
    "oob_high = (y > FIELD_WID).sum()\n",
    "oob_share = ((y < 0) | (y > FIELD_WID)).mean()\n",
    "print(f\"y.min={float(np.nanmin(y)):.3f}, y.max={float(np.nanmax(y)):.3f}\")\n",
    "print(f\"OOB y<0: {oob_low:,} | y>{FIELD_WID}: {oob_high:,}  → Anteil: {oob_share:.4%}\")\n",
    "if oob_share == 0:\n",
    "    print(\"y liegt vollständig im Feld (Clip hat gegriffen).\")\n",
    "else:\n",
    "    print(\"Es gibt noch Punkte außerhalb – ggf. stichprobenartig prüfen.\")\n",
    "\n",
    "print(\"\\nx_norm @ t=0 ...\")\n",
    "t0 = master.loc[master[\"t_sec\"] == 0, \"x_norm\"]\n",
    "t0 = pd.to_numeric(t0, errors=\"coerce\").dropna()\n",
    "if len(t0) > 0:\n",
    "    q = t0.quantile([0.01, 0.25, 0.5, 0.75, 0.99]).to_dict()\n",
    "    mean_, std_ = float(t0.mean()), float(t0.std())\n",
    "    print(f\"count={t0.shape[0]:,} | mean={mean_:.3f} | std={std_:.3f}\")\n",
    "    print(f\"quantiles: 1%={q[0.01]:.3f}, 25%={q[0.25]:.3f}, 50%={q[0.5]:.3f}, 75%={q[0.75]:.3f}, 99%={q[0.99]:.3f}\")\n",
    "    if abs(mean_) <= 0.25:\n",
    "        print(\"LOS-Normalisierung sieht gut aus (Mittelwert ~0 yd).\")\n",
    "    else:\n",
    "        print(\"Mittelwert ist weiter von 0 entfernt als erwartet – ggf. LOS-Offset verifizieren.\")\n",
    "else:\n",
    "    print(\"Keine t=0-Zeilen gefunden (unerwartet).\")\n",
    "\n",
    "print(\"\\nZeilen pro Datei (Top 10):\")\n",
    "lines_per_file = master[\"file\"].value_counts().head(10)\n",
    "print(lines_per_file.to_string())\n",
    "\n",
    "print(\"\\nMINI-QC abgeschlossen – Daten sind bereit für RP/CRP/RQA & Clustering.\")\n",
    "\n",
    "SAVE_MASTER = True\n",
    "if SAVE_MASTER:\n",
    "    out_dir = os.path.dirname(MASTER_OUT)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    master.to_parquet(MASTER_OUT, index=False)\n",
    "    print(f\"\\nMaster-Parquet gespeichert: {MASTER_OUT}\")\n",
    "    print(\"(Beim Weiterarbeiten kannst du direkt dieses File laden)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MASTER_OUT = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s.parquet\"\n",
    "\n",
    "try:\n",
    "    master \n",
    "except NameError:\n",
    "    master = pd.read_parquet(MASTER_OUT)\n",
    "\n",
    "t0 = master.loc[master[\"t_sec\"] == 0, [\"file\",\"play_uuid\",\"x_norm\",\"position_code\"]].copy()\n",
    "t0[\"abs_x0\"] = t0[\"x_norm\"].abs()\n",
    "\n",
    "print(f\"t0 rows: {len(t0):,}\")\n",
    "print(f\"Anteil |x_norm| @t0 > 12 yd: {(t0['abs_x0']>12).mean():.2%}\")\n",
    "\n",
    "print(\"\\nTop-Dateien mit vielen Ausreißern (|x_norm|>12yd) @t0:\")\n",
    "print(t0.loc[t0[\"abs_x0\"]>12].groupby(\"file\").size().sort_values(ascending=False).head(15).to_string())\n",
    "\n",
    "print(\"\\nSchlimmste 10 Plays (|x_norm| @t0):\")\n",
    "cols = [\"file\",\"play_uuid\",\"position_code\",\"x_norm\"]\n",
    "print(t0.sort_values(\"abs_x0\", ascending=False)[cols].head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_norm-Korrektur für Tracking-Daten\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "MASTER_IN  = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s.parquet\"\n",
    "MASTER_OUT = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s_fix.parquet\"\n",
    "\n",
    "print(\"Lade Master …\")\n",
    "try:\n",
    "    master  \n",
    "    print(\"   (nutze vorhandenen DataFrame 'master')\")\n",
    "except NameError:\n",
    "    master = pd.read_parquet(MASTER_IN)\n",
    "    print(f\"   geladen: {len(master):,} Zeilen\")\n",
    "\n",
    "# Sicherheit: numerische Typen erzwingen\n",
    "master[\"t_sec\"]  = pd.to_numeric(master[\"t_sec\"], errors=\"coerce\")\n",
    "master[\"x_norm\"] = pd.to_numeric(master[\"x_norm\"], errors=\"coerce\")\n",
    "\n",
    "t0_before = master.loc[master[\"t_sec\"]==0, \"x_norm\"].dropna()\n",
    "share_bad_before = (t0_before.abs() > 12).mean()\n",
    "print(f\"\\nVorher: |x_norm|@t0 > 12 yd = {share_bad_before:.2%}\")\n",
    "print(f\"   t0 count={t0_before.shape[0]:,} | mean={t0_before.mean():.3f} | std={t0_before.std():.3f}\")\n",
    "\n",
    "# offset pro Play berechnen\n",
    "def play_offset(g: pd.DataFrame) -> float:\n",
    "    t0 = g[g[\"t_sec\"]==0]\n",
    "    if t0.empty:\n",
    "        return 0.0\n",
    "    # Offense-Spieler bei t0\n",
    "    off_mask = (t0[\"track_team_id\"] == t0[\"offense_team_id\"])\n",
    "    if off_mask.sum() >= 8:\n",
    "        med = np.nanmedian(t0.loc[off_mask, \"x_norm\"])\n",
    "    else:\n",
    "        # Fallback: alle bei t0 (z.B. wenn Team-IDs fehlen)\n",
    "        med = np.nanmedian(t0[\"x_norm\"])\n",
    "    return float(med) if np.isfinite(med) else 0.0\n",
    "\n",
    "print(\"\\nBerechne Offsets pro play_uuid …\")\n",
    "offsets = master.groupby(\"play_uuid\", sort=False).apply(play_offset)\n",
    "\n",
    "# Kleine Übersicht der Offset-Verteilung\n",
    "q = offsets.quantile([0.01,0.25,0.5,0.75,0.99]).to_dict()\n",
    "print(f\"   Offsets quantiles (yd): 1%={q[0.01]:.2f}, 25%={q[0.25]:.2f}, 50%={q[0.5]:.2f}, 75%={q[0.75]:.2f}, 99%={q[0.99]:.2f}\")\n",
    "print(f\"   Anteil |Offset| > 12 yd: {(offsets.abs()>12).mean():.2%}\")\n",
    "\n",
    "# --- Anwenden: x_norm korrigieren ------------------------------------------\n",
    "print(\"\\n Wende Offsets an (x_norm_fix = x_norm - Offset) …\")\n",
    "master[\"x_norm_fix\"] = master[\"x_norm\"] - master[\"play_uuid\"].map(offsets)\n",
    "\n",
    "# --- Nachher-Diagnose -------------------------------------------------------\n",
    "t0_after = master.loc[master[\"t_sec\"]==0, \"x_norm_fix\"].dropna()\n",
    "share_bad_after = (t0_after.abs() > 12).mean()\n",
    "print(f\"\\nNachher: |x_norm_fix|@t0 > 12 yd = {share_bad_after:.2%}\")\n",
    "print(f\"   t0 count={t0_after.shape[0]:,} | mean={t0_after.mean():.3f} | std={t0_after.std():.3f}\")\n",
    "\n",
    "# Optional: very-bad plays markieren (falls du noch strenger filtern willst)\n",
    "# Ein simples Gütekriterium: Nach der Korrektur sollten >=90% der Spieler eines Plays bei t0 innerhalb ±12 yd liegen.\n",
    "t0_fix = master.loc[master[\"t_sec\"]==0, [\"play_uuid\",\"x_norm_fix\"]].copy()\n",
    "t0_fix[\"ok\"] = t0_fix[\"x_norm_fix\"].abs() <= 12\n",
    "good_share = t0_fix.groupby(\"play_uuid\")[\"ok\"].mean()\n",
    "bad_plays = good_share[good_share < 0.90].index\n",
    "print(f\"\\nPlays mit fraglicher Korrektur (t0 <90% in ±12 yd): {len(bad_plays):,}\")\n",
    "\n",
    "# --- Speichern --------------------------------------------------------------\n",
    "print(\"\\n Speichere Master mit x_norm_fix …\")\n",
    "Path(MASTER_OUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "master.to_parquet(MASTER_OUT, index=False)\n",
    "print(f\"   geschrieben: {MASTER_OUT}  (Zeilen: {len(master):,})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/Users/tunahansari/football_ra/out_1hz_clean\"\n",
    "IN_FIX = f\"{BASE}/master_1hz_4s_fix.parquet\"\n",
    "OUT_REZERO = f\"{BASE}/master_1hz_4s_rezero.parquet\"\n",
    "OUT_BADPLAYS = f\"{BASE}/bad_plays_t0_lt90.csv\"\n",
    "\n",
    "master = pd.read_parquet(IN_FIX)\n",
    "master[\"t_sec\"] = pd.to_numeric(master[\"t_sec\"], errors=\"coerce\")\n",
    "master[\"x_norm_fix\"] = pd.to_numeric(master[\"x_norm_fix\"], errors=\"coerce\")\n",
    "\n",
    "# 1) Globalen Restversatz @t0 entfernen (zentriert Median auf 0)\n",
    "t0_fix = master.loc[master[\"t_sec\"]==0, \"x_norm_fix\"].dropna()\n",
    "global_residual = float(t0_fix.median()) if len(t0_fix) else 0.0\n",
    "print(f\" Globaler Rest-Offset (Median @t0): {global_residual:.3f} yd\")\n",
    "\n",
    "master[\"x_norm_final\"] = master[\"x_norm_fix\"] - global_residual\n",
    "\n",
    "# Diagnose nach Re-Zentrierung\n",
    "t0_final = master.loc[master[\"t_sec\"]==0, \"x_norm_final\"].dropna()\n",
    "share_bad = (t0_final.abs() > 12).mean()\n",
    "print(f\" Nachher-final: |x_norm_final|@t0 > 12 yd = {share_bad:.2%}\")\n",
    "print(f\"   t0 count={t0_final.shape[0]:,} | mean={t0_final.mean():.3f} | std={t0_final.std():.3f} | median={t0_final.median():.3f}\")\n",
    "\n",
    "# 2) Plays mit <90% ok @t0 markieren & Report schreiben\n",
    "t0 = master.loc[master[\"t_sec\"]==0, [\"play_uuid\",\"x_norm_final\",\"file\"]].copy()\n",
    "t0[\"ok\"] = t0[\"x_norm_final\"].abs() <= 12\n",
    "per_play = t0.groupby(\"play_uuid\").agg(\n",
    "    share_ok=(\"ok\", \"mean\"),\n",
    "    n=(\"ok\",\"size\"),\n",
    "    n_ok=(\"ok\",\"sum\")\n",
    ").reset_index()\n",
    "\n",
    "bad_plays = per_play.loc[per_play[\"share_ok\"] < 0.90, \"play_uuid\"]\n",
    "print(f\"Plays mit t0<90% in ±12yd: {len(bad_plays):,}\")\n",
    "\n",
    "# Report: welche Dateien / wie stark betroffen\n",
    "bad_report = (\n",
    "    t0[t0[\"play_uuid\"].isin(bad_plays)]\n",
    "    .drop_duplicates(subset=[\"play_uuid\",\"file\"])\n",
    "    .merge(per_play, on=\"play_uuid\", how=\"left\")\n",
    "    .sort_values([\"share_ok\",\"file\"])\n",
    ")\n",
    "Path(OUT_BADPLAYS).parent.mkdir(parents=True, exist_ok=True)\n",
    "bad_report.to_csv(OUT_BADPLAYS, index=False)\n",
    "print(f\" Report gespeichert: {OUT_BADPLAYS} (Zeilen: {len(bad_report):,})\")\n",
    "\n",
    "# 3) x_norm ersetzen & speichern (für Downstream)\n",
    "master_out = master.drop(columns=[c for c in [\"x_norm\",\"x_norm_fix\"] if c in master.columns]) \\\n",
    "                   .rename(columns={\"x_norm_final\":\"x_norm\"})\n",
    "master_out.to_parquet(OUT_REZERO, index=False)\n",
    "print(f\" geschrieben: {OUT_REZERO}  (Zeilen: {len(master_out):,})\")\n",
    "\n",
    "print(\"\\nAlles fertig. Nutze ab jetzt dieses File für Clustering/RP/CRP/RQA:\")\n",
    "print(\" →\", OUT_REZERO)\n",
    "print(\"Und schau ggf. in den Bad-Play-Report:\")\n",
    "print(\" →\", OUT_BADPLAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREP B: Aus Zeitreihen eine Pro-Play-Feature-Tabelle bauen ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TS_PATH = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s_ready.parquet\"   # oder absoluter Pfad\n",
    "TS = pd.read_parquet(TS_PATH)\n",
    "\n",
    "# Spalten wie 'play_uuid' + Zeitreihen (z. B. speed, d_pos, v_rad, x_norm, y ...)\n",
    "def make_features_from_timeseries(df, id_col=\"play_uuid\"):\n",
    "    feats = []\n",
    "    for pid, g in df.groupby(id_col):\n",
    "        row = {id_col: pid, \"n_samples\": len(g)}\n",
    "        # Statistiken für die Zeitreihe\n",
    "        for col in [\"speed\", \"d_pos\", \"v_rad\", \"x_norm\", \"y\"]:\n",
    "            if col in g.columns:\n",
    "                med = float(g[col].median())\n",
    "                row[f\"{col}_med\"] = med\n",
    "                row[f\"{col}_mad\"] = float((g[col] - med).abs().median())\n",
    "                row[f\"{col}_iqr\"] = float(g[col].quantile(0.75) - g[col].quantile(0.25))\n",
    "                row[f\"{col}_trend_lr\"] = float(np.polyfit(np.arange(len(g)), g[col].to_numpy(), 1)[0]) if len(g) >= 3 else 0.0\n",
    "        feats.append(row)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "FEATURES = make_features_from_timeseries(TS, id_col=\"play_uuid\")\n",
    "print(FEATURES.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2790155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER-BLOCK \n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "id_col = \"play_uuid\"\n",
    "assert id_col in FEATURES.columns, f\"Spalte '{id_col}' fehlt in FEATURES.\"\n",
    "\n",
    "# 1) Feature-Spalten automatisch wählen (nur numerisch, ohne ID)\n",
    "num_cols = FEATURES.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in num_cols if c not in [id_col]]\n",
    "if len(feature_cols) < 2:\n",
    "    raise ValueError(\"Zu wenig numerische Feature-Spalten gefunden. Bitte Feature-Build prüfen.\")\n",
    "\n",
    "# 2) Arbeitskopie & NaNs füllen\n",
    "DF = deepcopy(FEATURES[[id_col] + feature_cols]).copy()\n",
    "X = DF[feature_cols].astype(float)\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# 3) Skalieren (+ optional PCA)\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "use_pca = True\n",
    "Xc = PCA(n_components=0.90, svd_solver=\"full\", random_state=0).fit_transform(Xs) if use_pca else Xs\n",
    "\n",
    "# 4) k per Silhouette (2..8)\n",
    "best = (-np.inf, None, None)\n",
    "for k in range(2, 9):\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "    lab = km.fit_predict(Xc)\n",
    "    sil = silhouette_score(Xc, lab) if len(set(lab)) > 1 else -np.inf\n",
    "    if sil > best[0]:\n",
    "        best = (sil, k, km)\n",
    "sil, k_best, km_best = best\n",
    "labs_km = km_best.predict(Xc)\n",
    "\n",
    "# 5) Agglomerativ (Ward) @k_best\n",
    "agg = AgglomerativeClustering(n_clusters=k_best, linkage=\"ward\")\n",
    "labs_agg = agg.fit_predict(Xc)\n",
    "\n",
    "# 6) Labels additiv an FEATURES hängen\n",
    "FEATURES = FEATURES.merge(\n",
    "    DF[[id_col]].assign(cl_kmeans=labs_km, cl_agg=labs_agg),\n",
    "    on=id_col, how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Clusterzahl (K-Means): k={k_best}, Silhouette={sil:.3f}\")\n",
    "print(FEATURES[[id_col, 'cl_kmeans','cl_agg']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6dda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "ks, sils = [], []\n",
    "for k in range(2,9):\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=0).fit(Xc)\n",
    "    lab = km.labels_\n",
    "    if len(set(lab))>1:\n",
    "        ks.append(k); sils.append(silhouette_score(Xc, lab))\n",
    "print(list(zip(ks, np.round(sils,3))))\n",
    "# Optional: kurzer Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(ks, sils, marker='o')\n",
    "plt.title('Silhouette je k')\n",
    "plt.xlabel('k'); plt.ylabel('Silhouette'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile = (FEATURES\n",
    "                   .groupby('cl_kmeans')[feature_cols]\n",
    "                   .median()\n",
    "                   .assign(n=FEATURES.groupby('cl_kmeans').size()))\n",
    "cluster_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630117dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "runs = []\n",
    "best = None\n",
    "for eps in (0.3,0.5,0.7,1.0):\n",
    "    for ms in (5,10,20):\n",
    "        db = DBSCAN(eps=eps, min_samples=ms).fit(Xc)\n",
    "        lab = db.labels_\n",
    "        k_eff = len(set(lab)) - (1 if -1 in lab else 0)\n",
    "        noise = (lab == -1).mean()\n",
    "        runs.append((eps, ms, k_eff, round(noise,3)))\n",
    "# pick eine sinnvolle Kombi (z.B. wenig Noise, k_eff 2–10) und fitten:\n",
    "db = DBSCAN(eps=0.5, min_samples=10).fit(Xc)\n",
    "FEATURES['cl_dbscan'] = db.labels_\n",
    "print('DBSCAN: -1 = Noise, sonst Cluster-ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lab = FEATURES['cl_dbscan']\n",
    "vals, cnts = np.unique(lab, return_counts=True)\n",
    "print(dict(zip(vals, cnts)))\n",
    "noise = float((lab == -1).mean())\n",
    "k_eff = len(set(lab)) - (1 if -1 in set(lab) else 0)\n",
    "print(f\"k_eff={k_eff}, Noise={noise:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-Labels in Dashboard-DF integrieren\n",
    "df = FEATURES.copy()\n",
    "\n",
    "# Vorherige Spalten sichern (für Merge-Check)\n",
    "cols_before = set(df.columns) - {'cl_kmeans','cl_agg','cl_dbscan'}\n",
    "unchanged = df[sorted(cols_before)].copy()\n",
    "\n",
    "# Neue Cluster-Labels hinzufügen\n",
    "added = {'cl_kmeans','cl_agg'} & set(df.columns)\n",
    "print(\"Neue Spalten (sollten nur die Cluster-Labels sein):\", added)\n",
    "print(\"Alte Spalten unverändert:\", True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "BASE = \"/Users/tunahansari/football_ra/out_1hz_clean\"\n",
    "IN_MASTER = f\"{BASE}/master_1hz_4s_rezero.parquet\"\n",
    "BAD = f\"{BASE}/bad_plays_t0_lt90.csv\"\n",
    "OUT_READY = f\"{BASE}/master_1hz_4s_ready.parquet\"\n",
    "\n",
    "master = pd.read_parquet(IN_MASTER)\n",
    "bad = pd.read_csv(BAD)[\"play_uuid\"].unique()\n",
    "print(\"Bad plays:\", len(bad))\n",
    "\n",
    "clean = master[~master[\"play_uuid\"].isin(bad)].copy()\n",
    "clean.to_parquet(OUT_READY, index=False)\n",
    "print(f\"geschrieben: {OUT_READY}  (Zeilen: {len(clean):,})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4545772",
   "metadata": {},
   "source": [
    "Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3840395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output, State, dash_table\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from flask_caching import Cache\n",
    "\n",
    "# Clustering\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ========================\n",
    "# CONFIG\n",
    "# ========================\n",
    "DEFAULT_FEATURES_PATH = \"out_1hz_clean/master_1hz_4s_ready.parquet\"\n",
    "FEATURES_PATH = os.getenv(\"FEATURES_PATH\", DEFAULT_FEATURES_PATH)\n",
    "\n",
    "MAX_HEATMAP_POINTS = int(os.getenv(\"MAX_HEATMAP_POINTS\", 60_000))\n",
    "SPEED_HIST_BINS    = 36\n",
    "RQA_SHOW_MAX_GAMES = 6\n",
    "\n",
    "# RQA Defaults\n",
    "RQA_DEFAULT_RR   = 0.10  # 10 %\n",
    "RQA_DEFAULT_LMIN = 2\n",
    "RQA_DEFAULT_VMIN = 2\n",
    "\n",
    "# RQA – Feature-Gewichte & Standardisierung\n",
    "RQA_FEATURE_WEIGHTS = (1.0, 1.0, 0.6)  # speed etwas geringer gewichten\n",
    "RQA_STANDARDIZE     = True             # z-Score je Achse vor Distanz\n",
    "\n",
    "# Limits für klassische RQA (Performance)\n",
    "RQA_CLASSIC_DEFAULT_MAXPTS = int(os.getenv(\"RQA_CLASSIC_MAXPTS\", 3000))  # Cap auf Matrix-Kantenlänge\n",
    "RQA_CLASSIC_DEFAULT_DECIM  = int(os.getenv(\"RQA_CLASSIC_DECIM\", 1))      # jeden k-ten Punkt\n",
    "\n",
    "# ========================\n",
    "# DATA LOADING\n",
    "# ========================\n",
    "NEEDED_COLS = [\n",
    "    \"play_uuid\",\"player_id\",\"t_sec\",\"x_norm\",\"y\",\n",
    "    \"player_name\",\"position_code\",\"team_id\",\"game_id\",\n",
    "    \"home_abbr\",\"away_abbr\",\"offense_team_id\",\"defense_team_id\",\n",
    "    \"play_quarter\",\"play_down\",\"play_yards_to_go\",\"play_type\",\n",
    "    \"dx\",\"dy\",\"speed\",\"heading_deg\"\n",
    "]\n",
    "\n",
    "def _detect_y_col(df: pd.DataFrame) -> str:\n",
    "    if \"y\" in df.columns: return \"y\"\n",
    "    if \"step_y\" in df.columns: return \"step_y\"\n",
    "    raise KeyError(\"Neither 'y' nor 'step_y' found.\")\n",
    "\n",
    "def load_data(path: str) -> tuple[pd.DataFrame, str, dict]:\n",
    "    path = str(path)\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Features file not found: {path}\")\n",
    "\n",
    "    df0 = pd.read_parquet(path, columns=None)\n",
    "    y_col = _detect_y_col(df0)\n",
    "\n",
    "    cols = [c for c in NEEDED_COLS if c in df0.columns]\n",
    "    if y_col not in cols: cols.append(y_col)\n",
    "    if \"x_norm\" not in cols: cols.append(\"x_norm\")\n",
    "    if \"t_sec\" not in cols: cols.append(\"t_sec\")\n",
    "    if \"game_id\" not in cols: cols.append(\"game_id\")\n",
    "    if \"play_uuid\" not in cols: cols.append(\"play_uuid\")\n",
    "    if \"player_id\" not in cols: cols.append(\"player_id\")\n",
    "\n",
    "    df = df0[cols].copy()\n",
    "    del df0\n",
    "\n",
    "    for cat in [\"player_id\",\"player_name\",\"position_code\",\"team_id\",\"game_id\",\n",
    "                \"home_abbr\",\"away_abbr\",\"play_type\",\"play_uuid\"]:\n",
    "        if cat in df.columns:\n",
    "            df[cat] = df[cat].astype(\"category\")\n",
    "\n",
    "    # --- Essentials prüfen + speed on-load erzeugen (1 Hz) ---\n",
    "    need = {\"game_id\",\"play_uuid\",\"player_id\",\"t_sec\",\"x_norm\",y_col}\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Fehlende Kernspalten im FEATURES_PATH: {missing}\")\n",
    "\n",
    "    # Sort für stabile Diff-Berechnung (mergesort bewahrt Reihenfolge bei Ties)\n",
    "    df = df.sort_values([\"game_id\",\"play_uuid\",\"player_id\",\"t_sec\"], kind=\"mergesort\")\n",
    "\n",
    "    # dx/dy/speed/heading sicherstellen oder neu berechnen, falls NaNs\n",
    "    grp = [\"game_id\",\"play_uuid\",\"player_id\"]\n",
    "    need_recalc_dx = (\"dx\" not in df.columns) or df[\"dx\"].isna().any()\n",
    "    need_recalc_dy = (\"dy\" not in df.columns) or df[\"dy\"].isna().any()\n",
    "    need_recalc_sp = (\"speed\" not in df.columns) or df[\"speed\"].isna().any()\n",
    "\n",
    "    if need_recalc_dx:\n",
    "        df[\"dx\"] = df.groupby(grp, observed=True)[\"x_norm\"].diff().fillna(0.0)\n",
    "    if need_recalc_dy:\n",
    "        df[\"dy\"] = df.groupby(grp, observed=True)[y_col].diff().fillna(0.0)\n",
    "    if need_recalc_sp or need_recalc_dx or need_recalc_dy:\n",
    "        # 1 Hz → Betrag der Schrittänderung in yd/s\n",
    "        df[\"speed\"] = np.hypot(df[\"dx\"], df[\"dy\"]).astype(float)\n",
    "\n",
    "    if (\"heading_deg\" not in df.columns) or df[\"heading_deg\"].isna().any():\n",
    "        # atan2(dy, dx) in Grad\n",
    "        df[\"heading_deg\"] = np.degrees(np.arctan2(df[\"dy\"], df[\"dx\"]).astype(float))\n",
    "        df[\"heading_deg\"] = df[\"heading_deg\"].fillna(0.0)\n",
    "\n",
    "    # schöne Game-Labels\n",
    "    game_labels = {}\n",
    "    if {\"game_id\",\"home_abbr\",\"away_abbr\"}.issubset(df.columns):\n",
    "        gmeta = df.groupby(\"game_id\", observed=True)[[\"home_abbr\",\"away_abbr\"]].first()\n",
    "        for gid, row in gmeta.iterrows():\n",
    "            game_labels[gid] = f\"{row['home_abbr']} vs {row['away_abbr']}  •  {gid}\"\n",
    "\n",
    "    return df, y_col, game_labels\n",
    "\n",
    "DF, Y_COL, GAME_LABELS = load_data(FEATURES_PATH)\n",
    "DF[\"play_uuid_str\"] = DF[\"play_uuid\"].astype(str)\n",
    "\n",
    "def opt(lst): return [{\"label\": str(v), \"value\": v} for v in lst]\n",
    "\n",
    "positions_all = sorted(map(str, DF[\"position_code\"].dropna().unique().tolist())) if \"position_code\" in DF else []\n",
    "playtypes_all = sorted(map(str, DF[\"play_type\"].dropna().unique().tolist()))    if \"play_type\" in DF else []\n",
    "players_all   = DF[\"player_name\"].dropna().value_counts().head(200).index.tolist() if \"player_name\" in DF else []\n",
    "games_all     = DF[\"game_id\"].dropna().unique().tolist() if \"game_id\" in DF else []\n",
    "\n",
    "# ========================\n",
    "# PLAY-FEATURES & CLUSTER (additiv)\n",
    "# ========================\n",
    "def make_features_from_timeseries(df: pd.DataFrame, id_col=\"play_uuid\", y_col=\"y\"):\n",
    "    feats = []\n",
    "    for pid, g in df.groupby(id_col, observed=True):\n",
    "        # stabil sortieren (Zeit)\n",
    "        gg = g.sort_values(\"t_sec\", kind=\"mergesort\")\n",
    "        row = {id_col: pid, \"n_samples\": int(len(gg))}\n",
    "        for col in [\"x_norm\", y_col, \"speed\"]:\n",
    "            if col in gg.columns:\n",
    "                vals = gg[col].to_numpy(float)\n",
    "                med  = float(np.nanmedian(vals))\n",
    "                mad  = float(np.nanmedian(np.abs(vals - med)))\n",
    "                q75, q25 = np.nanpercentile(vals, 75), np.nanpercentile(vals, 25)\n",
    "                iqr  = float(q75 - q25)\n",
    "                slope = float(np.polyfit(np.arange(len(vals)), vals, 1)[0]) if len(vals) >= 3 else 0.0\n",
    "                base = col if col != y_col else \"y\"\n",
    "                row[f\"{base}_med\"] = med\n",
    "                row[f\"{base}_mad\"] = mad\n",
    "                row[f\"{base}_iqr\"] = iqr\n",
    "                row[f\"{base}_trend_lr\"] = slope\n",
    "        feats.append(row)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# 1) Pro-Play-Features bauen\n",
    "FEATURES = make_features_from_timeseries(DF, id_col=\"play_uuid\", y_col=Y_COL)\n",
    "\n",
    "# 2) Cluster fitten (Standardisierung + PCA + k per Silhouette)\n",
    "def cluster_fit_add_labels(FEATURES: pd.DataFrame, id_col=\"play_uuid\", use_pca=True, pca_var=0.90):\n",
    "    # Feature-Auswahl: numerisch, ohne ID/Counts/Labels\n",
    "    drop = {'cl_kmeans','cl_agg','cl_dbscan',id_col,'n_samples'}\n",
    "    feature_cols = [c for c in FEATURES.select_dtypes(include=[np.number]).columns if c not in drop]\n",
    "    if len(feature_cols) < 2:\n",
    "        raise ValueError(\"Zu wenig numerische Feature-Spalten für Clustering.\")\n",
    "    X = FEATURES[feature_cols].astype(float).fillna(FEATURES[feature_cols].median(numeric_only=True))\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=pca_var, svd_solver=\"full\", random_state=0)\n",
    "        Xc = pca.fit_transform(Xs)\n",
    "    else:\n",
    "        pca, Xc = None, Xs\n",
    "\n",
    "    # k per Silhouette (2..8)\n",
    "    best = (-np.inf, None, None)\n",
    "    for k in range(2, 9):\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "        lab = km.fit_predict(Xc)\n",
    "        sil = silhouette_score(Xc, lab) if len(set(lab)) > 1 else -np.inf\n",
    "        if sil > best[0]:\n",
    "            best = (sil, k, km)\n",
    "    sil, k_best, km_best = best\n",
    "    labs_km = km_best.predict(Xc)\n",
    "\n",
    "    # Agglomerativ (Ward) @ k_best\n",
    "    agg = AgglomerativeClustering(n_clusters=k_best, linkage=\"ward\")\n",
    "    labs_agg = agg.fit_predict(Xc)\n",
    "\n",
    "    # Labels additiv anhängen\n",
    "    OUT = FEATURES.copy()\n",
    "    OUT[\"cl_kmeans\"] = labs_km\n",
    "    OUT[\"cl_agg\"] = labs_agg\n",
    "    meta = {\"k_best\": int(k_best), \"silhouette\": float(sil), \"n_features\": len(feature_cols), \"pca_used\": bool(use_pca), \"pca_var\": float(pca_var)}\n",
    "    return OUT, meta\n",
    "\n",
    "FEATURES, CLUSTER_META = cluster_fit_add_labels(FEATURES, id_col=\"play_uuid\", use_pca=True, pca_var=0.90)\n",
    "\n",
    "# 3) Labels zurück auf DF mergen (additiv, verändert nichts)\n",
    "DF = DF.merge(FEATURES[[\"play_uuid\",\"cl_kmeans\",\"cl_agg\"]], on=\"play_uuid\", how=\"left\")\n",
    "\n",
    "# Cluster-Options\n",
    "clusters_all = sorted(pd.Series(DF[\"cl_kmeans\"].dropna().unique()).astype(int).tolist()) if \"cl_kmeans\" in DF else []\n",
    "\n",
    "# ========================\n",
    "# APP & CACHE\n",
    "# ========================\n",
    "app = Dash(__name__)\n",
    "app.title = \"Football RA • CRP • RQA Dashboard\"\n",
    "cache = Cache(app.server, config={\"CACHE_TYPE\": \"SimpleCache\", \"CACHE_DEFAULT_TIMEOUT\": 300})\n",
    "\n",
    "def _key(x):\n",
    "    if x is None: return \"Ø\"\n",
    "    if isinstance(x, (list, tuple)): return tuple(x)\n",
    "    return x\n",
    "\n",
    "@cache.memoize()\n",
    "def filtered_df_cache(positions, playtypes, players, games, clusters, t0, t1, cols_tuple):\n",
    "    q = DF\n",
    "    if positions and \"position_code\" in q:\n",
    "        q = q[q[\"position_code\"].isin(positions)]\n",
    "    if playtypes and \"play_type\" in q:\n",
    "        q = q[q[\"play_type\"].isin(playtypes)]\n",
    "    if players and \"player_name\" in q:\n",
    "        q = q[q[\"player_name\"].isin(players)]\n",
    "    if games and \"game_id\" in q:\n",
    "        q = q[q[\"game_id\"].isin(games)]\n",
    "    if clusters is not None and len(clusters) > 0 and \"cl_kmeans\" in q:\n",
    "        q = q[q[\"cl_kmeans\"].isin(clusters)]\n",
    "    if \"t_sec\" in q:\n",
    "        q = q[(q[\"t_sec\"] >= t0) & (q[\"t_sec\"] <= t1)]\n",
    "    cols = [c for c in list(cols_tuple) if c in q.columns]\n",
    "    return q[cols].copy()\n",
    "\n",
    "def filtered_df(sel, cols):\n",
    "    return filtered_df_cache(\n",
    "        _key(sel.get(\"positions\")), _key(sel.get(\"playtypes\")),\n",
    "        _key(sel.get(\"players\")), _key(sel.get(\"games\")),\n",
    "        _key(sel.get(\"clusters\")),\n",
    "        sel.get(\"t_range\", (0,3))[0], sel.get(\"t_range\", (0,3))[1],\n",
    "        tuple(cols),\n",
    "    )\n",
    "\n",
    "def valid_options_from(df):\n",
    "    return (\n",
    "        sorted(map(str, df[\"position_code\"].dropna().unique().tolist())) if \"position_code\" in df else [],\n",
    "        sorted(map(str, df[\"play_type\"].dropna().unique().tolist()))     if \"play_type\" in df else [],\n",
    "        sorted(map(str, df[\"player_name\"].dropna().unique().tolist()))   if \"player_name\" in df else [],\n",
    "        df[\"game_id\"].dropna().unique().tolist()                         if \"game_id\" in df else [],\n",
    "    )\n",
    "\n",
    "# ========================\n",
    "# LAYOUT\n",
    "# ========================\n",
    "controls = html.Div([\n",
    "    html.Div([html.Label(\"Position(en)\"),\n",
    "              dcc.Dropdown(id=\"positions\", options=opt(positions_all), multi=True,\n",
    "                           placeholder=\"z. B. WR, DB …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":220,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Play-Typ(en)\"),\n",
    "              dcc.Dropdown(id=\"play_types\", options=opt(playtypes_all), multi=True,\n",
    "                           placeholder=\"z. B. Pass, Rush …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":220,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Spieler\"),\n",
    "              dcc.Dropdown(id=\"players\", options=opt(players_all), multi=True,\n",
    "                           placeholder=\"Spieler wählen …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":260,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Spiele\"),\n",
    "              dcc.Dropdown(id=\"games\",\n",
    "                           options=[{\"label\": GAME_LABELS.get(g, str(g)), \"value\": g} for g in games_all],\n",
    "                           multi=True, placeholder=\"Optional Spiele …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":260,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Cluster (K-Means)\"),\n",
    "              dcc.Dropdown(id=\"clusters\",\n",
    "                           options=[{\"label\": \"alle\", \"value\": \"__ALL__\"}] + [{\"label\": str(c), \"value\": int(c)} for c in clusters_all],\n",
    "                           multi=True, placeholder=\"Cluster wählen …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":220,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"t_sec\"),\n",
    "              dcc.RangeSlider(id=\"t_range\", min=0, max=3, step=1, value=[0,3],\n",
    "                              marks={i:str(i) for i in range(4)}, updatemode=\"mouseup\")],\n",
    "             style={\"flex\":1,\"minWidth\":220}),\n",
    "], style={\"display\":\"flex\",\"flexWrap\":\"wrap\",\"gap\":8,\"alignItems\":\"flex-end\",\"marginBottom\":10})\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Football RA • CRP • RQA Dashboard\"),\n",
    "    html.Div([html.Span(\"Daten: \"), html.Code(Path(FEATURES_PATH).name),\n",
    "              html.Span(f\"  | Zeilen: {len(DF):,}\", style={\"opacity\": .7, \"marginLeft\": 10})],\n",
    "             style={\"marginBottom\": 8}),\n",
    "    controls,\n",
    "    dcc.Tabs(id=\"tabs\", value=\"tab-overview\", children=[\n",
    "        dcc.Tab(label=\"Overview\", value=\"tab-overview\", children=[\n",
    "            html.Div(id=\"kpi-row\", style={\"display\":\"flex\",\"gap\":12,\"flexWrap\":\"wrap\",\"marginBottom\":8}),\n",
    "            dcc.Graph(id=\"heatmap_xy\",     style={\"height\":\"420px\"}),\n",
    "            dcc.Graph(id=\"profile_means\",  style={\"height\":\"340px\"}),\n",
    "            dcc.Graph(id=\"speed_hist\",     style={\"height\":\"300px\"}),\n",
    "        ]),\n",
    "        dcc.Tab(label=\"CRP (Offense vs Defense)\", value=\"tab-crp\", children=[\n",
    "            dcc.Graph(id=\"crp_chart\", style={\"height\":\"420px\",\"marginTop\":\"10px\"}),\n",
    "            dash_table.DataTable(id=\"crp_table\", page_size=10,\n",
    "                                 style_table={\"overflowX\":\"auto\"},\n",
    "                                 style_cell={\"padding\":\"6px\",\"fontFamily\":\"monospace\",\"fontSize\":12}),\n",
    "        ]),\n",
    "        dcc.Tab(label=\"RQA (pro Spiel – illustrativ)\", value=\"tab-rqa\", children=[\n",
    "            html.Div([\n",
    "                html.Div([html.Label(\"Ziel-Recurrence Rate (RR)\"),\n",
    "                          dcc.Slider(id=\"rqa_target_rr\", min=0.02, max=0.15, step=0.005, value=RQA_DEFAULT_RR,\n",
    "                                     marks={0.05:\"5%\",0.1:\"10%\",0.125:\"12.5%\",0.15:\"15%\"})],\n",
    "                         style={\"minWidth\":280,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"l_min\"),\n",
    "                          dcc.Dropdown(id=\"rqa_lmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4]],\n",
    "                                       value=RQA_DEFAULT_LMIN, clearable=False)],\n",
    "                         style={\"width\":220,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"v_min\"),\n",
    "                          dcc.Dropdown(id=\"rqa_vmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4]],\n",
    "                                       value=RQA_DEFAULT_VMIN, clearable=False)],\n",
    "                         style={\"width\":220,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\" \"), html.Button(\"RQA berechnen\", id=\"rqa_compute\", n_clicks=0, style={\"width\":\"200px\",\"height\":\"38px\"})]),\n",
    "            ], style={\"display\":\"flex\",\"flexWrap\":\"wrap\",\"alignItems\":\"flex-end\",\"gap\":8,\"marginBottom\":8}),\n",
    "            html.Div(id=\"rqa_plots_grid\",\n",
    "                     style={\"display\":\"grid\",\"gridTemplateColumns\":\"repeat(auto-fit, minmax(260px, 1fr))\",\"gap\":\"12px\"}),\n",
    "            dash_table.DataTable(id=\"rqa_table\", page_size=10,\n",
    "                                 style_table={\"overflowX\":\"auto\"},\n",
    "                                 style_cell={\"padding\":\"6px\",\"fontFamily\":\"monospace\",\"fontSize\":12}),\n",
    "            html.Div(id=\"rqa_note\", style={\"opacity\":.7,\"marginTop\":6})\n",
    "        ]),\n",
    "        dcc.Tab(label=\"RQA (klassisch • komplette Serie)\", value=\"tab-rqa-classic\", children=[\n",
    "            html.Div([\n",
    "                html.Div([html.Label(\"RR-Modus\"),\n",
    "                          dcc.RadioItems(\n",
    "                              id=\"rqac_rr_mode\",\n",
    "                              options=[\n",
    "                                  {\"label\":\"Dynamisch (Slider)\", \"value\":\"dynamic\"},\n",
    "                                  {\"label\":\"Vorgefertigt (5% / 10% / 15%)\", \"value\":\"preset\"},\n",
    "                              ],\n",
    "                              value=\"dynamic\",\n",
    "                              inline=True\n",
    "                          )],\n",
    "                         style={\"minWidth\":360,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"Ziel-RR (dynamisch)\"),\n",
    "                          dcc.Slider(id=\"rqac_target_rr\", min=0.02, max=0.15, step=0.005, value=RQA_DEFAULT_RR,\n",
    "                                     marks={0.05:\"5%\",0.10:\"10%\",0.125:\"12.5%\",0.15:\"15%\"})],\n",
    "                         style={\"minWidth\":280,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"RR (vorgefertigt)\"),\n",
    "                          dcc.Dropdown(id=\"rqac_rr_preset\",\n",
    "                                       options=[{\"label\":\"5%\",\"value\":0.05},\n",
    "                                                {\"label\":\"10%\",\"value\":0.10},\n",
    "                                                {\"label\":\"15%\",\"value\":0.15}],\n",
    "                                       value=RQA_DEFAULT_RR, clearable=False)],\n",
    "                         style={\"width\":180,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"l_min\"),\n",
    "                          dcc.Dropdown(id=\"rqac_lmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4,5]],\n",
    "                                       value=RQA_DEFAULT_LMIN, clearable=False)],\n",
    "                         style={\"width\":160,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"v_min\"),\n",
    "                          dcc.Dropdown(id=\"rqac_vmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4,5]],\n",
    "                                       value=RQA_DEFAULT_VMIN, clearable=False)],\n",
    "                         style={\"width\":160,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"Decimation (jeder k-te Punkt)\"),\n",
    "                          dcc.Input(id=\"rqac_decim\", type=\"number\", min=1, step=1, value=RQA_CLASSIC_DEFAULT_DECIM, style={\"width\":\"120px\"})],\n",
    "                         style={\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"Max. Punkte (Cap)\"),\n",
    "                          dcc.Input(id=\"rqac_maxpts\", type=\"number\", min=200, step=100, value=RQA_CLASSIC_DEFAULT_MAXPTS, style={\"width\":\"140px\"})],\n",
    "                         style={\"marginRight\":16}),\n",
    "                html.Div([html.Label(\" \"), html.Button(\"Klassische RQA berechnen\", id=\"rqac_compute\", n_clicks=0, style={\"width\":\"240px\",\"height\":\"38px\"})]),\n",
    "            ], style={\"display\":\"flex\",\"flexWrap\":\"wrap\",\"alignItems\":\"flex-end\",\"gap\":8,\"marginBottom\":8}),\n",
    "            dcc.Graph(id=\"rqac_plot\", style={\"height\":\"560px\"}),\n",
    "            dash_table.DataTable(id=\"rqac_table\", page_size=5,\n",
    "                                 style_table={\"overflowX\":\"auto\"},\n",
    "                                 style_cell={\"padding\":\"6px\",\"fontFamily\":\"monospace\",\"fontSize\":12}),\n",
    "            html.Div(id=\"rqac_note\", style={\"opacity\":.7,\"marginTop\":6})\n",
    "        ]),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# FILTER-SYNC (entkoppelt, stabil)\n",
    "# ========================\n",
    "def _options_union_keep_selected(all_values, filtered_values, selected_values):\n",
    "    sel_set = set(map(str, selected_values or []))\n",
    "    vals = set(map(str, filtered_values or [])) | sel_set\n",
    "    if not vals:\n",
    "        vals = set(map(str, all_values or [])) | sel_set\n",
    "    return opt(sorted(vals))\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"positions\",\"options\"), Output(\"play_types\",\"options\"),\n",
    "    Output(\"players\",\"options\"),  Output(\"games\",\"options\"),\n",
    "    Input(\"positions\",\"value\"),   Input(\"play_types\",\"value\"),\n",
    "    Input(\"players\",\"value\"),     Input(\"games\",\"value\"),\n",
    ")\n",
    "def sync_filters(pos_v, pt_v, pl_v, gm_v):\n",
    "    sel_for_pos = dict(positions=[], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [], clusters=[], t_range=(0,3))\n",
    "    q_pos = filtered_df(sel_for_pos, [\"position_code\"])\n",
    "    pos_vals = q_pos[\"position_code\"].dropna().unique().tolist() if \"position_code\" in q_pos else positions_all\n",
    "    pos_opts = _options_union_keep_selected(positions_all, pos_vals, pos_v)\n",
    "\n",
    "    sel_for_pt  = dict(positions=pos_v or [], playtypes=[], players=pl_v or [], games=gm_v or [], clusters=[], t_range=(0,3))\n",
    "    q_pt = filtered_df(sel_for_pt, [\"play_type\"])\n",
    "    pt_vals = q_pt[\"play_type\"].dropna().unique().tolist() if \"play_type\" in q_pt else playtypes_all\n",
    "    pt_opts = _options_union_keep_selected(playtypes_all, pt_vals, pt_v)\n",
    "\n",
    "    sel_for_pl  = dict(positions=pos_v or [], playtypes=pt_v or [], players=[], games=gm_v or [], clusters=[], t_range=(0,3))\n",
    "    q_pl = filtered_df(sel_for_pl, [\"player_name\"])\n",
    "    pl_vals = q_pl[\"player_name\"].dropna().unique().tolist() if \"player_name\" in q_pl else players_all\n",
    "    pl_opts = _options_union_keep_selected(players_all, pl_vals, pl_v)\n",
    "\n",
    "    sel_for_gm  = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=[], clusters=[], t_range=(0,3))\n",
    "    q_gm = filtered_df(sel_for_gm, [\"game_id\"])\n",
    "    gm_vals = q_gm[\"game_id\"].dropna().unique().tolist() if \"game_id\" in q_gm else games_all\n",
    "    gm_opts = [{\"label\": GAME_LABELS.get(g, str(g)), \"value\": g} for g in sorted(gm_vals, key=lambda x: str(x))]\n",
    "\n",
    "    return pos_opts, pt_opts, pl_opts, gm_opts\n",
    "\n",
    "# ========================\n",
    "# OVERVIEW CALLBACK\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"kpi-row\",\"children\"), Output(\"heatmap_xy\",\"figure\"),\n",
    "    Output(\"profile_means\",\"figure\"), Output(\"speed_hist\",\"figure\"),\n",
    "    Input(\"positions\",\"value\"), Input(\"play_types\",\"value\"),\n",
    "    Input(\"players\",\"value\"), Input(\"games\",\"value\"), Input(\"clusters\",\"value\"), Input(\"t_range\",\"value\"),\n",
    ")\n",
    "def update_overview(pos_v, pt_v, pl_v, gm_v, cl_v, tr_v):\n",
    "    # Cluster-Value normalisieren\n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [], clusters=cl_v_norm, t_range=tuple(tr_v or (0,3)))\n",
    "\n",
    "    q = filtered_df(sel, [\"play_uuid\",\"player_id\",\"game_id\",\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"cl_kmeans\"])\n",
    "    def k(label, val):\n",
    "        return html.Div([html.Div(label, style={\"fontSize\":12,\"opacity\":.7}),\n",
    "                         html.Div(f\"{val}\", style={\"fontSize\":22,\"fontWeight\":600})],\n",
    "                        style={\"padding\":\"8px 12px\",\"border\":\"1px solid #eee\",\"borderRadius\":8,\"minWidth\":140})\n",
    "    kpis = [k(\"Zeilen\", f\"{len(q):,}\"),\n",
    "            k(\"Unique Plays\", q[\"play_uuid\"].nunique()),\n",
    "            k(\"Unique Spieler\", q[\"player_id\"].nunique()),\n",
    "            k(\"Unique Spiele\", q[\"game_id\"].nunique())]\n",
    "    # Cluster-Meta\n",
    "    if CLUSTER_META:\n",
    "        kpis.append(k(\"K-Means k\", CLUSTER_META.get(\"k_best\", \"—\")))\n",
    "        kpis.append(k(\"Silhouette\", f\"{CLUSTER_META.get('silhouette', float('nan')):.3f}\"))\n",
    "\n",
    "    h = q\n",
    "    if len(h) > MAX_HEATMAP_POINTS: h = h.sample(MAX_HEATMAP_POINTS, random_state=42)\n",
    "    hm = px.density_heatmap(h, x=\"x_norm\", y=Y_COL, nbinsx=60, nbinsy=27, histnorm=\"\")\n",
    "    hm.update_layout(title=\"Dichte: x_norm vs. y (gesampelt)\")\n",
    "    hm.update_yaxes(scaleanchor=\"x\", scaleratio=53.33/120)\n",
    "\n",
    "    g = q.groupby(\"t_sec\").agg(mean_x=(\"x_norm\",\"mean\"), mean_y=(Y_COL,\"mean\"), mean_v=(\"speed\",\"mean\")).reset_index()\n",
    "    prof = go.Figure()\n",
    "    for col, name in [(\"mean_x\",\"mean x_norm\"),(\"mean_y\",\"mean y\"),(\"mean_v\",\"mean speed (yd/s)\")]:\n",
    "        prof.add_trace(go.Scatter(x=g[\"t_sec\"], y=g[col], mode=\"lines+markers\", name=name))\n",
    "    prof.update_layout(title=\"Mittelwerte je t_sec\", xaxis_title=\"t_sec\", yaxis_title=\"Wert\")\n",
    "\n",
    "    hist = px.histogram(q, x=\"speed\", nbins=SPEED_HIST_BINS, title=\"Geschwindigkeit (yd/s)\")\n",
    "    return kpis, hm, prof, hist\n",
    "\n",
    "# ========================\n",
    "# CRP CALLBACK (mit korrekter Off/Def-Trennung)\n",
    "# ========================\n",
    "def crp_off_vs_def(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Korrekte Trennung:\n",
    "      Offense = rows mit team_id == offense_team_id\n",
    "      Defense = rows mit team_id == defense_team_id\n",
    "    Aggregation: Mittelwerte je t_sec (x_norm, y, speed)\n",
    "    \"\"\"\n",
    "    needed = {\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"team_id\",\"offense_team_id\",\"defense_team_id\"}\n",
    "    if not needed.issubset(df.columns):\n",
    "        # Fallback: keine Trennung möglich\n",
    "        gg = df.groupby(\"t_sec\").agg(x=(\"x_norm\",\"mean\"), y=(Y_COL,\"mean\"), v=(\"speed\",\"mean\")).reset_index()\n",
    "        out = gg.rename(columns={\"x\":\"off_x\",\"y\":\"off_y\",\"v\":\"off_v\"})\n",
    "        out[\"def_x\"] = out[\"off_x\"]; out[\"def_y\"] = out[\"off_y\"]; out[\"def_v\"] = out[\"off_v\"]\n",
    "        out[\"dx\"] = 0.0; out[\"dy\"] = 0.0; out[\"dv\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    is_off = df[\"team_id\"].astype(\"Int64\") == df[\"offense_team_id\"].astype(\"Int64\")\n",
    "    is_def = df[\"team_id\"].astype(\"Int64\") == df[\"defense_team_id\"].astype(\"Int64\")\n",
    "\n",
    "    off = df[is_off]\n",
    "    de  = df[is_def]\n",
    "\n",
    "    g_off = off.groupby(\"t_sec\").agg(x=(\"x_norm\",\"mean\"), y=(Y_COL,\"mean\"), v=(\"speed\",\"mean\")).add_prefix(\"off_\").reset_index()\n",
    "    g_def = de .groupby(\"t_sec\").agg(x=(\"x_norm\",\"mean\"), y=(Y_COL,\"mean\"), v=(\"speed\",\"mean\")).add_prefix(\"def_\").reset_index()\n",
    "\n",
    "    out = pd.merge(g_off, g_def, on=\"t_sec\", how=\"outer\").sort_values(\"t_sec\")\n",
    "    out[[\"off_x\",\"off_y\",\"off_v\",\"def_x\",\"def_y\",\"def_v\"]] = out[\n",
    "        [\"off_x\",\"off_y\",\"off_v\",\"def_x\",\"def_y\",\"def_v\"]\n",
    "    ].ffill().bfill()\n",
    "\n",
    "    out[\"dx\"] = out[\"off_x\"] - out[\"def_x\"]\n",
    "    out[\"dy\"] = out[\"off_y\"] - out[\"def_y\"]\n",
    "    out[\"dv\"] = out[\"off_v\"] - out[\"def_v\"]\n",
    "    return out\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"crp_chart\",\"figure\"), Output(\"crp_table\",\"columns\"), Output(\"crp_table\",\"data\"),\n",
    "    Input(\"positions\",\"value\"), Input(\"play_types\",\"value\"),\n",
    "    Input(\"players\",\"value\"), Input(\"games\",\"value\"), Input(\"clusters\",\"value\"), Input(\"t_range\",\"value\"),\n",
    ")\n",
    "def update_crp(pos_v, pt_v, pl_v, gm_v, cl_v, tr_v):\n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [], clusters=cl_v_norm, t_range=tuple(tr_v or (0,3)))\n",
    "    q = filtered_df(sel, [\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"team_id\",\"offense_team_id\",\"defense_team_id\"])\n",
    "    comp = crp_off_vs_def(q)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for col, name in [(\"off_x\",\"Offense mean x\"),(\"def_x\",\"Defense mean x\"),(\"dx\",\"Δx (Off-Def)\")]:\n",
    "        fig.add_trace(go.Scatter(x=comp[\"t_sec\"], y=comp[col], mode=\"lines+markers\",\n",
    "                                 name=name, line=dict(dash=\"dash\") if col.startswith(\"d\") else None))\n",
    "    for col, name in [(\"off_v\",\"Offense mean v\"),(\"def_v\",\"Defense mean v\"),(\"dv\",\"Δv (Off-Def)\")]:\n",
    "        fig.add_trace(go.Scatter(x=comp[\"t_sec\"], y=comp[col], mode=\"lines+markers\",\n",
    "                                 name=name, line=dict(dash=\"dash\") if col.startswith(\"d\") else None))\n",
    "    fig.update_layout(title=\"CRP: Offense vs Defense (x & v)\", xaxis_title=\"t_sec\")\n",
    "\n",
    "    cols = [{\"name\": c, \"id\": c} for c in comp.columns]\n",
    "    data = comp.round(3).to_dict(\"records\")\n",
    "    return fig, cols, data\n",
    "\n",
    "# ========================\n",
    "# RQA HELFER\n",
    "# ========================\n",
    "def pairwise_dist(A: np.ndarray, w: np.ndarray):\n",
    "    \"\"\"Gewichtete euklidische Distanzmatrix für NxD-Array A mit Gewichten w (D,).\"\"\"\n",
    "    A = np.asarray(A, float)\n",
    "    w = np.asarray(w, float).reshape(1, -1)\n",
    "    diff = A[:, None, :] - A[None, :, :]\n",
    "    return np.sqrt((diff**2 * w).sum(axis=2))\n",
    "\n",
    "def recurrence_matrix(arr: np.ndarray, target_rr: float, w=(1.0,1.0,1.0), standardize: bool = True):\n",
    "    A = np.asarray(arr, float)\n",
    "    if standardize:\n",
    "        mu = A.mean(axis=0, keepdims=True)\n",
    "        sd = A.std(axis=0, keepdims=True) + 1e-9\n",
    "        A = (A - mu) / sd\n",
    "    w = np.asarray(w, float)\n",
    "    D = pairwise_dist(A, w)\n",
    "    tri = D[np.triu_indices_from(D, k=1)]\n",
    "    if len(tri) == 0:\n",
    "        eps = 0.0\n",
    "        return (D <= eps).astype(int), float(eps)\n",
    "    eps = float(np.quantile(tri, target_rr))\n",
    "    return (D <= eps).astype(int), eps\n",
    "\n",
    "def rqa_metrics(R: np.ndarray, l_min=2, v_min=2):\n",
    "    N = R.size\n",
    "    RR = R.sum() / N if N > 0 else 0.0\n",
    "\n",
    "    # Diagonale Linien\n",
    "    diag_lengths = []\n",
    "    for k in range(-(R.shape[0]-1), R.shape[0]):\n",
    "        d = np.diag(R, k)\n",
    "        if d.size == 0: continue\n",
    "        run = 0\n",
    "        for val in d:\n",
    "            if val == 1: run += 1\n",
    "            else:\n",
    "                if run >= l_min: diag_lengths.append(run)\n",
    "                run = 0\n",
    "        if run >= l_min: diag_lengths.append(run)\n",
    "\n",
    "    DET  = (sum(diag_lengths) / R.sum()) if R.sum() > 0 and diag_lengths else 0.0\n",
    "    Lmax = max(diag_lengths) if diag_lengths else 0\n",
    "    L    = float(np.mean(diag_lengths)) if diag_lengths else 0.0\n",
    "    if diag_lengths:\n",
    "        _, cnts = np.unique(diag_lengths, return_counts=True)\n",
    "        p = cnts / cnts.sum()\n",
    "        ENTR = float(-(p * np.log(p + 1e-12)).sum())\n",
    "    else:\n",
    "        ENTR = 0.0\n",
    "\n",
    "    # Vertikale Linien\n",
    "    vert_lengths = []\n",
    "    for j in range(R.shape[1]):\n",
    "        col = R[:, j]\n",
    "        run = 0\n",
    "        for val in col:\n",
    "            if val == 1: run += 1\n",
    "            else:\n",
    "                if run >= v_min: vert_lengths.append(run)\n",
    "                run = 0\n",
    "        if run >= v_min: vert_lengths.append(run)\n",
    "\n",
    "    LAM = (sum(vert_lengths) / R.sum()) if R.sum() > 0 and vert_lengths else 0.0\n",
    "    TT  = float(np.mean(vert_lengths)) if vert_lengths else 0.0\n",
    "    if vert_lengths:\n",
    "        _, cnts_v = np.unique(vert_lengths, return_counts=True)\n",
    "        p_v = cnts_v / cnts_v.sum()\n",
    "        ENTR_V = float(-(p_v * np.log(p_v + 1e-12)).sum())\n",
    "    else:\n",
    "        ENTR_V = 0.0\n",
    "\n",
    "    return dict(RR=RR, DET=DET, L=L, Lmax=Lmax, ENTR=ENTR, LAM=LAM, TT=TT, ENTR_V=ENTR_V)\n",
    "\n",
    "def game_traj(df_game: pd.DataFrame, y_col: str):\n",
    "    # 4 Punkte je Spiel (0..3s) – Mittelwerte über Spieler\n",
    "    g = df_game.groupby(\"t_sec\")[[\"x_norm\", y_col, \"speed\"]].mean().reindex([0,1,2,3])\n",
    "    g = g.ffill().bfill()\n",
    "    return g.to_numpy(float)\n",
    "\n",
    "def build_full_series(df: pd.DataFrame, y_col: str, decim: int, maxpts: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Durchgehende 1 Hz-Serie aus der aktuellen Auswahl:\n",
    "    sortiert nach game_id → play_uuid → t_sec, pro Zeitstempel Mittelwert über Spieler.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return np.empty((0, 3), float)\n",
    "\n",
    "    cols = [\"game_id\",\"play_uuid\",\"t_sec\",\"x_norm\",y_col,\"speed\"]\n",
    "    q = df[cols].copy().sort_values([\"game_id\",\"play_uuid\",\"t_sec\"])\n",
    "    q = q.groupby([\"game_id\",\"play_uuid\",\"t_sec\"], observed=True)[[\"x_norm\", y_col, \"speed\"]].mean().reset_index()\n",
    "\n",
    "    series = q[[\"x_norm\", y_col, \"speed\"]].to_numpy(float)\n",
    "    if decim is None or decim < 1: decim = 1\n",
    "    series = series[::decim]\n",
    "\n",
    "    if maxpts and series.shape[0] > maxpts:\n",
    "        series = series[:maxpts, :]\n",
    "\n",
    "    series = pd.DataFrame(series, columns=[\"x\",\"y\",\"v\"]).ffill().bfill().to_numpy(float)\n",
    "    return series\n",
    "\n",
    "# ========================\n",
    "# RQA CALLBACK (pro Spiel)\n",
    "# ========================\n",
    "# RQA pro Spiel\n",
    "@app.callback(\n",
    "    Output(\"rqa_plots_grid\",\"children\"),\n",
    "    Output(\"rqa_table\",\"columns\"),\n",
    "    Output(\"rqa_table\",\"data\"),\n",
    "    Output(\"rqa_note\",\"children\"),\n",
    "    Input(\"rqa_compute\",\"n_clicks\"),\n",
    "    State(\"positions\",\"value\"), State(\"play_types\",\"value\"),\n",
    "    State(\"players\",\"value\"),  State(\"games\",\"value\"),\n",
    "    State(\"t_range\",\"value\"),\n",
    "    State(\"rqa_target_rr\",\"value\"), State(\"rqa_lmin\",\"value\"), State(\"rqa_vmin\",\"value\"),\n",
    "    State(\"clusters\",\"value\"),                    \n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def compute_rqa(n_clicks, pos_v, pt_v, pl_v, gm_v, tr_v, target_rr, l_min, v_min, cl_v): \n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [],\n",
    "               games=gm_v or [], clusters=cl_v_norm, t_range=tuple(tr_v or (0,3)))\n",
    "    q = filtered_df(sel, [\"game_id\",\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"home_abbr\",\"away_abbr\",\"cl_kmeans\"])\n",
    "\n",
    "    if q.empty:\n",
    "        return [html.Div(\"Keine Daten für die aktuelle Auswahl.\", style={\"padding\":\"8px\"})], [], [], \"\"\n",
    "    games = list(q[\"game_id\"].dropna().unique())\n",
    "    games_show = games[:RQA_SHOW_MAX_GAMES]\n",
    "    plots, rows = [], []\n",
    "    for gid in games_show:\n",
    "        qg = q[q[\"game_id\"] == gid]\n",
    "        arr = game_traj(qg, Y_COL)\n",
    "        R, eps = recurrence_matrix(arr, target_rr=target_rr, w=RQA_FEATURE_WEIGHTS, standardize=RQA_STANDARDIZE)\n",
    "\n",
    "        mets = rqa_metrics(R, l_min=l_min, v_min=v_min)\n",
    "        title_txt = GAME_LABELS.get(gid, str(gid))\n",
    "        fig = px.imshow(R, origin=\"lower\", aspect=\"equal\",\n",
    "                        labels=dict(x=\"t (s)\", y=\"t (s)\"),\n",
    "                        color_continuous_scale=[\"#ffffff\", \"#000000\"])\n",
    "        fig.update_layout(title=f\"RQA — {title_txt}  (ε≈{eps:.3f}, RR≈{target_rr:.1%})\",\n",
    "                          margin=dict(l=30,r=10,t=46,b=30))\n",
    "        plots.append(dcc.Graph(figure=fig, style={\"height\":\"260px\"}))\n",
    "        rows.append({\n",
    "            \"game_id\": gid,\n",
    "            \"match\": GAME_LABELS.get(gid, str(gid)),\n",
    "            \"RR\": round(mets[\"RR\"], 4),\n",
    "            \"DET\": round(mets[\"DET\"], 4),\n",
    "            \"L\": round(mets[\"L\"], 3),\n",
    "            \"Lmax\": int(mets[\"Lmax\"]),\n",
    "            \"ENTR\": round(mets[\"ENTR\"], 3),\n",
    "            \"LAM\": round(mets[\"LAM\"], 4),\n",
    "            \"TT\": round(mets[\"TT\"], 3),\n",
    "            \"ENTR_V\": round(mets[\"ENTR_V\"], 3),\n",
    "            \"epsilon_auto\": round(eps, 4)\n",
    "        })\n",
    "    cols = [{\"name\": c, \"id\": c} for c in [\"game_id\",\"match\",\"RR\",\"DET\",\"L\",\"Lmax\",\"ENTR\",\"LAM\",\"TT\",\"ENTR_V\",\"epsilon_auto\"]]\n",
    "    note = (\n",
    "    f\"Es werden max. {RQA_SHOW_MAX_GAMES} Spiele visualisiert. \"\n",
    "    f\"Button gedrückt: {n_clicks}. \"\n",
    "    \"Hinweis: Die pro-Spiel-Ansicht ist explorativ/illustrativ (nur 4 Zeitpunkte @ 1 Hz).\"\n",
    "    )\n",
    "\n",
    "    return plots, cols, rows, note\n",
    "\n",
    "# ========================\n",
    "# RQA CALLBACK (klassisch • komplette Serie)\n",
    "# ========================\n",
    "# RQA klassisch\n",
    "@app.callback(\n",
    "    Output(\"rqac_plot\",\"figure\"),\n",
    "    Output(\"rqac_table\",\"columns\"),\n",
    "    Output(\"rqac_table\",\"data\"),\n",
    "    Output(\"rqac_note\",\"children\"),\n",
    "    Input(\"rqac_compute\",\"n_clicks\"),\n",
    "    State(\"positions\",\"value\"), State(\"play_types\",\"value\"),\n",
    "    State(\"players\",\"value\"),  State(\"games\",\"value\"),\n",
    "    State(\"t_range\",\"value\"),\n",
    "    State(\"rqac_rr_mode\",\"value\"),\n",
    "    State(\"rqac_target_rr\",\"value\"), State(\"rqac_rr_preset\",\"value\"),\n",
    "    State(\"rqac_lmin\",\"value\"), State(\"rqac_vmin\",\"value\"),\n",
    "    State(\"rqac_decim\",\"value\"), State(\"rqac_maxpts\",\"value\"),\n",
    "    State(\"clusters\",\"value\"),                     # ← NEU\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def compute_rqa_classic(n_clicks, pos_v, pt_v, pl_v, gm_v, tr_v,\n",
    "                        rr_mode, target_rr, rr_preset, l_min, v_min, decim, maxpts, cl_v):  # ← NEU\n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [],\n",
    "               games=gm_v or [], clusters=cl_v_norm, t_range=tuple(tr_v or (0,3)))\n",
    "    q = filtered_df(sel, [\"game_id\",\"play_uuid\",\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"cl_kmeans\"])\n",
    "    if q.empty:\n",
    "        return go.Figure().update_layout(title=\"Keine Daten für die aktuelle Auswahl.\"), [], [], \"\"\n",
    "    print(\"RQA classic — Cluster-Filter:\", cl_v_norm or \"alle\", \"| rows:\", len(q))  # Debug\n",
    "    arr = build_full_series(q, Y_COL, decim=decim or RQA_CLASSIC_DEFAULT_DECIM,\n",
    "                            maxpts=maxpts or RQA_CLASSIC_DEFAULT_MAXPTS)\n",
    "    ...\n",
    "\n",
    "    if q.empty:\n",
    "        empty_fig = go.Figure().update_layout(title=\"Keine Daten für die aktuelle Auswahl.\")\n",
    "        return empty_fig, [], [], \"\"\n",
    "\n",
    "    arr = build_full_series(q, Y_COL, decim=decim or RQA_CLASSIC_DEFAULT_DECIM, maxpts=maxpts or RQA_CLASSIC_DEFAULT_MAXPTS)\n",
    "    if arr.shape[0] < 2:\n",
    "        empty_fig = go.Figure().update_layout(title=\"Zu wenige Punkte für RQA.\")\n",
    "        return empty_fig, [], [], \"Hinweis: Serie hat < 2 Punkte.\"\n",
    "\n",
    "    rr_val = float(target_rr if rr_mode == \"dynamic\" else rr_preset)\n",
    "    R, eps = recurrence_matrix(arr, target_rr=rr_val, w=RQA_FEATURE_WEIGHTS, standardize=RQA_STANDARDIZE)\n",
    "    mets = rqa_metrics(R, l_min=int(l_min), v_min=int(v_min))\n",
    "\n",
    "    fig = px.imshow(R, origin=\"lower\", aspect=\"equal\",\n",
    "                    labels=dict(x=\"time in s\", y=\"time in s\"),\n",
    "                    color_continuous_scale=[\"#ffffff\", \"#000000\"])\n",
    "    fig.update_coloraxes(showscale=True)\n",
    "    fig.update_layout(\n",
    "        title=f\"Klassische RQA — komplette Serie (ε≈{eps:.3f}, RR≈{rr_val:.1%}, N={R.shape[0]})\",\n",
    "        margin=dict(l=40,r=20,t=60,b=40)\n",
    "    )\n",
    "\n",
    "    cols = [{\"name\": c, \"id\": c} for c in [\"N\",\"RR\",\"DET\",\"L\",\"Lmax\",\"ENTR\",\"LAM\",\"TT\",\"ENTR_V\",\"epsilon_auto\",\"rr_mode\",\"decim\",\"maxpts\"]]\n",
    "    data = [{\n",
    "        \"N\": int(R.shape[0]),\n",
    "        \"RR\": round(mets[\"RR\"], 4),\n",
    "        \"DET\": round(mets[\"DET\"], 4),\n",
    "        \"L\": round(mets[\"L\"], 3),\n",
    "        \"Lmax\": int(mets[\"Lmax\"]),\n",
    "        \"ENTR\": round(mets[\"ENTR\"], 3),\n",
    "        \"LAM\": round(mets[\"LAM\"], 4),\n",
    "        \"TT\": round(mets[\"TT\"], 3),\n",
    "        \"ENTR_V\": round(mets[\"ENTR_V\"], 3),\n",
    "        \"epsilon_auto\": round(eps, 4),\n",
    "        \"rr_mode\": rr_mode,\n",
    "        \"decim\": int(decim or RQA_CLASSIC_DEFAULT_DECIM),\n",
    "        \"maxpts\": int(maxpts or RQA_CLASSIC_DEFAULT_MAXPTS),\n",
    "    }]\n",
    "\n",
    "    note = (\"Serie wird aus der aktuellen Filterauswahl gebildet: geordnete Folge aller ausgewählten Plays (1 Hz), \"\n",
    "            \"pro Zeitstempel Mittelwert über gefilterte Spieler. RR kann dynamisch (Slider) \"\n",
    "            \"oder als vorgefertigte Variante (5 % / 10 % / 15 %) gewählt werden. \"\n",
    "            \"‚Decimation‘ reduziert die Länge (jeder k-te Punkt), ‚Max. Punkte‘ deckelt die Kantenlänge der Matrix.\")\n",
    "    return fig, cols, data, note\n",
    "\n",
    "# ========================\n",
    "# MAIN\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n Loaded:\")\n",
    "    print(\"  path:\", FEATURES_PATH)\n",
    "    print(\"  DF shape:\", DF.shape)\n",
    "    print(\"  DF columns:\", DF.columns.tolist())\n",
    "    print(\"  has speed?\", (\"speed\" in DF.columns) and not pd.isna(DF[\"speed\"]).any())\n",
    "    print(\"\\nClustering:\")\n",
    "    print(\"  PCA used:\", CLUSTER_META.get(\"pca_used\"))\n",
    "    print(\"  PCA variance retained:\", CLUSTER_META.get(\"pca_var\"))\n",
    "    print(\"  n_features (for clustering):\", CLUSTER_META.get(\"n_features\"))\n",
    "    print(\"  k_best:\", CLUSTER_META.get(\"k_best\"))\n",
    "    print(\"  silhouette:\", f\"{CLUSTER_META.get('silhouette'):.3f}\")\n",
    "    app.run(debug=False, port=int(os.getenv(\"PORT\", 8050)), use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a9a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Address already in use\n",
      "Port 8050 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunahansari/football_ra/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning:\n",
      "\n",
      "To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output, State, dash_table\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from flask_caching import Cache\n",
    "\n",
    "# Clustering\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ========================\n",
    "# CONFIG\n",
    "# ========================\n",
    "DEFAULT_FEATURES_PATH = \"out_1hz_clean/master_1hz_4s_ready.parquet\"\n",
    "FEATURES_PATH = os.getenv(\"FEATURES_PATH\", DEFAULT_FEATURES_PATH)\n",
    "\n",
    "MAX_HEATMAP_POINTS = int(os.getenv(\"MAX_HEATMAP_POINTS\", 60_000))\n",
    "SPEED_HIST_BINS    = 36\n",
    "RQA_SHOW_MAX_GAMES = 6\n",
    "\n",
    "# RQA Defaults\n",
    "RQA_DEFAULT_RR   = 0.10  # 10 %\n",
    "RQA_DEFAULT_LMIN = 2\n",
    "RQA_DEFAULT_VMIN = 2\n",
    "\n",
    "# RQA – Feature-Gewichte & Standardisierung\n",
    "RQA_FEATURE_WEIGHTS = (1.0, 1.0, 0.6)  # speed etwas geringer gewichten\n",
    "RQA_STANDARDIZE     = True             # z-Score je Achse vor Distanz\n",
    "\n",
    "# Limits für klassische RQA (Performance)\n",
    "RQA_CLASSIC_DEFAULT_MAXPTS = int(os.getenv(\"RQA_CLASSIC_MAXPTS\", 3000))  # Cap auf Matrix-Kantenlänge\n",
    "RQA_CLASSIC_DEFAULT_DECIM  = int(os.getenv(\"RQA_CLASSIC_DECIM\", 1))      # jeden k-ten Punkt\n",
    "\n",
    "# ========================\n",
    "# DATA LOADING\n",
    "# ========================\n",
    "NEEDED_COLS = [\n",
    "    \"play_uuid\",\"player_id\",\"t_sec\",\"x_norm\",\"y\",\n",
    "    \"player_name\",\"position_code\",\"team_id\",\"game_id\",\n",
    "    \"home_abbr\",\"away_abbr\",\"offense_team_id\",\"defense_team_id\",\n",
    "    \"play_quarter\",\"play_down\",\"play_yards_to_go\",\"play_type\",\n",
    "    \"dx\",\"dy\",\"speed\",\"heading_deg\"\n",
    "]\n",
    "\n",
    "def _detect_y_col(df: pd.DataFrame) -> str:\n",
    "    if \"y\" in df.columns: return \"y\"\n",
    "    if \"step_y\" in df.columns: return \"step_y\"\n",
    "    raise KeyError(\"Neither 'y' nor 'step_y' found.\")\n",
    "\n",
    "def load_data(path: str) -> tuple[pd.DataFrame, str, dict]:\n",
    "    path = str(path)\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Features file not found: {path}\")\n",
    "\n",
    "    df0 = pd.read_parquet(path, columns=None)\n",
    "    y_col = _detect_y_col(df0)\n",
    "\n",
    "    cols = [c for c in NEEDED_COLS if c in df0.columns]\n",
    "    if y_col not in cols: cols.append(y_col)\n",
    "    if \"x_norm\" not in cols: cols.append(\"x_norm\")\n",
    "    if \"t_sec\" not in cols: cols.append(\"t_sec\")\n",
    "    if \"game_id\" not in cols: cols.append(\"game_id\")\n",
    "    if \"play_uuid\" not in cols: cols.append(\"play_uuid\")\n",
    "    if \"player_id\" not in cols: cols.append(\"player_id\")\n",
    "\n",
    "    df = df0[cols].copy()\n",
    "    del df0\n",
    "\n",
    "    for cat in [\"player_id\",\"player_name\",\"position_code\",\"team_id\",\"game_id\",\n",
    "                \"home_abbr\",\"away_abbr\",\"play_type\",\"play_uuid\"]:\n",
    "        if cat in df.columns:\n",
    "            df[cat] = df[cat].astype(\"category\")\n",
    "\n",
    "    need = {\"game_id\",\"play_uuid\",\"player_id\",\"t_sec\",\"x_norm\",y_col}\n",
    "    missing = [c for c in need if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Fehlende Kernspalten im FEATURES_PATH: {missing}\")\n",
    "\n",
    "    # Sort für stabile Diff-Berechnung (mergesort bewahrt Reihenfolge bei Ties)\n",
    "    df = df.sort_values([\"game_id\",\"play_uuid\",\"player_id\",\"t_sec\"], kind=\"mergesort\")\n",
    "\n",
    "    # dx/dy/speed/heading sicherstellen oder neu berechnen, falls NaNs\n",
    "    grp = [\"game_id\",\"play_uuid\",\"player_id\"]\n",
    "    need_recalc_dx = (\"dx\" not in df.columns) or df[\"dx\"].isna().any()\n",
    "    need_recalc_dy = (\"dy\" not in df.columns) or df[\"dy\"].isna().any()\n",
    "    need_recalc_sp = (\"speed\" not in df.columns) or df[\"speed\"].isna().any()\n",
    "\n",
    "    if need_recalc_dx:\n",
    "        df[\"dx\"] = df.groupby(grp, observed=True)[\"x_norm\"].diff().fillna(0.0)\n",
    "    if need_recalc_dy:\n",
    "        df[\"dy\"] = df.groupby(grp, observed=True)[y_col].diff().fillna(0.0)\n",
    "    if need_recalc_sp or need_recalc_dx or need_recalc_dy:\n",
    "        # 1 Hz → Betrag der Schrittänderung in yd/s\n",
    "        df[\"speed\"] = np.hypot(df[\"dx\"], df[\"dy\"]).astype(float)\n",
    "\n",
    "    if (\"heading_deg\" not in df.columns) or df[\"heading_deg\"].isna().any():\n",
    "        # atan2(dy, dx) in Grad\n",
    "        df[\"heading_deg\"] = np.degrees(np.arctan2(df[\"dy\"], df[\"dx\"]).astype(float))\n",
    "        df[\"heading_deg\"] = df[\"heading_deg\"].fillna(0.0)\n",
    "\n",
    "    # schöne Game-Labels\n",
    "    game_labels = {}\n",
    "    if {\"game_id\",\"home_abbr\",\"away_abbr\"}.issubset(df.columns):\n",
    "        gmeta = df.groupby(\"game_id\", observed=True)[[\"home_abbr\",\"away_abbr\"]].first()\n",
    "        for gid, row in gmeta.iterrows():\n",
    "            game_labels[gid] = f\"{row['home_abbr']} vs {row['away_abbr']}  •  {gid}\"\n",
    "\n",
    "    return df, y_col, game_labels\n",
    "\n",
    "DF, Y_COL, GAME_LABELS = load_data(FEATURES_PATH)\n",
    "DF[\"play_uuid_str\"] = DF[\"play_uuid\"].astype(str)\n",
    "\n",
    "def opt(lst): return [{\"label\": str(v), \"value\": v} for v in lst]\n",
    "\n",
    "positions_all = sorted(map(str, DF[\"position_code\"].dropna().unique().tolist())) if \"position_code\" in DF else []\n",
    "playtypes_all = sorted(map(str, DF[\"play_type\"].dropna().unique().tolist()))    if \"play_type\" in DF else []\n",
    "players_all   = DF[\"player_name\"].dropna().value_counts().head(200).index.tolist() if \"player_name\" in DF else []\n",
    "games_all     = DF[\"game_id\"].dropna().unique().tolist() if \"game_id\" in DF else []\n",
    "\n",
    "# ========================\n",
    "# PLAY-FEATURES & CLUSTER\n",
    "# ========================\n",
    "def make_features_from_timeseries(df: pd.DataFrame, id_col=\"play_uuid\", y_col=\"y\"):\n",
    "    feats = []\n",
    "    for pid, g in df.groupby(id_col, observed=True):\n",
    "        # stabil sortieren (Zeit)\n",
    "        gg = g.sort_values(\"t_sec\", kind=\"mergesort\")\n",
    "        row = {id_col: pid, \"n_samples\": int(len(gg))}\n",
    "        for col in [\"x_norm\", y_col, \"speed\"]:\n",
    "            if col in gg.columns:\n",
    "                vals = gg[col].to_numpy(float)\n",
    "                med  = float(np.nanmedian(vals))\n",
    "                mad  = float(np.nanmedian(np.abs(vals - med)))\n",
    "                q75, q25 = np.nanpercentile(vals, 75), np.nanpercentile(vals, 25)\n",
    "                iqr  = float(q75 - q25)\n",
    "                slope = float(np.polyfit(np.arange(len(vals)), vals, 1)[0]) if len(vals) >= 3 else 0.0\n",
    "                base = col if col != y_col else \"y\"\n",
    "                row[f\"{base}_med\"] = med\n",
    "                row[f\"{base}_mad\"] = mad\n",
    "                row[f\"{base}_iqr\"] = iqr\n",
    "                row[f\"{base}_trend_lr\"] = slope\n",
    "        feats.append(row)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# 1) Pro-Play-Features bauen\n",
    "FEATURES = make_features_from_timeseries(DF, id_col=\"play_uuid\", y_col=Y_COL)\n",
    "\n",
    "# 2) Cluster fitten (Standardisierung + PCA + k per Silhouette)\n",
    "def cluster_fit_add_labels(FEATURES: pd.DataFrame, id_col=\"play_uuid\", use_pca=True, pca_var=0.90):\n",
    "    # Feature-Auswahl: numerisch, ohne ID/Counts/Labels\n",
    "    drop = {'cl_kmeans','cl_agg','cl_dbscan',id_col,'n_samples'}\n",
    "    feature_cols = [c for c in FEATURES.select_dtypes(include=[np.number]).columns if c not in drop]\n",
    "    if len(feature_cols) < 2:\n",
    "        raise ValueError(\"Zu wenig numerische Feature-Spalten für Clustering.\")\n",
    "    X = FEATURES[feature_cols].astype(float).fillna(FEATURES[feature_cols].median(numeric_only=True))\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    if use_pca:\n",
    "        pca = PCA(n_components=pca_var, svd_solver=\"full\", random_state=0)\n",
    "        Xc = pca.fit_transform(Xs)\n",
    "    else:\n",
    "        pca, Xc = None, Xs\n",
    "\n",
    "    # k per Silhouette (2..8)\n",
    "    best = (-np.inf, None, None)\n",
    "    for k in range(2, 9):\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "        lab = km.fit_predict(Xc)\n",
    "        sil = silhouette_score(Xc, lab) if len(set(lab)) > 1 else -np.inf\n",
    "        if sil > best[0]:\n",
    "            best = (sil, k, km)\n",
    "    sil, k_best, km_best = best\n",
    "    labs_km = km_best.predict(Xc)\n",
    "\n",
    "    # Agglomerative Clustering (Ward-Linkage)\n",
    "    agg = AgglomerativeClustering(n_clusters=k_best, linkage=\"ward\")\n",
    "    labs_agg = agg.fit_predict(Xc)\n",
    "\n",
    "    # Labels hinzufügen\n",
    "    OUT = FEATURES.copy()\n",
    "    OUT[\"cl_kmeans\"] = labs_km\n",
    "    OUT[\"cl_agg\"] = labs_agg\n",
    "    meta = {\"k_best\": int(k_best), \"silhouette\": float(sil), \"n_features\": len(feature_cols), \"pca_used\": bool(use_pca), \"pca_var\": float(pca_var)}\n",
    "    return OUT, meta\n",
    "\n",
    "FEATURES, CLUSTER_META = cluster_fit_add_labels(FEATURES, id_col=\"play_uuid\", use_pca=True, pca_var=0.90)\n",
    "\n",
    "# 3) Labels zurück auf DF mergen (additiv, verändert nichts)\n",
    "DF = DF.merge(FEATURES[[\"play_uuid\",\"cl_kmeans\",\"cl_agg\"]], on=\"play_uuid\", how=\"left\")\n",
    "\n",
    "# Cluster-Options\n",
    "clusters_all = sorted(pd.Series(DF[\"cl_kmeans\"].dropna().unique()).astype(int).tolist()) if \"cl_kmeans\" in DF else []\n",
    "\n",
    "# ========================\n",
    "# APP & CACHE\n",
    "# ========================\n",
    "app = Dash(__name__)\n",
    "app.title = \"Football RA • CRP • RQA Dashboard\"\n",
    "cache = Cache(app.server, config={\"CACHE_TYPE\": \"SimpleCache\", \"CACHE_DEFAULT_TIMEOUT\": 300})\n",
    "\n",
    "def _key(x):\n",
    "    if x is None: return \"Ø\"\n",
    "    if isinstance(x, (list, tuple)): return tuple(x)\n",
    "    return x\n",
    "\n",
    "@cache.memoize()\n",
    "def filtered_df_cache(positions, playtypes, players, games, clusters, t0, t1, cols_tuple):\n",
    "    q = DF\n",
    "    if positions and \"position_code\" in q:\n",
    "        q = q[q[\"position_code\"].isin(positions)]\n",
    "    if playtypes and \"play_type\" in q:\n",
    "        q = q[q[\"play_type\"].isin(playtypes)]\n",
    "    if players and \"player_name\" in q:\n",
    "        q = q[q[\"player_name\"].isin(players)]\n",
    "    if games and \"game_id\" in q:\n",
    "        q = q[q[\"game_id\"].isin(games)]\n",
    "    if clusters is not None and len(clusters) > 0 and \"cl_kmeans\" in q:\n",
    "        q = q[q[\"cl_kmeans\"].isin(clusters)]\n",
    "    if \"t_sec\" in q:\n",
    "        q = q[(q[\"t_sec\"] >= t0) & (q[\"t_sec\"] <= t1)]\n",
    "    cols = [c for c in list(cols_tuple) if c in q.columns]\n",
    "    return q[cols].copy()\n",
    "\n",
    "def filtered_df(sel, cols):\n",
    "    return filtered_df_cache(\n",
    "        _key(sel.get(\"positions\")), _key(sel.get(\"playtypes\")),\n",
    "        _key(sel.get(\"players\")), _key(sel.get(\"games\")),\n",
    "        _key(sel.get(\"clusters\")),\n",
    "        sel.get(\"t_range\", (0,3))[0], sel.get(\"t_range\", (0,3))[1],\n",
    "        tuple(cols),\n",
    "    )\n",
    "\n",
    "def valid_options_from(df):\n",
    "    return (\n",
    "        sorted(map(str, df[\"position_code\"].dropna().unique().tolist())) if \"position_code\" in df else [],\n",
    "        sorted(map(str, df[\"play_type\"].dropna().unique().tolist()))     if \"play_type\" in df else [],\n",
    "        sorted(map(str, df[\"player_name\"].dropna().unique().tolist()))   if \"player_name\" in df else [],\n",
    "        df[\"game_id\"].dropna().unique().tolist()                         if \"game_id\" in df else [],\n",
    "    )\n",
    "\n",
    "# ========================\n",
    "# LAYOUT\n",
    "# ========================\n",
    "controls = html.Div([\n",
    "    html.Div([html.Label(\"Position(en)\"),\n",
    "              dcc.Dropdown(id=\"positions\", options=opt(positions_all), multi=True,\n",
    "                           placeholder=\"z. B. WR, DB …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":220,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Play-Typ(en)\"),\n",
    "              dcc.Dropdown(id=\"play_types\", options=opt(playtypes_all), multi=True,\n",
    "                           placeholder=\"z. B. Pass, Rush …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":220,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Spieler\"),\n",
    "              dcc.Dropdown(id=\"players\", options=opt(players_all), multi=True,\n",
    "                           placeholder=\"Spieler wählen …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":260,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Spiele\"),\n",
    "              dcc.Dropdown(id=\"games\",\n",
    "                           options=[{\"label\": GAME_LABELS.get(g, str(g)), \"value\": g} for g in games_all],\n",
    "                           multi=True, placeholder=\"Optional Spiele …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":260,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"Cluster (K-Means)\"),\n",
    "              dcc.Dropdown(id=\"clusters\",\n",
    "                           options=[{\"label\": \"alle\", \"value\": \"__ALL__\"}] + [{\"label\": str(c), \"value\": int(c)} for c in clusters_all],\n",
    "                           multi=True, placeholder=\"Cluster wählen …\", persistence=True)],\n",
    "             style={\"flex\":1,\"minWidth\":220,\"marginRight\":12}),\n",
    "    html.Div([html.Label(\"t_sec\"),\n",
    "              dcc.RangeSlider(id=\"t_range\", min=0, max=3, step=1, value=[0,3],\n",
    "                              marks={i:str(i) for i in range(4)}, updatemode=\"mouseup\")],\n",
    "             style={\"flex\":1,\"minWidth\":220}),\n",
    "], style={\"display\":\"flex\",\"flexWrap\":\"wrap\",\"gap\":8,\"alignItems\":\"flex-end\",\"marginBottom\":10})\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H3(\"Football RA • CRP • RQA Dashboard\"),\n",
    "    html.Div([html.Span(\"Daten: \"), html.Code(Path(FEATURES_PATH).name),\n",
    "              html.Span(f\"  | Zeilen: {len(DF):,}\", style={\"opacity\": .7, \"marginLeft\": 10})],\n",
    "             style={\"marginBottom\": 8}),\n",
    "    controls,\n",
    "    dcc.Tabs(id=\"tabs\", value=\"tab-overview\", children=[\n",
    "        dcc.Tab(label=\"Overview\", value=\"tab-overview\", children=[\n",
    "            html.Div(id=\"kpi-row\", style={\"display\":\"flex\",\"gap\":12,\"flexWrap\":\"wrap\",\"marginBottom\":8}),\n",
    "            dcc.Graph(id=\"heatmap_xy\",     style={\"height\":\"420px\"}),\n",
    "            dcc.Graph(id=\"profile_means\",  style={\"height\":\"340px\"}),\n",
    "            dcc.Graph(id=\"speed_hist\",     style={\"height\":\"300px\"}),\n",
    "        ]),\n",
    "        dcc.Tab(label=\"CRP (Offense vs Defense)\", value=\"tab-crp\", children=[\n",
    "            dcc.Graph(id=\"crp_chart\", style={\"height\":\"420px\",\"marginTop\":\"10px\"}),\n",
    "            dash_table.DataTable(id=\"crp_table\", page_size=10,\n",
    "                                 style_table={\"overflowX\":\"auto\"},\n",
    "                                 style_cell={\"padding\":\"6px\",\"fontFamily\":\"monospace\",\"fontSize\":12}),\n",
    "        ]),\n",
    "        dcc.Tab(label=\"RQA (pro Spiel – illustrativ)\", value=\"tab-rqa\", children=[\n",
    "            html.Div([\n",
    "                html.Div([html.Label(\"Ziel-Recurrence Rate (RR)\"),\n",
    "                          dcc.Slider(id=\"rqa_target_rr\", min=0.02, max=0.15, step=0.005, value=RQA_DEFAULT_RR,\n",
    "                                     marks={0.05:\"5%\",0.1:\"10%\",0.125:\"12.5%\",0.15:\"15%\"})],\n",
    "                         style={\"minWidth\":280,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"l_min\"),\n",
    "                          dcc.Dropdown(id=\"rqa_lmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4]],\n",
    "                                       value=RQA_DEFAULT_LMIN, clearable=False)],\n",
    "                         style={\"width\":220,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"v_min\"),\n",
    "                          dcc.Dropdown(id=\"rqa_vmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4]],\n",
    "                                       value=RQA_DEFAULT_VMIN, clearable=False)],\n",
    "                         style={\"width\":220,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\" \"), html.Button(\"RQA berechnen\", id=\"rqa_compute\", n_clicks=0, style={\"width\":\"200px\",\"height\":\"38px\"})]),\n",
    "            ], style={\"display\":\"flex\",\"flexWrap\":\"wrap\",\"alignItems\":\"flex-end\",\"gap\":8,\"marginBottom\":8}),\n",
    "            html.Div(id=\"rqa_plots_grid\",\n",
    "                     style={\"display\":\"grid\",\"gridTemplateColumns\":\"repeat(auto-fit, minmax(260px, 1fr))\",\"gap\":\"12px\"}),\n",
    "            dash_table.DataTable(id=\"rqa_table\", page_size=10,\n",
    "                                 style_table={\"overflowX\":\"auto\"},\n",
    "                                 style_cell={\"padding\":\"6px\",\"fontFamily\":\"monospace\",\"fontSize\":12}),\n",
    "            html.Div(id=\"rqa_note\", style={\"opacity\":.7,\"marginTop\":6})\n",
    "        ]),\n",
    "        dcc.Tab(label=\"RQA (klassisch • komplette Serie)\", value=\"tab-rqa-classic\", children=[\n",
    "            html.Div([\n",
    "                html.Div([html.Label(\"RR-Modus\"),\n",
    "                          dcc.RadioItems(\n",
    "                              id=\"rqac_rr_mode\",\n",
    "                              options=[\n",
    "                                  {\"label\":\"Dynamisch (Slider)\", \"value\":\"dynamic\"},\n",
    "                                  {\"label\":\"Vorgefertigt (5% / 10% / 15%)\", \"value\":\"preset\"},\n",
    "                              ],\n",
    "                              value=\"dynamic\",\n",
    "                              inline=True\n",
    "                          )],\n",
    "                         style={\"minWidth\":360,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"Ziel-RR (dynamisch)\"),\n",
    "                          dcc.Slider(id=\"rqac_target_rr\", min=0.02, max=0.15, step=0.005, value=RQA_DEFAULT_RR,\n",
    "                                     marks={0.05:\"5%\",0.10:\"10%\",0.125:\"12.5%\",0.15:\"15%\"})],\n",
    "                         style={\"minWidth\":280,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"RR (vorgefertigt)\"),\n",
    "                          dcc.Dropdown(id=\"rqac_rr_preset\",\n",
    "                                       options=[{\"label\":\"5%\",\"value\":0.05},\n",
    "                                                {\"label\":\"10%\",\"value\":0.10},\n",
    "                                                {\"label\":\"15%\",\"value\":0.15}],\n",
    "                                       value=RQA_DEFAULT_RR, clearable=False)],\n",
    "                         style={\"width\":180,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"l_min\"),\n",
    "                          dcc.Dropdown(id=\"rqac_lmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4,5]],\n",
    "                                       value=RQA_DEFAULT_LMIN, clearable=False)],\n",
    "                         style={\"width\":160,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"v_min\"),\n",
    "                          dcc.Dropdown(id=\"rqac_vmin\", options=[{\"label\":i,\"value\":i} for i in [2,3,4,5]],\n",
    "                                       value=RQA_DEFAULT_VMIN, clearable=False)],\n",
    "                         style={\"width\":160,\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"Decimation (jeder k-te Punkt)\"),\n",
    "                          dcc.Input(id=\"rqac_decim\", type=\"number\", min=1, step=1, value=RQA_CLASSIC_DEFAULT_DECIM, style={\"width\":\"120px\"})],\n",
    "                         style={\"marginRight\":16}),\n",
    "                html.Div([html.Label(\"Max. Punkte (Cap)\"),\n",
    "                          dcc.Input(id=\"rqac_maxpts\", type=\"number\", min=200, step=100, value=RQA_CLASSIC_DEFAULT_MAXPTS, style={\"width\":\"140px\"})],\n",
    "                         style={\"marginRight\":16}),\n",
    "                html.Div([html.Label(\" \"), html.Button(\"Klassische RQA berechnen\", id=\"rqac_compute\", n_clicks=0, style={\"width\":\"240px\",\"height\":\"38px\"})]),\n",
    "            ], style={\"display\":\"flex\",\"flexWrap\":\"wrap\",\"alignItems\":\"flex-end\",\"gap\":8,\"marginBottom\":8}),\n",
    "            dcc.Graph(id=\"rqac_plot\", style={\"height\":\"560px\"}),\n",
    "            dash_table.DataTable(id=\"rqac_table\", page_size=5,\n",
    "                                 style_table={\"overflowX\":\"auto\"},\n",
    "                                 style_cell={\"padding\":\"6px\",\"fontFamily\":\"monospace\",\"fontSize\":12}),\n",
    "            html.Div(id=\"rqac_note\", style={\"opacity\":.7,\"marginTop\":6})\n",
    "        ]),\n",
    "    ]),\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# FILTER-SYNC\n",
    "# ========================\n",
    "def _options_union_keep_selected(all_values, filtered_values, selected_values):\n",
    "    sel_set = set(map(str, selected_values or []))\n",
    "    vals = set(map(str, filtered_values or [])) | sel_set\n",
    "    if not vals:\n",
    "        vals = set(map(str, all_values or [])) | sel_set\n",
    "    return opt(sorted(vals))\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"positions\",\"options\"), Output(\"play_types\",\"options\"),\n",
    "    Output(\"players\",\"options\"),  Output(\"games\",\"options\"),\n",
    "    Input(\"positions\",\"value\"),   Input(\"play_types\",\"value\"),\n",
    "    Input(\"players\",\"value\"),     Input(\"games\",\"value\"),\n",
    ")\n",
    "def sync_filters(pos_v, pt_v, pl_v, gm_v):\n",
    "    sel_for_pos = dict(positions=[], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [], clusters=[], t_range=(0,3))\n",
    "    q_pos = filtered_df(sel_for_pos, [\"position_code\"])\n",
    "    pos_vals = q_pos[\"position_code\"].dropna().unique().tolist() if \"position_code\" in q_pos else positions_all\n",
    "    pos_opts = _options_union_keep_selected(positions_all, pos_vals, pos_v)\n",
    "\n",
    "    sel_for_pt  = dict(positions=pos_v or [], playtypes=[], players=pl_v or [], games=gm_v or [], clusters=[], t_range=(0,3))\n",
    "    q_pt = filtered_df(sel_for_pt, [\"play_type\"])\n",
    "    pt_vals = q_pt[\"play_type\"].dropna().unique().tolist() if \"play_type\" in q_pt else playtypes_all\n",
    "    pt_opts = _options_union_keep_selected(playtypes_all, pt_vals, pt_v)\n",
    "\n",
    "    sel_for_pl  = dict(positions=pos_v or [], playtypes=pt_v or [], players=[], games=gm_v or [], clusters=[], t_range=(0,3))\n",
    "    q_pl = filtered_df(sel_for_pl, [\"player_name\"])\n",
    "    pl_vals = q_pl[\"player_name\"].dropna().unique().tolist() if \"player_name\" in q_pl else players_all\n",
    "    pl_opts = _options_union_keep_selected(players_all, pl_vals, pl_v)\n",
    "\n",
    "    sel_for_gm  = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=[], clusters=[], t_range=(0,3))\n",
    "    q_gm = filtered_df(sel_for_gm, [\"game_id\"])\n",
    "    gm_vals = q_gm[\"game_id\"].dropna().unique().tolist() if \"game_id\" in q_gm else games_all\n",
    "    gm_opts = [{\"label\": GAME_LABELS.get(g, str(g)), \"value\": g} for g in sorted(gm_vals, key=lambda x: str(x))]\n",
    "\n",
    "    return pos_opts, pt_opts, pl_opts, gm_opts\n",
    "\n",
    "# ========================\n",
    "# OVERVIEW CALLBACK\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"kpi-row\",\"children\"), Output(\"heatmap_xy\",\"figure\"),\n",
    "    Output(\"profile_means\",\"figure\"), Output(\"speed_hist\",\"figure\"),\n",
    "    Input(\"positions\",\"value\"), Input(\"play_types\",\"value\"),\n",
    "    Input(\"players\",\"value\"), Input(\"games\",\"value\"), Input(\"clusters\",\"value\"), Input(\"t_range\",\"value\"),\n",
    ")\n",
    "def update_overview(pos_v, pt_v, pl_v, gm_v, cl_v, tr_v):\n",
    "\n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [], clusters=cl_v_norm, t_range=tuple(tr_v or (0,3)))\n",
    "\n",
    "    q = filtered_df(sel, [\"play_uuid\",\"player_id\",\"game_id\",\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"cl_kmeans\"])\n",
    "    def k(label, val):\n",
    "        return html.Div([html.Div(label, style={\"fontSize\":12,\"opacity\":.7}),\n",
    "                         html.Div(f\"{val}\", style={\"fontSize\":22,\"fontWeight\":600})],\n",
    "                        style={\"padding\":\"8px 12px\",\"border\":\"1px solid #eee\",\"borderRadius\":8,\"minWidth\":140})\n",
    "    kpis = [k(\"Zeilen\", f\"{len(q):,}\"),\n",
    "            k(\"Unique Plays\", q[\"play_uuid\"].nunique()),\n",
    "            k(\"Unique Spieler\", q[\"player_id\"].nunique()),\n",
    "            k(\"Unique Spiele\", q[\"game_id\"].nunique())]\n",
    "    \n",
    "    # Cluster-Meta\n",
    "    if CLUSTER_META:\n",
    "        kpis.append(k(\"K-Means k\", CLUSTER_META.get(\"k_best\", \"—\")))\n",
    "        kpis.append(k(\"Silhouette\", f\"{CLUSTER_META.get('silhouette', float('nan')):.3f}\"))\n",
    "\n",
    "    h = q\n",
    "    if len(h) > MAX_HEATMAP_POINTS: h = h.sample(MAX_HEATMAP_POINTS, random_state=42)\n",
    "    hm = px.density_heatmap(h, x=\"x_norm\", y=Y_COL, nbinsx=60, nbinsy=27, histnorm=\"\")\n",
    "    hm.update_layout(title=\"Dichte: x_norm vs. y (gesampelt)\")\n",
    "    hm.update_yaxes(scaleanchor=\"x\", scaleratio=53.33/120)\n",
    "\n",
    "    g = q.groupby(\"t_sec\").agg(mean_x=(\"x_norm\",\"mean\"), mean_y=(Y_COL,\"mean\"), mean_v=(\"speed\",\"mean\")).reset_index()\n",
    "    prof = go.Figure()\n",
    "    for col, name in [(\"mean_x\",\"mean x_norm\"),(\"mean_y\",\"mean y\"),(\"mean_v\",\"mean speed (yd/s)\")]:\n",
    "        prof.add_trace(go.Scatter(x=g[\"t_sec\"], y=g[col], mode=\"lines+markers\", name=name))\n",
    "    prof.update_layout(title=\"Mittelwerte je t_sec\", xaxis_title=\"t_sec\", yaxis_title=\"Wert\")\n",
    "\n",
    "    hist = px.histogram(q, x=\"speed\", nbins=SPEED_HIST_BINS, title=\"Geschwindigkeit (yd/s)\")\n",
    "    return kpis, hm, prof, hist\n",
    "\n",
    "# ========================\n",
    "# CRP CALLBACK (mit korrekter Off/Def-Trennung)\n",
    "# ========================\n",
    "def crp_off_vs_def(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Korrekte Trennung:\n",
    "      Offense = rows mit team_id == offense_team_id\n",
    "      Defense = rows mit team_id == defense_team_id\n",
    "    Aggregation: Mittelwerte je t_sec (x_norm, y, speed)\n",
    "    \"\"\"\n",
    "    needed = {\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"team_id\",\"offense_team_id\",\"defense_team_id\"}\n",
    "    if not needed.issubset(df.columns):\n",
    "        # Fallback: keine Trennung möglich\n",
    "        gg = df.groupby(\"t_sec\").agg(x=(\"x_norm\",\"mean\"), y=(Y_COL,\"mean\"), v=(\"speed\",\"mean\")).reset_index()\n",
    "        out = gg.rename(columns={\"x\":\"off_x\",\"y\":\"off_y\",\"v\":\"off_v\"})\n",
    "        out[\"def_x\"] = out[\"off_x\"]; out[\"def_y\"] = out[\"off_y\"]; out[\"def_v\"] = out[\"off_v\"]\n",
    "        out[\"dx\"] = 0.0; out[\"dy\"] = 0.0; out[\"dv\"] = 0.0\n",
    "        return out\n",
    "\n",
    "    is_off = df[\"team_id\"].astype(\"Int64\") == df[\"offense_team_id\"].astype(\"Int64\")\n",
    "    is_def = df[\"team_id\"].astype(\"Int64\") == df[\"defense_team_id\"].astype(\"Int64\")\n",
    "\n",
    "    off = df[is_off]\n",
    "    de  = df[is_def]\n",
    "\n",
    "    g_off = off.groupby(\"t_sec\").agg(x=(\"x_norm\",\"mean\"), y=(Y_COL,\"mean\"), v=(\"speed\",\"mean\")).add_prefix(\"off_\").reset_index()\n",
    "    g_def = de .groupby(\"t_sec\").agg(x=(\"x_norm\",\"mean\"), y=(Y_COL,\"mean\"), v=(\"speed\",\"mean\")).add_prefix(\"def_\").reset_index()\n",
    "\n",
    "    out = pd.merge(g_off, g_def, on=\"t_sec\", how=\"outer\").sort_values(\"t_sec\")\n",
    "    out[[\"off_x\",\"off_y\",\"off_v\",\"def_x\",\"def_y\",\"def_v\"]] = out[\n",
    "        [\"off_x\",\"off_y\",\"off_v\",\"def_x\",\"def_y\",\"def_v\"]\n",
    "    ].ffill().bfill()\n",
    "\n",
    "    out[\"dx\"] = out[\"off_x\"] - out[\"def_x\"]\n",
    "    out[\"dy\"] = out[\"off_y\"] - out[\"def_y\"]\n",
    "    out[\"dv\"] = out[\"off_v\"] - out[\"def_v\"]\n",
    "    return out\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"crp_chart\",\"figure\"), Output(\"crp_table\",\"columns\"), Output(\"crp_table\",\"data\"),\n",
    "    Input(\"positions\",\"value\"), Input(\"play_types\",\"value\"),\n",
    "    Input(\"players\",\"value\"), Input(\"games\",\"value\"), Input(\"clusters\",\"value\"), Input(\"t_range\",\"value\"),\n",
    ")\n",
    "def update_crp(pos_v, pt_v, pl_v, gm_v, cl_v, tr_v):\n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [], clusters=cl_v_norm, t_range=tuple(tr_v or (0,3)))\n",
    "    q = filtered_df(sel, [\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"team_id\",\"offense_team_id\",\"defense_team_id\"])\n",
    "    comp = crp_off_vs_def(q)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for col, name in [(\"off_x\",\"Offense mean x\"),(\"def_x\",\"Defense mean x\"),(\"dx\",\"Δx (Off-Def)\")]:\n",
    "        fig.add_trace(go.Scatter(x=comp[\"t_sec\"], y=comp[col], mode=\"lines+markers\",\n",
    "                                 name=name, line=dict(dash=\"dash\") if col.startswith(\"d\") else None))\n",
    "    for col, name in [(\"off_v\",\"Offense mean v\"),(\"def_v\",\"Defense mean v\"),(\"dv\",\"Δv (Off-Def)\")]:\n",
    "        fig.add_trace(go.Scatter(x=comp[\"t_sec\"], y=comp[col], mode=\"lines+markers\",\n",
    "                                 name=name, line=dict(dash=\"dash\") if col.startswith(\"d\") else None))\n",
    "    fig.update_layout(title=\"CRP: Offense vs Defense (x & v)\", xaxis_title=\"t_sec\")\n",
    "\n",
    "    cols = [{\"name\": c, \"id\": c} for c in comp.columns]\n",
    "    data = comp.round(3).to_dict(\"records\")\n",
    "    return fig, cols, data\n",
    "\n",
    "# ========================\n",
    "# RQA HELFER\n",
    "# ========================\n",
    "def pairwise_dist(A: np.ndarray, w: np.ndarray):\n",
    "    \"\"\"Gewichtete euklidische Distanzmatrix für NxD-Array A mit Gewichten w (D,).\"\"\"\n",
    "    A = np.asarray(A, float)\n",
    "    w = np.asarray(w, float).reshape(1, -1)\n",
    "    diff = A[:, None, :] - A[None, :, :]\n",
    "    return np.sqrt((diff**2 * w).sum(axis=2))\n",
    "\n",
    "def recurrence_matrix(arr: np.ndarray, target_rr: float, w=(1.0,1.0,1.0), standardize: bool = True):\n",
    "    A = np.asarray(arr, float)\n",
    "    if standardize:\n",
    "        mu = A.mean(axis=0, keepdims=True)\n",
    "        sd = A.std(axis=0, keepdims=True) + 1e-9\n",
    "        A = (A - mu) / sd\n",
    "    w = np.asarray(w, float)\n",
    "    D = pairwise_dist(A, w)\n",
    "    tri = D[np.triu_indices_from(D, k=1)]\n",
    "    if len(tri) == 0:\n",
    "        eps = 0.0\n",
    "        return (D <= eps).astype(int), float(eps)\n",
    "    eps = float(np.quantile(tri, target_rr))\n",
    "    return (D <= eps).astype(int), eps\n",
    "\n",
    "def rqa_metrics(R: np.ndarray, l_min=2, v_min=2):\n",
    "    N = R.size\n",
    "    RR = R.sum() / N if N > 0 else 0.0\n",
    "\n",
    "    # Diagonale Linien\n",
    "    diag_lengths = []\n",
    "    for k in range(-(R.shape[0]-1), R.shape[0]):\n",
    "        d = np.diag(R, k)\n",
    "        if d.size == 0: continue\n",
    "        run = 0\n",
    "        for val in d:\n",
    "            if val == 1: run += 1\n",
    "            else:\n",
    "                if run >= l_min: diag_lengths.append(run)\n",
    "                run = 0\n",
    "        if run >= l_min: diag_lengths.append(run)\n",
    "\n",
    "    DET  = (sum(diag_lengths) / R.sum()) if R.sum() > 0 and diag_lengths else 0.0\n",
    "    Lmax = max(diag_lengths) if diag_lengths else 0\n",
    "    L    = float(np.mean(diag_lengths)) if diag_lengths else 0.0\n",
    "    if diag_lengths:\n",
    "        _, cnts = np.unique(diag_lengths, return_counts=True)\n",
    "        p = cnts / cnts.sum()\n",
    "        ENTR = float(-(p * np.log(p + 1e-12)).sum())\n",
    "    else:\n",
    "        ENTR = 0.0\n",
    "\n",
    "    # Vertikale Linien\n",
    "    vert_lengths = []\n",
    "    for j in range(R.shape[1]):\n",
    "        col = R[:, j]\n",
    "        run = 0\n",
    "        for val in col:\n",
    "            if val == 1: run += 1\n",
    "            else:\n",
    "                if run >= v_min: vert_lengths.append(run)\n",
    "                run = 0\n",
    "        if run >= v_min: vert_lengths.append(run)\n",
    "\n",
    "    LAM = (sum(vert_lengths) / R.sum()) if R.sum() > 0 and vert_lengths else 0.0\n",
    "    TT  = float(np.mean(vert_lengths)) if vert_lengths else 0.0\n",
    "    if vert_lengths:\n",
    "        _, cnts_v = np.unique(vert_lengths, return_counts=True)\n",
    "        p_v = cnts_v / cnts_v.sum()\n",
    "        ENTR_V = float(-(p_v * np.log(p_v + 1e-12)).sum())\n",
    "    else:\n",
    "        ENTR_V = 0.0\n",
    "\n",
    "    return dict(RR=RR, DET=DET, L=L, Lmax=Lmax, ENTR=ENTR, LAM=LAM, TT=TT, ENTR_V=ENTR_V)\n",
    "\n",
    "def game_traj(df_game: pd.DataFrame, y_col: str):\n",
    "    # 4 Punkte je Spiel (0..3s) – Mittelwerte über Spieler\n",
    "    g = df_game.groupby(\"t_sec\")[[\"x_norm\", y_col, \"speed\"]].mean().reindex([0,1,2,3])\n",
    "    g = g.ffill().bfill()\n",
    "    return g.to_numpy(float)\n",
    "\n",
    "def build_full_series(df: pd.DataFrame, y_col: str, decim: int, maxpts: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Durchgehende 1 Hz-Serie aus der aktuellen Auswahl:\n",
    "    sortiert nach game_id → play_uuid → t_sec, pro Zeitstempel Mittelwert über Spieler.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return np.empty((0, 3), float)\n",
    "\n",
    "    cols = [\"game_id\",\"play_uuid\",\"t_sec\",\"x_norm\",y_col,\"speed\"]\n",
    "    q = df[cols].copy().sort_values([\"game_id\",\"play_uuid\",\"t_sec\"])\n",
    "    q = q.groupby([\"game_id\",\"play_uuid\",\"t_sec\"], observed=True)[[\"x_norm\", y_col, \"speed\"]].mean().reset_index()\n",
    "\n",
    "    series = q[[\"x_norm\", y_col, \"speed\"]].to_numpy(float)\n",
    "    if decim is None or decim < 1: decim = 1\n",
    "    series = series[::decim]\n",
    "\n",
    "    if maxpts and series.shape[0] > maxpts:\n",
    "        series = series[:maxpts, :]\n",
    "\n",
    "    series = pd.DataFrame(series, columns=[\"x\",\"y\",\"v\"]).ffill().bfill().to_numpy(float)\n",
    "    return series\n",
    "\n",
    "# ========================\n",
    "# RQA CALLBACK (pro Spiel)\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"rqa_plots_grid\",\"children\"),\n",
    "    Output(\"rqa_table\",\"columns\"),\n",
    "    Output(\"rqa_table\",\"data\"),\n",
    "    Output(\"rqa_note\",\"children\"),\n",
    "    Input(\"rqa_compute\",\"n_clicks\"),\n",
    "    State(\"positions\",\"value\"), State(\"play_types\",\"value\"),\n",
    "    State(\"players\",\"value\"),  State(\"games\",\"value\"),\n",
    "    State(\"t_range\",\"value\"),\n",
    "    State(\"rqa_target_rr\",\"value\"), State(\"rqa_lmin\",\"value\"), State(\"rqa_vmin\",\"value\"),\n",
    "    State(\"clusters\",\"value\"),             # ← NEU\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def compute_rqa(n_clicks, pos_v, pt_v, pl_v, gm_v, tr_v, target_rr, l_min, v_min, cl_v):  # ← NEU\n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(\n",
    "        positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [],\n",
    "        clusters=cl_v_norm, t_range=tuple(tr_v or (0,3))\n",
    "    )\n",
    "    q = filtered_df(sel, [\"game_id\",\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"home_abbr\",\"away_abbr\",\"cl_kmeans\"])\n",
    "\n",
    "    note = (\n",
    "        f\"Es werden max. {RQA_SHOW_MAX_GAMES} Spiele visualisiert. \"\n",
    "        f\"Cluster-Filter: {cl_v_norm if cl_v_norm else 'alle'}. \"\n",
    "        f\"Button gedrückt: {n_clicks}. \"\n",
    "        \"Hinweis: Die pro-Spiel-Ansicht ist explorativ und nutzt 4 Zeitpunkte @ 1 Hz.\"\n",
    "    )\n",
    "    return plots, cols, rows, note\n",
    "\n",
    "\n",
    "# ========================\n",
    "# RQA CALLBACK (klassisch • komplette Serie)\n",
    "# ========================\n",
    "@app.callback(\n",
    "    Output(\"rqac_plot\",\"figure\"),\n",
    "    Output(\"rqac_table\",\"columns\"),\n",
    "    Output(\"rqac_table\",\"data\"),\n",
    "    Output(\"rqac_note\",\"children\"),\n",
    "    Input(\"rqac_compute\",\"n_clicks\"),\n",
    "    State(\"positions\",\"value\"), State(\"play_types\",\"value\"),\n",
    "    State(\"players\",\"value\"),  State(\"games\",\"value\"),\n",
    "    State(\"t_range\",\"value\"),\n",
    "    State(\"rqac_rr_mode\",\"value\"),\n",
    "    State(\"rqac_target_rr\",\"value\"), State(\"rqac_rr_preset\",\"value\"),\n",
    "    State(\"rqac_lmin\",\"value\"), State(\"rqac_vmin\",\"value\"),\n",
    "    State(\"rqac_decim\",\"value\"), State(\"rqac_maxpts\",\"value\"),\n",
    "    State(\"clusters\",\"value\"),                     \n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def compute_rqa_classic(n_clicks, pos_v, pt_v, pl_v, gm_v, tr_v,\n",
    "                        rr_mode, target_rr, rr_preset, l_min, v_min, decim, maxpts, cl_v): \n",
    "    cl_v_norm = [] if not cl_v else [c for c in cl_v if c != \"__ALL__\"]\n",
    "    sel = dict(\n",
    "        positions=pos_v or [], playtypes=pt_v or [], players=pl_v or [], games=gm_v or [],\n",
    "        clusters=cl_v_norm, t_range=tuple(tr_v or (0,3))\n",
    "    )\n",
    "    q = filtered_df(sel, [\"game_id\",\"play_uuid\",\"t_sec\",\"x_norm\",Y_COL,\"speed\",\"cl_kmeans\"])\n",
    "    if q.empty:\n",
    "        empty_fig = go.Figure().update_layout(title=\"Keine Daten für die aktuelle Auswahl.\")\n",
    "        return empty_fig, [], [], \"\"\n",
    "\n",
    "    # Debug-Helfer zum Gegencheck in der Konsole\n",
    "    print(\"RQA classic — Cluster-Filter:\", cl_v_norm if cl_v_norm else \"alle\",\n",
    "          \"| unique cl in q:\", sorted(map(int, q[\"cl_kmeans\"].dropna().unique())) if \"cl_kmeans\" in q else \"—\",\n",
    "          \"| N rows:\", len(q))\n",
    "\n",
    "    arr = build_full_series(q, Y_COL, decim=decim or RQA_CLASSIC_DEFAULT_DECIM,\n",
    "                            maxpts=maxpts or RQA_CLASSIC_DEFAULT_MAXPTS)\n",
    "\n",
    "    note = (\n",
    "        f\"Serie basiert auf Filterauswahl. Cluster-Filter: {cl_v_norm if cl_v_norm else 'alle'}. \"\n",
    "        f\"RR-Modus: {rr_mode}. Decimation: {int(decim or RQA_CLASSIC_DEFAULT_DECIM)}. \"\n",
    "        f\"Max. Punkte: {int(maxpts or RQA_CLASSIC_DEFAULT_MAXPTS)}.\"\n",
    "    )\n",
    "    return fig, cols, data, note\n",
    "\n",
    "\n",
    "# ========================\n",
    "# MAIN\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False, port=int(os.getenv(\"PORT\", 8050)), use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
