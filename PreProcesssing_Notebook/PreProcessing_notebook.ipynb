{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9194ae",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, gzip, json, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, math\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbaf0c6",
   "metadata": {},
   "source": [
    "Konstante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026622f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_GLOB = \"/Users/tunahansari/football_ra/data/tracking/SB_tracking_*.json.gz\"\n",
    "\n",
    "MASTER_OUT = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SECONDS = 4             \n",
    "PLAY_MIN_VALID_SHARE = 0.90    \n",
    "OOB_DROP_YARDS = 2.0          \n",
    "FIELD_LEN, FIELD_WID = 120.0, 53.33\n",
    "ENDZONE = 10.0\n",
    "T_MIN, T_MAX = 0, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7647472",
   "metadata": {},
   "source": [
    "Tracking-Daten Unterschuen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42abe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output pro Datei speichern\n",
    "OUTPUT_DIR = Path(\"/Users/tunahansari/football_ra/out_simple\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Master-Output (Zusammenführung)\n",
    "MASTER_OUT_DIR = Path(\"/Users/tunahansari/football_ra/out_1hz_clean\")\n",
    "MASTER_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MASTER_BASENAME = \"master_1hz_4s_ready\"  \n",
    "\n",
    "# Bestehende Outputs überschreiben\n",
    "FORCE_OVERWRITE = False\n",
    "\n",
    "# Master bauen\n",
    "REBUILD_MASTER = False  \n",
    "\n",
    "# yards, feet oder None\n",
    "FORCE_UNITS = None\n",
    "\n",
    "print(f\"Config: files='{INPUT_GLOB}', out='{OUTPUT_DIR}', master='{MASTER_OUT_DIR}', overwrite={FORCE_OVERWRITE}, rebuild_master={REBUILD_MASTER}\")\n",
    "\n",
    "# ----- Hilfsfunktionen -----\n",
    "def _to_float(v):\n",
    "    try:\n",
    "        x = float(v)\n",
    "        if math.isnan(x):\n",
    "            return None\n",
    "        return x\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _overshoot_mag(x, y):\n",
    "    # Abstand außerhalb des Spielfelds berechnen\n",
    "    ox = (0 - x) if x < 0 else (x - FIELD_LEN) if x > FIELD_LEN else 0.0\n",
    "    oy = (0 - y) if y < 0 else (y - FIELD_WID) if y > FIELD_WID else 0.0\n",
    "    return math.hypot(ox, oy)\n",
    "\n",
    "def _clip_xy(x, y):\n",
    "    # (x,y) an Spielfeldgrenzen anpassen\n",
    "    return (min(max(x, 0.0), FIELD_LEN), min(max(y, 0.0), FIELD_WID))\n",
    "\n",
    "def _safe_parquet_path(base_dir: Path, stem: str, ts: bool = True) -> Path:\n",
    "    \"\"\"Erzeuge sicheren Speicherpfad mit Zeitstempel.\"\"\"\n",
    "    if ts:\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        return base_dir / f\"{stem}_{tag}.parquet\"\n",
    "    return base_dir / f\"{stem}.parquet\"\n",
    "\n",
    "def _save_parquet(df: pd.DataFrame, out_path: Path):\n",
    "    try:\n",
    "        df.to_parquet(out_path, index=False, engine=\"pyarrow\")\n",
    "        print(f\" gespeichert: {out_path} (Zeilen: {len(df):,})\")\n",
    "    except Exception as e:\n",
    "        fb = out_path.with_suffix(\".pkl\")\n",
    "        df.to_pickle(fb)\n",
    "        print(f\" Parquet fehlgeschlagen ({e}); Fallback: {fb}\")\n",
    "\n",
    "def _pick_xy_keys(first_play):\n",
    "    # Ermitteln plausible (x,y)-Schlüssel aus ersten Daten\n",
    "    XY_KEYS = [(\"x\",\"y\"), (\"track_x\",\"track_y\"), (\"ngs_x\",\"ngs_y\"), (\"px\",\"py\"), (\"X\",\"Y\")]\n",
    "    tracks = (first_play or {}).get(\"tracks\") or []\n",
    "    for tr in tracks:\n",
    "        steps = tr.get(\"steps\") or tr.get(\"track_steps\") or []\n",
    "        if not steps:\n",
    "            continue\n",
    "        s0 = steps[0]\n",
    "        for kx, ky in XY_KEYS:\n",
    "            if kx in s0 and ky in s0:\n",
    "                return kx, ky\n",
    "    return \"x\", \"y\"  # Standard\n",
    "\n",
    "def _gather_sample_xy(plays, kx, ky, max_n=5000):\n",
    "    # (x,y)-Werte zur Einheitenerkennung\n",
    "    out = []\n",
    "    for play in plays:\n",
    "        for tr in (play.get(\"tracks\") or []):\n",
    "            for s in (tr.get(\"steps\") or []):\n",
    "                if len(out) >= max_n:\n",
    "                    return out\n",
    "                x = _to_float(s.get(kx)); y = _to_float(s.get(ky))\n",
    "                if x is None or y is None:\n",
    "                    continue\n",
    "                out.append((x,y))\n",
    "    return out\n",
    "\n",
    "def _auto_units(sample_xy):\n",
    "    if FORCE_UNITS in (\"yards\",\"feet\"):\n",
    "        return FORCE_UNITS\n",
    "    if not sample_xy:\n",
    "        return \"yards\"\n",
    "    def score_xy(pairs):\n",
    "        n = min(len(pairs), 2000)\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        inb = 0\n",
    "        for i in range(n):\n",
    "            x,y = pairs[i]\n",
    "            if 0 <= x <= FIELD_LEN and 0 <= y <= FIELD_WID:\n",
    "                inb += 1\n",
    "        return inb / n\n",
    "    yards_pairs = sample_xy\n",
    "    feet_pairs  = [(x/3.0, y/3.0) for (x,y) in sample_xy]\n",
    "    sy, sf = score_xy(yards_pairs), score_xy(feet_pairs)\n",
    "    return \"yards\" if sy >= sf else \"feet\"\n",
    "\n",
    "files = sorted(glob.glob(INPUT_GLOB))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien gefunden für das Muster: {INPUT_GLOB}\")\n",
    "\n",
    "print(f\"Starte Preprocessing: {len(files)} Dateien\")\n",
    "qc_rows = []\n",
    "per_file_outputs = []\n",
    "\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    out_parquet = OUTPUT_DIR / (name.replace(\".json.gz\", \".parquet\"))\n",
    "    print(f\"\\n[{i:03d}/{len(files)}] {name}\")\n",
    "\n",
    "    if out_parquet.exists() and not FORCE_OVERWRITE:\n",
    "        print(f\"  ↪ Datei existiert bereits, überspringe (FORCE_OVERWRITE={FORCE_OVERWRITE})\")\n",
    "        per_file_outputs.append(out_parquet)\n",
    "        continue\n",
    "\n",
    "    with gzip.open(fp, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    plays = data.get(\"plays\") or []\n",
    "    if not plays:\n",
    "        print(\"  Keine Plays gefunden, überspringe\")\n",
    "        continue\n",
    "\n",
    "    # (x,y)-Schlüssel und Einheiten\n",
    "    kx, ky = _pick_xy_keys(plays[0])\n",
    "    sample_xy_raw = _gather_sample_xy(plays, kx, ky, max_n=4000)\n",
    "    units = _auto_units(sample_xy_raw)\n",
    "    print(f\" Keys=({kx},{ky})  Einheiten={units}\")\n",
    "\n",
    "    # Zähler initialisieren\n",
    "    steps_total = steps_numeric_win = steps_kept_win = 0\n",
    "    drop_cal = drop_oob_gt2 = clip_oob_le2 = 0\n",
    "    rows_acc = defaultdict(lambda: {\n",
    "        \"sx\":0.0,\"sy\":0.0,\"c\":0,\n",
    "        \"pname\":None,\"pos\":None,\"tid\":None,\n",
    "        \"gid\":data.get(\"game_id\"),\n",
    "        \"home\":data.get(\"home_abbr\") or (data.get(\"home_team\",{}) or {}).get(\"nfl_team_id\"),\n",
    "        \"away\":data.get(\"away_abbr\") or (data.get(\"away_team\",{}) or {}).get(\"nfl_team_id\"),\n",
    "        \"off\":None,\"def\":None,\n",
    "        \"q\":None,\"down\":None,\"ytg\":None,\"ptype\":None\n",
    "    })\n",
    "\n",
    "    for play in plays:\n",
    "        puid = play.get(\"play_uuid\")\n",
    "        if not puid:\n",
    "            continue\n",
    "\n",
    "        ltr = bool(play.get(\"offense_left_to_right\", True))\n",
    "        yln = _to_float(play.get(\"play_yardline\"))\n",
    "        if yln is None or not (0.0 <= yln <= 100.0):\n",
    "            # Ungültige Yardline überspringen\n",
    "            continue\n",
    "\n",
    "        off_id = play.get(\"play_offense_team_id\") or play.get(\"offense_team_id\")\n",
    "        def_id = play.get(\"play_defense_team_id\") or play.get(\"defense_team_id\")\n",
    "\n",
    "        meta_by_play[puid] = dict(\n",
    "            ltr=ltr, yln=yln, off=off_id, de=def_id,\n",
    "            q=play.get(\"play_quarter\"),\n",
    "            down=play.get(\"play_down\"),\n",
    "            ytg=play.get(\"play_yards_to_go\"),\n",
    "            ptype=play.get(\"play_type\")\n",
    "        )\n",
    "\n",
    "        for tr in (play.get(\"tracks\") or []):\n",
    "            player = tr.get(\"player\") or tr.get(\"track_player\") or {}\n",
    "            pid = player.get(\"player_id\")\n",
    "            if pid is None:\n",
    "                continue\n",
    "            pname = player.get(\"name\")\n",
    "            ppos  = player.get(\"position_code\")\n",
    "            tid   = tr.get(\"team_id\") or tr.get(\"track_team_id\") or tr.get(\"nfl_team_id\")\n",
    "            steps = tr.get(\"steps\") or tr.get(\"track_steps\") or []\n",
    "            for s in steps:\n",
    "                tss = _to_float(s.get(\"time_since_snap\"))\n",
    "                if tss is None or tss < 0:\n",
    "                    continue\n",
    "                t_sec = int(math.floor(tss))\n",
    "                if t_sec < 0 or t_sec >= WINDOW_SECONDS:\n",
    "                    continue\n",
    "\n",
    "                steps_total += 1\n",
    "\n",
    "                xr = _to_float(s.get(kx)); yr = _to_float(s.get(ky))\n",
    "                if xr is None or yr is None:\n",
    "                    continue\n",
    "\n",
    "                # Einheitstransformation\n",
    "                if units == \"feet\":\n",
    "                    x_raw, y_raw = xr/3.0, yr/3.0\n",
    "                else:\n",
    "                    x_raw, y_raw = xr, yr\n",
    "\n",
    "                steps_numeric_win += 1\n",
    "\n",
    "                # calibration_fault prüfen\n",
    "                cal = s.get(\"calibration_fault\")\n",
    "                if cal is None:\n",
    "                    cal = s.get(\"step_calibration_fault\")\n",
    "                if bool(cal):\n",
    "                    drop_cal += 1\n",
    "                    continue\n",
    "\n",
    "                # OOB-Check vor Orientierung\n",
    "                m = _overshoot_mag(x_raw, y_raw)\n",
    "                if m > OOB_DROP_YARDS:\n",
    "                    drop_oob_gt2 += 1\n",
    "                    continue\n",
    "                if m > 0:\n",
    "                    x_raw, y_raw = _clip_xy(x_raw, y_raw)\n",
    "                    clip_oob_le2 += 1\n",
    "\n",
    "                # Orientierung: X \n",
    "                x = x_raw if ltr else (FIELD_LEN - x_raw)\n",
    "                y = y_raw\n",
    "\n",
    "                play_seen[puid] += 1\n",
    "\n",
    "                # Aggregation pro (play, player, Sekunde)\n",
    "                key = (puid, pid, t_sec)\n",
    "                acc = rows_acc[key]\n",
    "                acc[\"sx\"] += x\n",
    "                acc[\"sy\"] += y\n",
    "                acc[\"c\"]  += 1\n",
    "                if acc[\"pname\"] is None: acc[\"pname\"] = pname\n",
    "                if acc[\"pos\"]   is None: acc[\"pos\"]  = ppos\n",
    "                if acc[\"tid\"]   is None: acc[\"tid\"]  = tid\n",
    "                if acc[\"off\"]   is None: acc[\"off\"]  = off_id\n",
    "                if acc[\"def\"]   is None: acc[\"def\"]  = def_id\n",
    "                if acc[\"q\"]     is None: acc[\"q\"]    = play.get(\"play_quarter\")\n",
    "                if acc[\"down\"]  is None: acc[\"down\"] = play.get(\"play_down\")\n",
    "                if acc[\"ytg\"]   is None: acc[\"ytg\"]  = play.get(\"play_yards_to_go\")\n",
    "                if acc[\"ptype\"] is None: acc[\"ptype\"]= play.get(\"play_type\")\n",
    "\n",
    "                steps_kept_win += 1\n",
    "                play_kept[puid] += \n",
    "\n",
    "    # Spiele mit zu wenigen gültigen Schritten verwerfen\n",
    "    drop_plays = set()\n",
    "    for puid, seen in play_seen.items():\n",
    "        kept = play_kept.get(puid, 0)\n",
    "        share = kept / max(seen, 1)\n",
    "        if share < PLAY_MIN_VALID_SHARE:\n",
    "            drop_plays.add(puid)\n",
    "\n",
    "    print(f\"Steps: total={steps_total:,} | numeric={steps_numeric_win:,} | kept={steps_kept_win:,}\")\n",
    "    print(f\"    - calibration_fault: {drop_cal:,}\")\n",
    "    print(f\"    - OOB >{OOB_DROP_YARDS}yd gedroppt: {drop_oob_gt2:,}\")\n",
    "    print(f\"    - OOB ≤{OOB_DROP_YARDS}yd geclippt: {clip_oob_le2:,}\")\n",
    "    print(f\"Plays: total={len(plays)} | gedroppt (<{int(PLAY_MIN_VALID_SHARE*100)}% gültig): {len(drop_plays)}\")\n",
    "\n",
    "    # Ausgabe-Daten erstellen\n",
    "    rows = []\n",
    "    for (puid, pid, t_sec), a in rows_acc.items():\n",
    "        if a[\"c\"] == 0 or puid in drop_plays:\n",
    "            continue\n",
    "        meta = meta_by_play.get(puid, {})\n",
    "        ltr = meta.get(\"ltr\", True)\n",
    "        yln = meta.get(\"yln\", 0.0)\n",
    "\n",
    "        # LOS relativ zur Orientierung\n",
    "        L   = (ENDZONE + yln) if ltr else (110.0 - yln)\n",
    "        rows.append({\n",
    "            \"play_uuid\": puid,\n",
    "            \"player_id\": pid,\n",
    "            \"t_sec\": t_sec,\n",
    "            \"x_norm\": (a[\"sx\"]/a[\"c\"]) - L,   \n",
    "            \"y\": a[\"sy\"]/a[\"c\"],              \n",
    "            \"player_name\": a[\"pname\"],\n",
    "            \"position_code\": a[\"pos\"],\n",
    "            \"team_id\": a[\"tid\"],\n",
    "            \"game_id\": a[\"gid\"],\n",
    "            \"home_abbr\": a[\"home\"],\n",
    "            \"away_abbr\": a[\"away\"],\n",
    "            \"offense_team_id\": a[\"off\"],\n",
    "            \"defense_team_id\": a[\"def\"],\n",
    "            \"play_quarter\": a[\"q\"],\n",
    "            \"play_down\": a[\"down\"],\n",
    "            \"play_yards_to_go\": a[\"ytg\"],\n",
    "            \"play_type\": a[\"ptype\"],\n",
    "            \"play_yardline\": yln,\n",
    "            \"ori\": \"KEEP\" if ltr else \"MIRROR\",\n",
    "            \"units\": units,\n",
    "            \"x_key\": kx, \"y_key\": ky,\n",
    "        })\n",
    "\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows).sort_values([\"play_uuid\",\"player_id\",\"t_sec\"])\n",
    "        # Einzeldatei speichern\n",
    "        if out_parquet.exists() and not FORCE_OVERWRITE:\n",
    "            print(f\"  Ziel existiert bereits und FORCE_OVERWRITE=False → Skip Save: {out_parquet}\")\n",
    "        else:\n",
    "            _save_parquet(df, out_parquet)\n",
    "            per_file_outputs.append(out_parquet)\n",
    "    else:\n",
    "        print(\"  Nichts zu speichern (alle Daten verworfen)\")\n",
    "\n",
    "    qc_rows.append({\n",
    "        \"file\": name,\n",
    "        \"plays_total\": len(plays),\n",
    "        \"plays_dropped\": len(drop_plays),\n",
    "        \"steps_total_4s\": steps_total,\n",
    "        \"steps_numeric_4s\": steps_numeric_win,\n",
    "        \"steps_kept_4s\": steps_kept_win,\n",
    "        \"drop_calibration\": drop_cal,\n",
    "        \"drop_oob_gt2\": drop_oob_gt2,\n",
    "        \"clip_oob_le2\": clip_oob_le2,\n",
    "    })\n",
    "\n",
    "# Gesamt-QC anzeigen\n",
    "df_qc = pd.DataFrame(qc_rows)\n",
    "print(\"\\n Fertig (pro Datei).\")\n",
    "if not df_qc.empty:\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_qc.head(10))\n",
    "        display(df_qc[[\"steps_total_4s\",\"steps_numeric_4s\",\"steps_kept_4s\",\"drop_calibration\",\"drop_oob_gt2\",\"clip_oob_le2\"]].sum())\n",
    "    except Exception:\n",
    "        print(df_qc.head(10).to_string(index=False))\n",
    "        sums = df_qc[[\"steps_total_4s\",\"steps_numeric_4s\",\"steps_kept_4s\",\"drop_calibration\",\"drop_oob_gt2\",\"clip_oob_le2\"]].sum()\n",
    "        for k,v in sums.items():\n",
    "            print(f\"  {k}: {int(v):,}\")\n",
    "\n",
    "# Master-Output erstellen (Concat aller Parquets)\n",
    "if REBUILD_MASTER:\n",
    "    print(\"\\n Baue Master…\")\n",
    "    # Alle Parquets im OUTPUT_DIR verwenden\n",
    "    parts = sorted(OUTPUT_DIR.glob(\"*.parquet\"))\n",
    "    if not parts:\n",
    "        print(\"  Keine Teile gefunden – Master entfällt.\")\n",
    "    else:\n",
    "        dfs = []\n",
    "        for p in parts:\n",
    "            try:\n",
    "                d = pd.read_parquet(p)\n",
    "                # Prüfung der Kernspalten\n",
    "                need = {\"play_uuid\",\"player_id\",\"t_sec\",\"x_norm\",\"y\"}\n",
    "                if not need.issubset(d.columns):\n",
    "                    print(f\"  {p.name}: fehlende Spalten {need - set(d.columns)} – Teil überspringen\")\n",
    "                    continue\n",
    "                dfs.append(d)\n",
    "            except Exception as e:\n",
    "                print(f\"  {p.name}: Read-Error {e} – Teil überspringen\")\n",
    "\n",
    "        if not dfs:\n",
    "            print(\"  Keine verwertbaren Teile – Master entfällt.\")\n",
    "        else:\n",
    "            master = pd.concat(dfs, ignore_index=True)\n",
    "            master.sort_values([\"game_id\",\"play_uuid\",\"player_id\",\"t_sec\"], inplace=True)\n",
    "\n",
    "            # Berechne dx, dy, speed pro 1 Hz\n",
    "            grp = [\"game_id\",\"play_uuid\",\"player_id\"]\n",
    "            master[\"dx\"] = master.groupby(grp, observed=True)[\"x_norm\"].diff().fillna(0.0)\n",
    "            master[\"dy\"] = master.groupby(grp, observed=True)[\"y\"].diff().fillna(0.0)\n",
    "            master[\"speed\"] = np.sqrt(master[\"dx\"]**2 + master[\"dy\"]**2)\n",
    "\n",
    "            # Schreibpfad mit Zeitstempel\n",
    "            out_master = _safe_parquet_path(MASTER_OUT_DIR, MASTER_BASENAME, ts=True)\n",
    "            _save_parquet(master, out_master)\n",
    "            print(f\"Master geschrieben → {out_master}\")\n",
    "else:\n",
    "    print(\"\\nREBUILD_MASTER=False – kein Master erstellt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e907f4e",
   "metadata": {},
   "source": [
    "Datenverarbeitungspipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93108f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x): print(x)\n",
    "\n",
    "BASE_DIR = Path(\"/Users/tunahansari/football_ra/out_1hz_clean\")\n",
    "\n",
    "if not BASE_DIR.exists():\n",
    "    BASE_DIR = Path.cwd() / \"out_1hz_clean\"\n",
    "\n",
    "PARQUET_GLOB = str(BASE_DIR / \"*.parquet\")\n",
    "MASTER_OUT = str(BASE_DIR / \"master_1hz_4s.parquet\")\n",
    "\n",
    "FIELD_WID = 53.33\n",
    "T_MIN, T_MAX = 0, 3\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"play_uuid\", \"player_id\", \"t_sec\", \"x_norm\",\n",
    "    \"position_code\", \"track_team_id\", \"offense_team_id\", \"defense_team_id\",\n",
    "    \"play_yardline\", \"play_type\", \"home_abbr\", \"away_abbr\", \"game_id\", \"gsis_play_id\"\n",
    "]\n",
    "\n",
    "ALIASES = {\n",
    "    \"player_id\": [\"player_id\", \"nfl_id\", \"nflId\"],\n",
    "    \"gsis_play_id\": [\"gsis_play_id\", \"play_id\", \"gsisPlayId\"],\n",
    "    \"position_code\": [\"position_code\", \"position\"],\n",
    "    \"track_team_id\": [\"track_team_id\", \"team_id\", \"teamId\", \"team\"],\n",
    "    \"offense_team_id\": [\"offense_team_id\", \"offenseTeamId\", \"offense_team\"],\n",
    "    \"defense_team_id\": [\"defense_team_id\", \"defenseTeamId\", \"defense_team\"],\n",
    "    \"play_yardline\": [\"play_yardline\", \"yardline\", \"yardLine\"],\n",
    "    \"play_type\": [\"play_type\", \"playType\"],\n",
    "    \"home_abbr\": [\"home_abbr\", \"homeTeamAbbr\", \"home_team\"],\n",
    "    \"away_abbr\": [\"away_abbr\", \"awayTeamAbbr\", \"away_team\"],\n",
    "    \"game_id\": [\"game_id\", \"gameId\"],\n",
    "}\n",
    "\n",
    "def ensure_alias_cols(df, required_cols, aliases):\n",
    "    missing = []\n",
    "    for col in required_cols:\n",
    "        if col in df.columns:\n",
    "            continue\n",
    "        if col in (\"x_norm\", \"t_sec\", \"play_uuid\"):\n",
    "            if col not in df.columns:\n",
    "                missing.append(col)\n",
    "            continue\n",
    "        for a in aliases.get(col, []):\n",
    "            if a in df.columns:\n",
    "                df[col] = df[a]\n",
    "                break\n",
    "        else:\n",
    "            missing.append(col)\n",
    "    return df, missing\n",
    "\n",
    "print(\"Suche Parquet-Dateien ...\")\n",
    "files = sorted(glob.glob(PARQUET_GLOB))\n",
    "files = [f for f in files if not os.path.basename(f).startswith(\"master_\")]\n",
    "print(f\"Gefunden: {len(files)} Dateien in {BASE_DIR}\")\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"Keine Dateien gefunden. Bitte prüfen: {PARQUET_GLOB}\")\n",
    "\n",
    "print(\"\\nBestimme Spaltennamen für y (step_y vs. y) aus der ersten Datei ...\")\n",
    "probe = pd.read_parquet(files[0])\n",
    "if \"y\" in probe.columns:\n",
    "    Y_COL = \"y\"\n",
    "elif \"step_y\" in probe.columns:\n",
    "    Y_COL = \"step_y\"\n",
    "else:\n",
    "    raise KeyError(\"Weder 'y' noch 'step_y' in den Parquet-Dateien gefunden.\")\n",
    "print(f\"y-Spalte: {Y_COL}\")\n",
    "\n",
    "print(\"\\nLade & merge alle Dateien (das dauert je nach Platte kurz) ...\")\n",
    "dfs = []\n",
    "running_rows = 0\n",
    "for i, fp in enumerate(files, 1):\n",
    "    name = os.path.basename(fp)\n",
    "    df = pd.read_parquet(fp)\n",
    "    df[\"file\"] = name\n",
    "    dfs.append(df)\n",
    "    running_rows += len(df)\n",
    "    if i % 10 == 0 or i == len(files):\n",
    "        print(f\"[{i:03d}/{len(files)}] geladen: {name}  (aktuelle Gesamtzeilen ~ {running_rows:,})\")\n",
    "\n",
    "master = pd.concat(dfs, ignore_index=True)\n",
    "del dfs, probe\n",
    "\n",
    "print(\"\\nMerge fertig.\")\n",
    "print(f\"master.shape = {master.shape[0]:,} Zeilen × {master.shape[1]} Spalten\")\n",
    "mem_mb = master.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"geschätzter Speicherbedarf: {mem_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nMINI-QC startet ...\")\n",
    "\n",
    "print(\"\\nPflichtspalten prüfen ...\")\n",
    "master, missing_after_alias = ensure_alias_cols(master, REQUIRED_COLS, ALIASES)\n",
    "\n",
    "HARD_REQ = {\"play_uuid\", \"player_id\", \"t_sec\", \"x_norm\"}\n",
    "hard_missing = [c for c in HARD_REQ if c not in master.columns]\n",
    "soft_missing = [c for c in missing_after_alias if c not in HARD_REQ]\n",
    "\n",
    "if hard_missing:\n",
    "    print(f\"Harte Pflichtspalten fehlen: {hard_missing}\")\n",
    "    raise KeyError(f\"Pflichtspalten fehlen: {hard_missing}\")\n",
    "if soft_missing:\n",
    "    for c in soft_missing:\n",
    "        print(f\"Hinweis: optionale/Meta-Spalte fehlt: {c}\")\n",
    "print(\"Pflichtspalten ok.\")\n",
    "\n",
    "print(\"\\nt_sec-Check ... (erwartet 0..3)\")\n",
    "t_min, t_max = master[\"t_sec\"].min(), master[\"t_sec\"].max()\n",
    "vals = np.sort(master[\"t_sec\"].unique())\n",
    "share_out_range = ((master[\"t_sec\"] < T_MIN) | (master[\"t_sec\"] > T_MAX)).mean()\n",
    "print(f\"t_sec Werte: min={t_min}, max={t_max}, Unique={vals[:10]}{' ...' if len(vals) > 10 else ''}\")\n",
    "print(f\"Anteil außerhalb [{T_MIN},{T_MAX}]: {share_out_range:.4%}\")\n",
    "if share_out_range > 0:\n",
    "    counts_out = master.loc[(master[\"t_sec\"] < T_MIN) | (master[\"t_sec\"] > T_MAX), \"t_sec\"].value_counts().sort_index()\n",
    "    print(\"Werte außerhalb Range (Counts):\")\n",
    "    print(counts_out.to_string())\n",
    "\n",
    "print(\"\\ny-Grenzen (0 .. 53.33 yd) ...\")\n",
    "y = pd.to_numeric(master[Y_COL], errors=\"coerce\")\n",
    "oob_low = (y < 0).sum()\n",
    "oob_high = (y > FIELD_WID).sum()\n",
    "oob_share = ((y < 0) | (y > FIELD_WID)).mean()\n",
    "print(f\"y.min={float(np.nanmin(y)):.3f}, y.max={float(np.nanmax(y)):.3f}\")\n",
    "print(f\"OOB y<0: {oob_low:,} | y>{FIELD_WID}: {oob_high:,}  → Anteil: {oob_share:.4%}\")\n",
    "if oob_share == 0:\n",
    "    print(\"y liegt vollständig im Feld (Clip hat gegriffen).\")\n",
    "else:\n",
    "    print(\"Es gibt noch Punkte außerhalb – ggf. stichprobenartig prüfen.\")\n",
    "\n",
    "print(\"\\nx_norm @ t=0 ...\")\n",
    "t0 = master.loc[master[\"t_sec\"] == 0, \"x_norm\"]\n",
    "t0 = pd.to_numeric(t0, errors=\"coerce\").dropna()\n",
    "if len(t0) > 0:\n",
    "    q = t0.quantile([0.01, 0.25, 0.5, 0.75, 0.99]).to_dict()\n",
    "    mean_, std_ = float(t0.mean()), float(t0.std())\n",
    "    print(f\"count={t0.shape[0]:,} | mean={mean_:.3f} | std={std_:.3f}\")\n",
    "    print(f\"quantiles: 1%={q[0.01]:.3f}, 25%={q[0.25]:.3f}, 50%={q[0.5]:.3f}, 75%={q[0.75]:.3f}, 99%={q[0.99]:.3f}\")\n",
    "    if abs(mean_) <= 0.25:\n",
    "        print(\"LOS-Normalisierung sieht gut aus (Mittelwert ~0 yd).\")\n",
    "    else:\n",
    "        print(\"Mittelwert ist weiter von 0 entfernt als erwartet – ggf. LOS-Offset verifizieren.\")\n",
    "else:\n",
    "    print(\"Keine t=0-Zeilen gefunden (unerwartet).\")\n",
    "\n",
    "print(\"\\nZeilen pro Datei (Top 10):\")\n",
    "lines_per_file = master[\"file\"].value_counts().head(10)\n",
    "print(lines_per_file.to_string())\n",
    "\n",
    "print(\"\\nMINI-QC abgeschlossen – Daten sind bereit für RP/CRP/RQA & Clustering.\")\n",
    "\n",
    "SAVE_MASTER = True\n",
    "if SAVE_MASTER:\n",
    "    out_dir = os.path.dirname(MASTER_OUT)\n",
    "    if out_dir and not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    master.to_parquet(MASTER_OUT, index=False)\n",
    "    print(f\"\\nMaster-Parquet gespeichert: {MASTER_OUT}\")\n",
    "    print(\"(Beim Weiterarbeiten kannst du direkt dieses File laden)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fbba3d",
   "metadata": {},
   "source": [
    " Plausibilität der initialen Spielerpositionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    master \n",
    "except NameError:\n",
    "    master = pd.read_parquet(MASTER_OUT)\n",
    "\n",
    "t0 = master.loc[master[\"t_sec\"] == 0, [\"file\",\"play_uuid\",\"x_norm\",\"position_code\"]].copy()\n",
    "t0[\"abs_x0\"] = t0[\"x_norm\"].abs()\n",
    "\n",
    "print(f\"t0 rows: {len(t0):,}\")\n",
    "print(f\"Anteil |x_norm| @t0 > 12 yd: {(t0['abs_x0']>12).mean():.2%}\")\n",
    "\n",
    "print(\"\\nTop-Dateien mit vielen Ausreißern (|x_norm|>12yd) @t0:\")\n",
    "print(t0.loc[t0[\"abs_x0\"]>12].groupby(\"file\").size().sort_values(ascending=False).head(15).to_string())\n",
    "\n",
    "print(\"\\nSchlimmste 10 Plays (|x_norm| @t0):\")\n",
    "cols = [\"file\",\"play_uuid\",\"position_code\",\"x_norm\"]\n",
    "print(t0.sort_values(\"abs_x0\", ascending=False)[cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03ade5",
   "metadata": {},
   "source": [
    "Verschiebungen in den normalisierten x-Positionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_IN  = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s.parquet\"\n",
    "MASTER_OUT = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s_fix.parquet\"\n",
    "\n",
    "try:\n",
    "    master  \n",
    "    print(\"   (nutze vorhandenen DataFrame 'master')\")\n",
    "except NameError:\n",
    "    master = pd.read_parquet(MASTER_IN)\n",
    "    print(f\"   geladen: {len(master):,} Zeilen\")\n",
    "\n",
    "# Sicherheit: numerische Typen erzwingen\n",
    "master[\"t_sec\"]  = pd.to_numeric(master[\"t_sec\"], errors=\"coerce\")\n",
    "master[\"x_norm\"] = pd.to_numeric(master[\"x_norm\"], errors=\"coerce\")\n",
    "\n",
    "t0_before = master.loc[master[\"t_sec\"]==0, \"x_norm\"].dropna()\n",
    "share_bad_before = (t0_before.abs() > 12).mean()\n",
    "print(f\"\\nVorher: |x_norm|@t0 > 12 yd = {share_bad_before:.2%}\")\n",
    "print(f\"   t0 count={t0_before.shape[0]:,} | mean={t0_before.mean():.3f} | std={t0_before.std():.3f}\")\n",
    "\n",
    "# offset pro Play berechnen\n",
    "def play_offset(g: pd.DataFrame) -> float:\n",
    "    t0 = g[g[\"t_sec\"]==0]\n",
    "    if t0.empty:\n",
    "        return 0.0\n",
    "    # Offense-Spieler bei t0\n",
    "    off_mask = (t0[\"track_team_id\"] == t0[\"offense_team_id\"])\n",
    "    if off_mask.sum() >= 8:\n",
    "        med = np.nanmedian(t0.loc[off_mask, \"x_norm\"])\n",
    "    else:\n",
    "        # Fallback: alle bei t0 (z.B. wenn Team-IDs fehlen)\n",
    "        med = np.nanmedian(t0[\"x_norm\"])\n",
    "    return float(med) if np.isfinite(med) else 0.0\n",
    "\n",
    "print(\"\\nBerechne Offsets pro play_uuid …\")\n",
    "offsets = master.groupby(\"play_uuid\", sort=False).apply(play_offset)\n",
    "\n",
    "# Kleine Übersicht der Offset-Verteilung\n",
    "q = offsets.quantile([0.01,0.25,0.5,0.75,0.99]).to_dict()\n",
    "print(f\"   Offsets quantiles (yd): 1%={q[0.01]:.2f}, 25%={q[0.25]:.2f}, 50%={q[0.5]:.2f}, 75%={q[0.75]:.2f}, 99%={q[0.99]:.2f}\")\n",
    "print(f\"   Anteil |Offset| > 12 yd: {(offsets.abs()>12).mean():.2%}\")\n",
    "\n",
    "# --- Anwenden: x_norm korrigieren ----\n",
    "print(\"\\n Wende Offsets an (x_norm_fix = x_norm - Offset) …\")\n",
    "master[\"x_norm_fix\"] = master[\"x_norm\"] - master[\"play_uuid\"].map(offsets)\n",
    "\n",
    "# --- Nachher-Diagnose ----\n",
    "t0_after = master.loc[master[\"t_sec\"]==0, \"x_norm_fix\"].dropna()\n",
    "share_bad_after = (t0_after.abs() > 12).mean()\n",
    "print(f\"\\nNachher: |x_norm_fix|@t0 > 12 yd = {share_bad_after:.2%}\")\n",
    "print(f\"   t0 count={t0_after.shape[0]:,} | mean={t0_after.mean():.3f} | std={t0_after.std():.3f}\")\n",
    "\n",
    "# Optional: very-bad plays markieren (falls du noch strenger filtern willst)\n",
    "# Ein simples Gütekriterium: Nach der Korrektur sollten >=90% der Spieler eines Plays bei t0 innerhalb ±12 yd liegen.\n",
    "t0_fix = master.loc[master[\"t_sec\"]==0, [\"play_uuid\",\"x_norm_fix\"]].copy()\n",
    "t0_fix[\"ok\"] = t0_fix[\"x_norm_fix\"].abs() <= 12\n",
    "good_share = t0_fix.groupby(\"play_uuid\")[\"ok\"].mean()\n",
    "bad_plays = good_share[good_share < 0.90].index\n",
    "print(f\"\\nPlays mit fraglicher Korrektur (t0 <90% in ±12 yd): {len(bad_plays):,}\")\n",
    "\n",
    "# --- Speichern -----\n",
    "print(\"\\n Speichere Master mit x_norm_fix …\")\n",
    "Path(MASTER_OUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "master.to_parquet(MASTER_OUT, index=False)\n",
    "print(f\"   geschrieben: {MASTER_OUT}  (Zeilen: {len(master):,})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a95375",
   "metadata": {},
   "source": [
    "finale Stufe der Datenbereinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/Users/tunahansari/football_ra/out_1hz_clean\"\n",
    "IN_FIX = f\"{BASE}/master_1hz_4s_fix.parquet\"\n",
    "OUT_REZERO = f\"{BASE}/master_1hz_4s_rezero.parquet\"\n",
    "OUT_BADPLAYS = f\"{BASE}/bad_plays_t0_lt90.csv\"\n",
    "\n",
    "master = pd.read_parquet(IN_FIX)\n",
    "master[\"t_sec\"] = pd.to_numeric(master[\"t_sec\"], errors=\"coerce\")\n",
    "master[\"x_norm_fix\"] = pd.to_numeric(master[\"x_norm_fix\"], errors=\"coerce\")\n",
    "\n",
    "# 1) Globalen Restversatz @t0 entfernen (zentriert Median auf 0)\n",
    "t0_fix = master.loc[master[\"t_sec\"]==0, \"x_norm_fix\"].dropna()\n",
    "global_residual = float(t0_fix.median()) if len(t0_fix) else 0.0\n",
    "print(f\" Globaler Rest-Offset (Median @t0): {global_residual:.3f} yd\")\n",
    "\n",
    "master[\"x_norm_final\"] = master[\"x_norm_fix\"] - global_residual\n",
    "\n",
    "# Diagnose nach Re-Zentrierung\n",
    "t0_final = master.loc[master[\"t_sec\"]==0, \"x_norm_final\"].dropna()\n",
    "share_bad = (t0_final.abs() > 12).mean()\n",
    "print(f\" Nachher-final: |x_norm_final|@t0 > 12 yd = {share_bad:.2%}\")\n",
    "print(f\"   t0 count={t0_final.shape[0]:,} | mean={t0_final.mean():.3f} | std={t0_final.std():.3f} | median={t0_final.median():.3f}\")\n",
    "\n",
    "# 2) Plays mit <90% ok @t0 markieren & Report schreiben\n",
    "t0 = master.loc[master[\"t_sec\"]==0, [\"play_uuid\",\"x_norm_final\",\"file\"]].copy()\n",
    "t0[\"ok\"] = t0[\"x_norm_final\"].abs() <= 12\n",
    "per_play = t0.groupby(\"play_uuid\").agg(\n",
    "    share_ok=(\"ok\", \"mean\"),\n",
    "    n=(\"ok\",\"size\"),\n",
    "    n_ok=(\"ok\",\"sum\")\n",
    ").reset_index()\n",
    "\n",
    "bad_plays = per_play.loc[per_play[\"share_ok\"] < 0.90, \"play_uuid\"]\n",
    "print(f\"Plays mit t0<90% in ±12yd: {len(bad_plays):,}\")\n",
    "\n",
    "# Report: welche Dateien / wie stark betroffen\n",
    "bad_report = (\n",
    "    t0[t0[\"play_uuid\"].isin(bad_plays)]\n",
    "    .drop_duplicates(subset=[\"play_uuid\",\"file\"])\n",
    "    .merge(per_play, on=\"play_uuid\", how=\"left\")\n",
    "    .sort_values([\"share_ok\",\"file\"])\n",
    ")\n",
    "Path(OUT_BADPLAYS).parent.mkdir(parents=True, exist_ok=True)\n",
    "bad_report.to_csv(OUT_BADPLAYS, index=False)\n",
    "print(f\" Report gespeichert: {OUT_BADPLAYS} (Zeilen: {len(bad_report):,})\")\n",
    "\n",
    "# 3) x_norm ersetzen & speichern (für Downstream)\n",
    "master_out = master.drop(columns=[c for c in [\"x_norm\",\"x_norm_fix\"] if c in master.columns]) \\\n",
    "                   .rename(columns={\"x_norm_final\":\"x_norm\"})\n",
    "master_out.to_parquet(OUT_REZERO, index=False)\n",
    "print(f\" geschrieben: {OUT_REZERO}  (Zeilen: {len(master_out):,})\")\n",
    "\n",
    "print(\"\\nAlles fertig. Nutze ab jetzt dieses File für Clustering/RP/CRP/RQA:\")\n",
    "print(\" →\", OUT_REZERO)\n",
    "print(\"Und schau ggf. in den Bad-Play-Report:\")\n",
    "print(\" →\", OUT_BADPLAYS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b536026",
   "metadata": {},
   "source": [
    "Transformation von Hz in kompakte Feature-Tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b36e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aus Zeitreihen eine Pro-Play-Feature-Tabelle bauen \n",
    "\n",
    "TS_PATH = \"/Users/tunahansari/football_ra/out_1hz_clean/master_1hz_4s_ready.parquet\"   \n",
    "TS = pd.read_parquet(TS_PATH)\n",
    "\n",
    "# Spalten wie 'play_uuid' + Zeitreihen (z. B. speed, d_pos, v_rad, x_norm, y ...)\n",
    "def make_features_from_timeseries(df, id_col=\"play_uuid\"):\n",
    "    feats = []\n",
    "    for pid, g in df.groupby(id_col):\n",
    "        row = {id_col: pid, \"n_samples\": len(g)}\n",
    "        # Statistiken für die Zeitreihe\n",
    "        for col in [\"speed\", \"d_pos\", \"v_rad\", \"x_norm\", \"y\"]:\n",
    "            if col in g.columns:\n",
    "                med = float(g[col].median())\n",
    "                row[f\"{col}_med\"] = med\n",
    "                row[f\"{col}_mad\"] = float((g[col] - med).abs().median())\n",
    "                row[f\"{col}_iqr\"] = float(g[col].quantile(0.75) - g[col].quantile(0.25))\n",
    "                row[f\"{col}_trend_lr\"] = float(np.polyfit(np.arange(len(g)), g[col].to_numpy(), 1)[0]) if len(g) >= 3 else 0.0\n",
    "        feats.append(row)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "FEATURES = make_features_from_timeseries(TS, id_col=\"play_uuid\")\n",
    "print(FEATURES.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c6dc8",
   "metadata": {},
   "source": [
    "Umfassende Implementierung der Clusteranalyse von Football-Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f315e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = \"play_uuid\"\n",
    "assert id_col in FEATURES.columns, f\"Spalte '{id_col}' fehlt in FEATURES.\"\n",
    "\n",
    "# 1) Feature-Spalten automatisch wählen (nur numerisch, ohne ID)\n",
    "num_cols = FEATURES.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in num_cols if c not in [id_col]]\n",
    "if len(feature_cols) < 2:\n",
    "    raise ValueError(\"Zu wenig numerische Feature-Spalten gefunden. Bitte Feature-Build prüfen.\")\n",
    "\n",
    "# 2) Arbeitskopie & NaNs füllen\n",
    "DF = deepcopy(FEATURES[[id_col] + feature_cols]).copy()\n",
    "X = DF[feature_cols].astype(float)\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# 3) Skalieren (+ optional PCA)\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "use_pca = True\n",
    "Xc = PCA(n_components=0.90, svd_solver=\"full\", random_state=0).fit_transform(Xs) if use_pca else Xs\n",
    "\n",
    "# 4) k per Silhouette (2..8)\n",
    "best = (-np.inf, None, None)\n",
    "for k in range(2, 9):\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=0)\n",
    "    lab = km.fit_predict(Xc)\n",
    "    sil = silhouette_score(Xc, lab) if len(set(lab)) > 1 else -np.inf\n",
    "    if sil > best[0]:\n",
    "        best = (sil, k, km)\n",
    "sil, k_best, km_best = best\n",
    "labs_km = km_best.predict(Xc)\n",
    "\n",
    "# 5) Agglomerativ (Ward) @k_best\n",
    "agg = AgglomerativeClustering(n_clusters=k_best, linkage=\"ward\")\n",
    "labs_agg = agg.fit_predict(Xc)\n",
    "\n",
    "# 6) Labels additiv an FEATURES hängen\n",
    "FEATURES = FEATURES.merge(\n",
    "    DF[[id_col]].assign(cl_kmeans=labs_km, cl_agg=labs_agg),\n",
    "    on=id_col, how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Clusterzahl (K-Means): k={k_best}, Silhouette={sil:.3f}\")\n",
    "print(FEATURES[[id_col, 'cl_kmeans','cl_agg']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5c34e",
   "metadata": {},
   "source": [
    "Silhouettenanalyse (Analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks, sils = [], []\n",
    "for k in range(2,9):\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=0).fit(Xc)\n",
    "    lab = km.labels_\n",
    "    if len(set(lab))>1:\n",
    "        ks.append(k); sils.append(silhouette_score(Xc, lab))\n",
    "print(list(zip(ks, np.round(sils,3))))\n",
    "# kurzer Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(ks, sils, marker='o')\n",
    "plt.title('Silhouette je k')\n",
    "plt.xlabel('k'); plt.ylabel('Silhouette'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e004e",
   "metadata": {},
   "source": [
    "Cluster-Profil erstellen (Mittelwerte pro Cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile = (FEATURES\n",
    "                   .groupby('cl_kmeans')[feature_cols]\n",
    "                   .median()\n",
    "                   .assign(n=FEATURES.groupby('cl_kmeans').size()))\n",
    "cluster_profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392aa90",
   "metadata": {},
   "source": [
    "DBSCAN Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "best = None\n",
    "for eps in (0.3,0.5,0.7,1.0):\n",
    "    for ms in (5,10,20):\n",
    "        db = DBSCAN(eps=eps, min_samples=ms).fit(Xc)\n",
    "        lab = db.labels_\n",
    "        k_eff = len(set(lab)) - (1 if -1 in lab else 0)\n",
    "        noise = (lab == -1).mean()\n",
    "        runs.append((eps, ms, k_eff, round(noise,3)))\n",
    "# pick eine sinnvolle Kombi (z.B. wenig Noise, k_eff 2–10) und fitten:\n",
    "db = DBSCAN(eps=0.5, min_samples=10).fit(Xc)\n",
    "FEATURES['cl_dbscan'] = db.labels_\n",
    "print('DBSCAN: -1 = Noise, sonst Cluster-ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d58230",
   "metadata": {},
   "source": [
    "Cluster-Statistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e021ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = FEATURES['cl_dbscan']\n",
    "vals, cnts = np.unique(lab, return_counts=True)\n",
    "print(dict(zip(vals, cnts)))\n",
    "noise = float((lab == -1).mean())\n",
    "k_eff = len(set(lab)) - (1 if -1 in set(lab) else 0)\n",
    "print(f\"k_eff={k_eff}, Noise={noise:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf9b2c",
   "metadata": {},
   "source": [
    "Cluster-Labels in FEATURES integrieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e873d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster-Labels in Dashboard-DF integrieren\n",
    "df = FEATURES.copy()\n",
    "\n",
    "# Vorherige Spalten sichern (für Merge-Check)\n",
    "cols_before = set(df.columns) - {'cl_kmeans','cl_agg','cl_dbscan'}\n",
    "unchanged = df[sorted(cols_before)].copy()\n",
    "\n",
    "# Neue Cluster-Labels hinzufügen\n",
    "added = {'cl_kmeans','cl_agg'} & set(df.columns)\n",
    "print(\"Neue Spalten (sollten nur die Cluster-Labels sein):\", added)\n",
    "print(\"Alte Spalten unverändert:\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7ec10",
   "metadata": {},
   "source": [
    "Merge-Check: ob alle alten Spalten noch da sind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"/Users/tunahansari/football_ra/out_1hz_clean\"\n",
    "IN_MASTER = f\"{BASE}/master_1hz_4s_rezero.parquet\"\n",
    "BAD = f\"{BASE}/bad_plays_t0_lt90.csv\"\n",
    "OUT_READY = f\"{BASE}/master_1hz_4s_ready.parquet\"\n",
    "\n",
    "master = pd.read_parquet(IN_MASTER)\n",
    "bad = pd.read_csv(BAD)[\"play_uuid\"].unique()\n",
    "print(\"Bad plays:\", len(bad))\n",
    "\n",
    "clean = master[~master[\"play_uuid\"].isin(bad)].copy()\n",
    "clean.to_parquet(OUT_READY, index=False)\n",
    "print(f\"geschrieben: {OUT_READY}  (Zeilen: {len(clean):,})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
